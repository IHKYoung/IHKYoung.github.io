<!DOCTYPE html>
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\[','\]']],
      }
    });
</script>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>使用TensorRT实现模型加速 | AhaKnow</title>
<meta name="keywords" content="编程日常">
<meta name="description" content="第一步：先测试模型的基准速度 使用ONNX测试模型运行速度基准 目标： 使用ONNX Runtime来测试机器学习模型的推理时间基准。 步骤： 安装ON">
<meta name="author" content="CKYoung">
<link rel="canonical" href="http://ahaknow.com/posts/know/%E4%BD%BF%E7%94%A8tensorrt%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E5%8A%A0%E9%80%9F/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<link rel="icon" href="http://ahaknow.com/Q.gif">
<link rel="icon" type="image/png" sizes="16x16" href="http://ahaknow.com/Q.gif">
<link rel="icon" type="image/png" sizes="32x32" href="http://ahaknow.com/Q.gif">
<link rel="apple-touch-icon" href="http://ahaknow.com/Q.gif">
<link rel="mask-icon" href="http://ahaknow.com/Q.gif">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="使用TensorRT实现模型加速" />
<meta property="og:description" content="第一步：先测试模型的基准速度 使用ONNX测试模型运行速度基准 目标： 使用ONNX Runtime来测试机器学习模型的推理时间基准。 步骤： 安装ON" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://ahaknow.com/posts/know/%E4%BD%BF%E7%94%A8tensorrt%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E5%8A%A0%E9%80%9F/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-05T19:06:48+08:00" />
<meta property="article:modified_time" content="2023-09-05T19:06:48+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="使用TensorRT实现模型加速"/>
<meta name="twitter:description" content="第一步：先测试模型的基准速度 使用ONNX测试模型运行速度基准 目标： 使用ONNX Runtime来测试机器学习模型的推理时间基准。 步骤： 安装ON"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "📚专栏",
      "item": "http://ahaknow.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "🌟知识",
      "item": "http://ahaknow.com/posts/know/"
    }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "使用TensorRT实现模型加速",
      "item": "http://ahaknow.com/posts/know/%E4%BD%BF%E7%94%A8tensorrt%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E5%8A%A0%E9%80%9F/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "使用TensorRT实现模型加速",
  "name": "使用TensorRT实现模型加速",
  "description": "第一步：先测试模型的基准速度 使用ONNX测试模型运行速度基准 目标： 使用ONNX Runtime来测试机器学习模型的推理时间基准。 步骤： 安装ON",
  "keywords": [
    "编程日常"
  ],
  "articleBody": "第一步：先测试模型的基准速度 使用ONNX测试模型运行速度基准 目标： 使用ONNX Runtime来测试机器学习模型的推理时间基准。\n步骤：\n安装ONNX和ONNX Runtime： 这两个Python库是测试基准的关键工具。ONNX是一个开放的模型表示标准，它允许在不同的机器学习框架之间移动模型。ONNX Runtime则是一个用于运行ONNX模型的高性能推理引擎。可以通过pip进行安装，命令为pip install onnx onnxruntime-gpu。这里的onnxruntime-gpu版本是支持GPU的ONNX Runtime。\n加载模型： 使用ONNX Runtime加载模型，具体做法是创建一个onnxruntime.InferenceSession的实例，并指定模型的文件路径和推理引擎提供商（这里选择的是'CUDAExecutionProvider'，即使用CUDA来利用GPU加速）。\n获取输入信息并创建输入数据： 需要知道模型的输入节点的名称和形状，这可以通过session.get_inputs()来获取。有了这些信息后，就可以创建与之匹配的随机输入数据。\n推理并计时： 进行100次模型推理，并使用time.time()来计时。注意，这里忽略了第一次推理的时间，因为第一次推理可能会包含一些初始化操作，导致耗时偏大。\n计算平均推理时间： 通过将所有推理的时间加起来，然后除以推理次数，就得到了平均推理时间。\n问题和解决：\n如何创建匹配模型输入的数据： 模型的输入可能有多个，也可能有各种各样的形状和类型。需要通过session.get_inputs()获取这些信息，然后根据这些信息创建相应的输入数据。\n如何确保使用了GPU： 需要在创建onnxruntime.InferenceSession时明确指定使用'CUDAExecutionProvider'。如果没有正确地使用GPU，可能会导致推理速度大幅度降低。\n如何准确计时： 选择忽略了第一次推理的时间，因为第一次推理可能会包含一些初始化操作，导致耗时偏大。\nPython脚本代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import numpy as np import onnxruntime import time # 加载模型 model_path = \"/home/metoak/Projects/ros_project/src/metoak_camera_driver/models/model_640_384_6125.onnx\" # session = onnxruntime.InferenceSession(model_path) session = onnxruntime.InferenceSession(model_path, providers=['CUDAExecutionProvider']) # 获取输入名称和形状 input_names = [inp.name for inp in session.get_inputs()] input_shapes = [inp.shape for inp in session.get_inputs()] print(\"Input names:\", input_names) print(\"Input shapes:\", input_shapes) # 创建两个随机输入 input1 = np.random.randint(0, 256, size=input_shapes[0]).astype(np.uint8) input2 = np.random.randint(0, 256, size=input_shapes[1]).astype(np.uint8) # 准备输入数据 inputs = {input_names[0]: input1, input_names[1]: input2} # 推理次数 num_inferences = 100 times = [] # 执行推断并计时 for i in range(num_inferences): start = time.time() output = session.run(None, inputs) end = time.time() inference_time = end - start if i == 0: continue times.append(inference_time) print(f\"Inference {i+1}, Time: {inference_time * 1000} ms\") # 计算平均推断时间 avg_time = sum(times) / (num_inferences - 1) # 打印平均推断时间 print(f\"Average inference time: {avg_time * 1000} ms\") 模型速度基准 1 2 3 4 5 6 7 8 9 10 Model name: model_640_384_6125.onnx Input names: ['onnx::Cast_0', 'onnx::Cast_1'] Input shapes: [[1, 3, 384, 640], [1, 3, 384, 640]] Inference 2, Time: 299.41678047180176 ms Inference 3, Time: 306.2708377838135 ms ... Inference 98, Time: 332.33070373535156 ms Inference 99, Time: 317.1732425689697 ms Inference 100, Time: 333.3919048309326 ms Average inference time: 322.4095816564078 ms 第二步：使用TensorRT实现加速 如果使用的3080Ti显卡\n浮点计算能力：\nSingle Precision Perf.: ==34.1 TFLOPS==\nTensor Perf. (FP16): ==136 TFLOPS==\nTensor Perf. (FP16-Sparse): ==273 TFLOPS==\n如果使用Tensor来处理全部精度为16位浮点数的算法，那么理论的运算速率可以达到至少是单精度浮点运算的 136 / 34.1 = 3.99，也就是4倍速度的提升，而如果模型本身就是混合进度进行运算的，那么提升的效果会更大！（如果再考虑上稀疏优化，那么Tensor加速的倍数会更高，达到 273 / 34.1 = 8倍的提升）\n🌟TensorRT环境部署 安装方法对了，安装其实很简单，一句话说明就是：\n==cuda、tensorrt都使用dep包，然后使用apt安装==\n但是细节很关键！下面的过程涉及安装CUDA、cuDNN、TensorRT\n官方文档\n1. 安装CUDA CUDA和Nvida Driver这俩个是独立不影响的，所以单独安装CUDA版本即可，与Nvida Driver版本无关，比如安装==cuda 11.8==\nUbuntu20.04:\n1 2 3 4 5 6 7 wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda-repo-ubuntu2004-11-8-local_11.8.0-520.61.05-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu2004-11-8-local_11.8.0-520.61.05-1_amd64.deb sudo cp /var/cuda-repo-ubuntu2004-11-8-local/cuda-*-keyring.gpg /usr/share/keyrings/ sudo apt-get update sudo apt-get -y install cuda 如果使用了runfile的方式安装过cuda了，那么最后一步只需要：\n1 sudo apt install cuda-libraries-11-8 2. 安装cuDNN 需要注册Nvidia开发者账号，下载对应的CUDA版本的即可，使用如下方式安装：\n官网\n下载deb版本即可\n1 2 3 4 sudo dpkg -i cudnn-local-repo-ubuntu2004-8.9.2.26_1.0-1_amd64.deb sudo cp /var/cudnn-local-repo-ubuntu2004-8.9.2.26/cudnn-local-6D0A7AE1-keyring.gpg /usr/share/keyrings/ sudo apt update sudo apt-get install libcudnn8 3. 安装TensorRT 官网\n下载最新版本TensorRT 8，同样需要开发者账号；\n下载对应系统的对应CUDA的最新版即可\n1 2 3 4 sudo dpkg -i nv-tensorrt-local-repo-ubuntu2004-8.6.1-cuda-11.8_1.0-1_amd64.deb sudo cp /var/nv-tensorrt-local-repo-ubuntu2004-8.6.1-cuda-11.8_1.0-1/*-keyring.gpg /usr/share/keyrings/ wozate sudo apt install tensorrt 使用trtexec工具实现模型转换 1 2 # 使用例子 trtexec --onnx=/home/metoak/Projects/ros_project/src/metoak_camera_driver/models/model_sim_1280_704.onnx --saveEngine=model_1280_704_fp16.engine --fp16 --avgRuns=100 可以使用trtexec这个工具来直接转换onnx模型，但实际可能不理想或者遇到一些问题：\nonnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32. [07/25/2023-10:29:30] [W] [TRT] onnx2trt_utils.cpp:400: One or more weights outside the range of INT32 was clamped [07/25/2023-10:29:30] [I] Finished parsing network model. Parse time: 1.3651 [07/25/2023-10:29:30] [I] [TRT] Graph optimization time: 0.282094 seconds. [07/25/2023-10:29:30] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2630, GPU 3661 (MiB) [07/25/2023-10:29:30] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 2630, GPU 3671 (MiB) [07/25/2023-10:29:30] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\nYour ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.: 这个警告表明你的ONNX模型中包含了INT64类型的权重，而TensorRT原生不支持INT64类型。TensorRT正在尝试将这些权重转换（cast）为INT32类型。这可能会导致一些精度损失，因为INT32的范围小于INT64。\nOne or more weights outside the range of INT32 was clamped: 这个警告表明在转换权重类型的过程中，有一个或多个权重的值超出了INT32类型的范围，并被压缩（clamp）到了INT32的范围内。这也可能会导致一些精度损失。\nFinished parsing network model. Parse time: 1.3651: 这表示TensorRT已经完成了对ONNX模型的解析，整个解析过程花费了1.3651秒。\nGraph optimization time: 0.282094 seconds: 这表示TensorRT对模型进行优化的时间是0.282094秒。\nInit cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2630, GPU 3661 (MiB) 和 Init cuDNN: CPU +0, GPU +10, now: CPU 2630, GPU 3671 (MiB): 这些行是关于TensorRT初始化cuBLAS和cuDNN库（这些库用于GPU加速的线性代数运算和深度神经网络计算）所需内存的信息。这些行显示了在初始化这些库后，CPU和GPU内存的使用情况。\nLocal timing cache in use. Profiling results in this builder pass will not be stored.: 这表示TensorRT正在使用本地的计时缓存进行操作，而这次的分析结果不会被存储。\n使用trtexec来转换模型，一般情况下，得到下面的输出时，会成功，但也有可能得到模型无法正常使用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [07/25/2023-14:44:13] [I] Starting inference [07/25/2023-14:44:16] [I] Warmup completed 12 queries over 200 ms [07/25/2023-14:44:16] [I] Timing trace has 180 queries over 3.05014 s [07/25/2023-14:44:16] [I] [07/25/2023-14:44:16] [I] === Trace details === [07/25/2023-14:44:16] [I] Trace averages of 100 runs: [07/25/2023-14:44:16] [I] Average on 100 runs - GPU latency: 16.9045 ms - Host latency: 17.8527 ms (enqueue 11.5822 ms) [07/25/2023-14:44:16] [I] [07/25/2023-14:44:16] [I] === Performance summary === [07/25/2023-14:44:16] [I] Throughput: 59.0137 qps [07/25/2023-14:44:16] [I] Latency: min = 14.8851 ms, max = 27.6843 ms, mean = 17.7984 ms, median = 16.9878 ms, percentile(90%) = 21.1488 ms, percentile(95%) = 23.4043 ms, percentile(99%) = 27.5272 ms [07/25/2023-14:44:16] [I] Enqueue Time: min = 3.59753 ms, max = 22.9529 ms, mean = 11.5029 ms, median = 10.7087 ms, percentile(90%) = 13.9353 ms, percentile(95%) = 16.418 ms, percentile(99%) = 20.6213 ms [07/25/2023-14:44:16] [I] H2D Latency: min = 0.0994873 ms, max = 0.790527 ms, mean = 0.303106 ms, median = 0.268311 ms, percentile(90%) = 0.458588 ms, percentile(95%) = 0.503906 ms, percentile(99%) = 0.564697 ms [07/25/2023-14:44:16] [I] GPU Compute Time: min = 14.0032 ms, max = 26.499 ms, mean = 16.8485 ms, median = 16.0006 ms, percentile(90%) = 19.6926 ms, percentile(95%) = 22.436 ms, percentile(99%) = 26.4436 ms [07/25/2023-14:44:16] [I] D2H Latency: min = 0.319824 ms, max = 1.09839 ms, mean = 0.646768 ms, median = 0.621216 ms, percentile(90%) = 0.887512 ms, percentile(95%) = 0.963013 ms, percentile(99%) = 1.05774 ms [07/25/2023-14:44:16] [I] Total Host Walltime: 3.05014 s [07/25/2023-14:44:16] [I] Total GPU Compute Time: 3.03273 s [07/25/2023-14:44:16] [W] * GPU compute time is unstable, with coefficient of variance = 15.1765%. [07/25/2023-14:44:16] [W] If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability. [07/25/2023-14:44:16] [I] Explanations of the performance metrics are printed in the verbose logs. [07/25/2023-14:44:16] [I] \u0026\u0026\u0026\u0026 PASSED TensorRT.trtexec [TensorRT v8601] # /usr/src/tensorrt/bin/trtexec --onnx=/home/metoak/Projects/ros_project/src/metoak_camera_driver/models/model_640_384_6125.onnx --saveEngine=model_640_384.engine --fp16 --avgRuns=100 这是通过使用TensorRT工具trtexec的一段测试输出，下面是每个重要部分的解释：\n[I] Warmup completed 12 queries over 200 ms：在开始正式计时之前，trtexec首先执行了12个预热查询，总耗时200毫秒。\n[I] Timing trace has 180 queries over 3.05014 s：这表示运行了180次推断，总耗时约为3.05秒。\n[I] Average on 100 runs - GPU latency: 16.9045 ms - Host latency: 17.8527 ms：这表示在100次运行中，平均每次推断在GPU上的延迟是16.9045毫秒，在主机上的延迟是17.8527毫秒。\n[I] Throughput: 59.0137 qps：这表示模型的吞吐量为59.0137查询每秒（qps）。\n[I] Latency: min = 14.8851 ms, max = 27.6843 ms, mean = 17.7984 ms, median = 16.9878 ms：这是对延迟的一些统计描述，包括最小值、最大值、平均值和中位数。\n[W] * GPU compute time is unstable, with coefficient of variance = 15.1765%.：这是一个警告，表示GPU计算时间的变化是不稳定的，方差系数为15.1765%，可能需要通过锁定GPU时钟频率或使用--useSpinWait选项来改善这种情况。\n\u0026\u0026\u0026\u0026 PASSED TensorRT.trtexec [TensorRT v8601] # /usr/src/tensorrt/bin/trtexec --onnx=/home/metoak/Projects/ros_project/src/metoak_camera_driver/models/model_640_384_6125.onnx --saveEngine=model_640_384.engine --fp16 --avgRuns=100：这表示trtexec测试通过了，并且显示了你用来运行trtexec的命令行参数。\n验证engine模型是否可用？ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import tensorrt as trt TRT_LOGGER = trt.Logger(trt.Logger.WARNING) trt_runtime = trt.Runtime(TRT_LOGGER) engine_path = \"/home/metoak/Projects/models/model_resnet101.engine\" with open(engine_path, \"rb\") as f: engine_data = f.read() engine = trt_runtime.deserialize_cuda_engine(engine_data) print(\"Number of bindings: \", engine.num_bindings) for i in range(engine.num_bindings): binding_name = engine.get_tensor_name(i) print(\"Binding \", i, \":\", \"Binding name: \", binding_name, \"Binding shape: \", engine.get_tensor_shape(binding_name), \"Binding data type: \", engine.get_tensor_dtype(binding_name), \"Is binding an input? \", engine.get_tensor_mode(binding_name) == trt.TensorIOMode.INPUT) print(\"Number of layers: \", engine.num_layers) 如果所有的打印输出都正常，那么说明转换的模型可以，==并且以正常方法进行推理也是可行的==\n之后就是将模型推理部分整合进入项目代码即可。\n第三步：一般的方法不行怎么办？（‼️非常关键） 使用trtexec提供的转换模型方法，无法直接使用\n原因分析：模型比较复杂，TensorRT无法支持 —— 这个问题在安装TensorRT最新版之后解决，但TensorRT增加了新特征（使用第三方插件），无法直接使用trtexec转换后的模型：\nIn TensorRT 8.6 there are two implementations of InstanceNormalization that may perform differently depending on various parameters. By default the parser will insert an InstanceNormalization plugin layer as it performs best for general use cases. Users that want to benchmark using the native TensorRT implementation of InstanceNorm can set the parser flag kNATIVE_INSTANCENORM prior to parsing the model. For building version compatible or hardware compatible engines, this flag must be set.\n使用python脚本来处理，debug功能较弱，尝试使用C++的代码来重写整个工具链\n使用新版本特性来转换Onnx 直接上代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 // Usage: ./onnx_to_tensorrt onnx_path #include #include #include #include // 继承自 nvinfer1::ILogger 以便我们可以修改日志处理方式 class Logger : public nvinfer1::ILogger { public: void log(Severity severity, const char *msg) noexcept override { // 调试完整信息 if (severity != Severity::kINFO) std::cout \u003c\u003c msg \u003c\u003c std::endl; } } gLogger; #include int main(int argc, char *argv[]) { if (argc \u003c 2) { std::cerr \u003c\u003c \"Please specify path to the ONNX model.\\n\"; return 1; } auto builder = nvinfer1::createInferBuilder(gLogger); const auto explicitBatch = 1U \u003c\u003c static_cast\u003cuint32_t\u003e(nvinfer1::NetworkDefinitionCreationFlag::kEXPLICIT_BATCH); auto network = builder-\u003ecreateNetworkV2(explicitBatch); auto config = builder-\u003ecreateBuilderConfig(); auto parser = nvonnxparser::createParser(*network, gLogger); // 注意这里：使用TensorRT自带的InstanceNormalization实现 auto flag = 1U \u003c\u003c static_cast\u003cuint32_t\u003e(nvonnxparser::OnnxParserFlag::kNATIVE_INSTANCENORM); parser-\u003esetFlags(flag); std::string model_path = argv[1]; if (!parser-\u003eparseFromFile(model_path.c_str(), 1)) { std::cerr \u003c\u003c \"Failed to parse the ONNX model.\\n\"; return 1; } auto engine = builder-\u003ebuildEngineWithConfig(*network, *config); if (!engine) { std::cerr \u003c\u003c \"Failed to create the TensorRT engine.\\n\"; return 1; } std::string engine_path = model_path.substr(0, model_path.find_last_of('.')) + \".trt\"; std::cout \u003c\u003c \"Serializing the TensorRT engine to \" \u003c\u003c engine_path \u003c\u003c \"...\\n\"; nvinfer1::IHostMemory *serializedModel = engine-\u003eserialize(); std::ofstream engineFile(engine_path, std::ios::binary); engineFile.write(reinterpret_cast\u003cconst char *\u003e(serializedModel-\u003edata()), serializedModel-\u003esize()); engineFile.close(); serializedModel-\u003edestroy(); parser-\u003edestroy(); engine-\u003edestroy(); config-\u003edestroy(); network-\u003edestroy(); builder-\u003edestroy(); return 0; } 直接使用此代码进行模型转换;\n验证模型可行性 \u0026\u0026 模型推理过程 这里直接写一个demo，来实现模型的推理过程，从而验证模型的可行性，这部分直接上代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 // tensorrt_image.cpp #include #include #include #include #include #define TEST_TIME 0 // 继承自 nvinfer1::ILogger 以便我们可以修改日志处理方式 class Logger : public nvinfer1::ILogger { public: void log(Severity severity, const char *msg) noexcept override { // Severity::kVERBOSE 或 kINFO // 只打印错误信息 if (severity == Severity::kERROR \u0026\u0026 severity == Severity::kINTERNAL_ERROR) { std::cout \u003c\u003c msg \u003c\u003c std::endl; } } } gLogger; int main(int argc, char *argv[]) { if (argc \u003c 2) { std::cerr \u003c\u003c \"Please specify path to the engine file.\\n\"; return 1; } // 打开并反序列化 TensorRT engine std::ifstream engineFile(argv[1], std::ios::binary); if (!engineFile) { std::cerr \u003c\u003c \"Error opening engine file\\n\"; return -1; } engineFile.seekg(0, engineFile.end); long int fsize = engineFile.tellg(); engineFile.seekg(0, engineFile.beg); std::vector\u003cchar\u003e engineData(fsize); engineFile.read(engineData.data(), fsize); if (!engineFile) { std::cerr \u003c\u003c \"Error reading engine file\\n\"; return -1; } nvinfer1::IRuntime *runtime = nvinfer1::createInferRuntime(gLogger); nvinfer1::ICudaEngine *engine = runtime-\u003edeserializeCudaEngine(engineData.data(), fsize, nullptr); // 创建推理上下文 nvinfer1::IExecutionContext *context = engine-\u003ecreateExecutionContext(); // 读取并预处理图像 cv::Mat img1 = cv::imread(\"/home/metoak/Projects/dependency/InferenceDemo/images/left.png\", cv::IMREAD_COLOR); cv::Mat img2 = cv::imread(\"/home/metoak/Projects/dependency/InferenceDemo/images/right.png\", cv::IMREAD_COLOR); cv::resize(img1, img1, cv::Size(640, 384)); cv::resize(img2, img2, cv::Size(640, 384)); img1.convertTo(img1, CV_32FC3, 1 / 255.0); img2.convertTo(img2, CV_32FC3, 1 / 255.0); // 创建输入数据 int num_elements_input = 1 * 3 * 384 * 640; // 根据你的模型的实际需求进行调整 // 分割通道 std::vector\u003ccv::Mat\u003e channels1(3); std::vector\u003ccv::Mat\u003e channels2(3); cv::split(img1, channels1); cv::split(img2, channels2); std::vector\u003cfloat\u003e data1; std::vector\u003cfloat\u003e data2; // 将每个通道的数据添加到相应的向量 for (auto \u0026channel : channels1) { std::vector\u003cfloat\u003e channelData(channel.begin\u003cfloat\u003e(), channel.end\u003cfloat\u003e()); data1.insert(data1.end(), channelData.begin(), channelData.end()); } for (auto \u0026channel : channels2) { std::vector\u003cfloat\u003e channelData(channel.begin\u003cfloat\u003e(), channel.end\u003cfloat\u003e()); data2.insert(data2.end(), channelData.begin(), channelData.end()); } #if TEST_TIME == 1 // 打印图像的前10个像素值 std::cout \u003c\u003c \"First 10 pixels of the input image:\\n\"; for (int i = 0; i \u003c 10; ++i) { std::cout \u003c\u003c data1[i] \u003c\u003c \" \"; } #endif // 转换回图像 std::vector\u003ccv::Mat\u003e channels(3); for (int c = 0; c \u003c 3; ++c) { channels[c] = cv::Mat(384, 640, CV_32F, data1.data() + c * 384 * 640); } cv::Mat imgData1; cv::merge(channels, imgData1); // 归一化并将数据类型转换回 CV_8UC3 以便显示 imgData1 = imgData1 * 255.0; imgData1.convertTo(imgData1, CV_8UC3); cv::imwrite(\"/home/metoak/Projects/input.png\", imgData1); // 分配 GPU 内存用于输入 void *gpuData1; void *gpuData2; cudaMalloc(\u0026gpuData1, num_elements_input * sizeof(float)); cudaMalloc(\u0026gpuData2, num_elements_input * sizeof(float)); // 复制输入数据到 GPU cudaMemcpy(gpuData1, data1.data(), num_elements_input * sizeof(float), cudaMemcpyHostToDevice); cudaMemcpy(gpuData2, data2.data(), num_elements_input * sizeof(float), cudaMemcpyHostToDevice); // 根据你的模型的输出大小来创建输出数据 int num_elements_output = 1 * 3 * 384 * 640; // 你需要根据你的模型的实际输出大小进行调整 // 分配 GPU 内存用于输出 void *gpuOutput[7]; for (int i = 0; i \u003c 7; ++i) { cudaMalloc(\u0026gpuOutput[i], num_elements_output * sizeof(float)); } // 创建输入输出的指针数组 void *bindings[9] = {gpuData1, gpuData2, gpuOutput[0], gpuOutput[1], gpuOutput[2], gpuOutput[3], gpuOutput[4], gpuOutput[5], gpuOutput[6]}; // 创建 CUDA stream cudaStream_t stream; cudaStreamCreate(\u0026stream); #if TEST_TIME == 1 // 100次推理 cudaEvent_t start, stop; cudaEventCreate(\u0026start); cudaEventCreate(\u0026stop); float totalTime = 0; int iteration = 100; // 推理次数 for (int i = 0; i \u003c iteration; i++) { cudaEventRecord(start, stream); // 执行推理 context-\u003eenqueue(1, bindings, stream, nullptr); std::cout \u003c\u003c \"Inference iteration: \" \u003c\u003c i \u003c\u003c std::endl; cudaEventRecord(stop, stream); cudaEventSynchronize(stop); float milliseconds = 0; cudaEventElapsedTime(\u0026milliseconds, start, stop); totalTime += milliseconds; } float avgTime = totalTime / iteration; std::cout \u003c\u003c \"Average inference time per image: \" \u003c\u003c avgTime \u003c\u003c \" ms.\" \u003c\u003c std::endl; cudaEventDestroy(start); cudaEventDestroy(stop); #else // 执行推理 context-\u003eenqueue(1, bindings, stream, nullptr); #endif // 将需要的输出数据从 GPU 内存复制回 CPU 内存 std::vector\u003cfloat\u003e outputData(num_elements_output); cudaMemcpy(outputData.data(), gpuOutput[6], num_elements_output * sizeof(float), cudaMemcpyDeviceToHost); #if TEST_TIME == 1 // 打印图像的前10个像素值 std::cout \u003c\u003c \"First 10 pixels of the output image:\\n\"; for (int i = 0; i \u003c 10; ++i) { std::cout \u003c\u003c outputData[i] \u003c\u003c \" \"; } #endif // 将一维向量转换为3通道图像 cv::Mat img_out(384, 640, CV_32F, outputData.data()); cv::Mat img_out_normalized; cv::normalize(img_out, img_out_normalized, 0, 255, cv::NORM_MINMAX, CV_8U); cv::Mat img_color_mapped; cv::applyColorMap(img_out_normalized, img_color_mapped, cv::COLORMAP_JET); // cv::resize(img_out, img_out, cv::Size(1920, 1080)); // 保存图像 cv::imwrite(\"/home/metoak/Projects/output.png\", img_color_mapped); std::cout \u003c\u003c \"Output image size: \" \u003c\u003c img_out_normalized.size() \u003c\u003c \"\\n\"; // 清理 cudaFree(gpuData1); cudaFree(gpuData2); for (int i = 0; i \u003c 7; ++i) { cudaFree(gpuOutput[i]); } context-\u003edestroy(); engine-\u003edestroy(); runtime-\u003edestroy(); // 销毁 CUDA stream cudaStreamDestroy(stream); return 0; } 这里有几点注意：\n数据的前处理需要根据的模型的结构来调整，这里是输入两组图像\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // 分配 GPU 内存用于输入 void *gpuData1; void *gpuData2; cudaMalloc(\u0026gpuData1, num_elements_input * sizeof(float)); cudaMalloc(\u0026gpuData2, num_elements_input * sizeof(float)); // 复制输入数据到 GPU cudaMemcpy(gpuData1, data1.data(), num_elements_input * sizeof(float), cudaMemcpyHostToDevice); cudaMemcpy(gpuData2, data2.data(), num_elements_input * sizeof(float), cudaMemcpyHostToDevice); // 根据你的模型的输出大小来创建输出数据 int num_elements_output = 1 * 3 * 384 * 640; // 你需要根据你的模型的实际输出大小进行调整 // 分配 GPU 内存用于输出 void *gpuOutput[7]; for (int i = 0; i \u003c 7; ++i) { cudaMalloc(\u0026gpuOutput[i], num_elements_output * sizeof(float)); } 这一块就是根据模型的结构来调整输入和输出的\n这里加入了很多手工调试的信息，比如保存模型输入前后的图像来验证\n这是一段完整的推理代码，如果这一段代码能够成功运行，那么将这段代码整合到实际的功能中，定义好输入输出的接口，模型也一样可以正常使用\n这里很多地方是手动申请内存空间的，所以一定注意使用后的销毁（当然也可以使用类来实现析构这种方法）\n1 2 3 4 5 6 7 8 9 10 11 12 // 清理 cudaFree(gpuData1); cudaFree(gpuData2); for (int i = 0; i \u003c 7; ++i) { cudaFree(gpuOutput[i]); } context-\u003edestroy(); engine-\u003edestroy(); runtime-\u003edestroy(); // 销毁 CUDA stream cudaStreamDestroy(stream); 注意理解这一部分的功能和必要性\n有了这两部分后，模型从ONNX到TensorRT，再到TensorRT的推理就打通了！\n对于上面的代码编译，直接使用cmake的方法，CMakeLists.txt例子如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 cmake_minimum_required(VERSION 3.10) project(OnnxInferenceDemo) # 调试模式 set(CMAKE_BUILD_TYPE Debug) set(CMAKE_CXX_STANDARD 14) # 设置METOAK的驱动路径 set(MO_SDK_DIR /opt/moak) find_package(OpenCV REQUIRED) find_package(PCL REQUIRED) find_package(CUDA REQUIRED) find_path(TENSORRT_INCLUDE_DIR NvInfer.h HINTS ${TENSORRT_ROOT_DIR} PATH_SUFFIXES include/) find_library(TENSORRT_LIBRARY nvinfer HINTS ${TENSORRT_ROOT_DIR} PATH_SUFFIXES lib lib64 lib/x64) find_library(TENSORRT_ONNX_LIBRARY nvonnxparser HINTS ${TENSORRT_ROOT_DIR} PATH_SUFFIXES lib lib64 lib/x64) set(TENSORRT_INCLUDE_DIRS ${TENSORRT_INCLUDE_DIR}) set(TENSORRT_LIBRARIES ${TENSORRT_LIBRARY} ${TENSORRT_ONNX_LIBRARY}) include_directories(${TENSORRT_INCLUDE_DIRS}) include_directories(${CUDA_INCLUDE_DIRS}) include_directories( include ${OpenCV_INCLUDE_DIRS} ${MO_SDK_DIR}/3rdparty/ONNX/x86_64/GPU/include ) link_directories(${MO_SDK_DIR}/3rdparty/ONNX/x86_64/GPU/lib) link_directories(lib) add_executable(onnx_to_tensorrt src/onnx_to_tensorrt.cpp) target_link_libraries(onnx_to_tensorrt ${TENSORRT_LIBRARIES} ${CUDA_LIBRARIES}) add_executable(tensorrt_image src/tensorrt_image.cpp) target_link_libraries(tensorrt_image ${OpenCV_LIBS} ${TENSORRT_LIBRARIES} ${CUDA_LIBRARIES}) 总结 使用TensorRT来进行模型加速，如果很凑巧，当前版本无法支持Onnx的模型转换，那可能正常方法就无法使用，就比如这里的模型，在TensorRT 8.6之前的版本，都无法完成正常的转换，并且报出段错误异常，这种情况，可能就是无法支持；\n而使用了最新版本发现，模型可以转换，直接无法直接进行模型的推理，那么就要排查一下是哪里的问题了，比如这里的模型，在TensorRT 8.6.1上可以完整转换，但是需要手动指定使用的插件，内建的还是其他的，这就会影响模型对应的推理程序的书写。\n另外一点：\nTensorRT的安装和环境部署其实很简单，但是因为网络上的搜索出来的教程杂七杂八，导致没有按照统一的方式来安装CUDA、cuDNN、TensorRT，导致apt安装时出现依赖异常，其实只要全部采用dep包管理的方式来安装就不会有问题。\n",
  "wordCount" : "6046",
  "inLanguage": "zh",
  "datePublished": "2023-09-05T19:06:48+08:00",
  "dateModified": "2023-09-05T19:06:48+08:00",
  "author":[{
    "@type": "Person",
    "name": "CKYoung"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://ahaknow.com/posts/know/%E4%BD%BF%E7%94%A8tensorrt%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E5%8A%A0%E9%80%9F/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "AhaKnow",
    "logo": {
      "@type": "ImageObject",
      "url": "http://ahaknow.com/Q.gif"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://ahaknow.com" accesskey="h" title="AhaKnow (Alt + H)">
                <img src="http://ahaknow.com/Q.gif" alt="" aria-label="logo"
                    height="35">AhaKnow</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://ahaknow.com/" title="🏡主页">
                    <span>🏡主页</span>
                </a>
            </li>
            <li>
                <a href="http://ahaknow.com/posts" title="📚专栏">
                    <span>📚专栏</span>
                </a>
            </li>
            <li>
                <a href="http://ahaknow.com/archives" title="⏱️时间线">
                    <span>⏱️时间线</span>
                </a>
            </li>
            <li>
                <a href="http://ahaknow.com/tags" title="🏷️标签">
                    <span>🏷️标签</span>
                </a>
            </li>
            <li>
                <a href="http://ahaknow.com/categories" title="🖇️归档">
                    <span>🖇️归档</span>
                </a>
            </li>
            <li>
                <a href="http://ahaknow.com/search" title="🔍搜索 (Alt &#43; /)" accesskey=/>
                    <span>🔍搜索</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://ahaknow.com">🏡主页</a>&nbsp;»&nbsp;<a href="http://ahaknow.com/posts/">📚专栏</a>&nbsp;»&nbsp;<a href="http://ahaknow.com/posts/know/">🌟知识</a></div>
    <h1 class="post-title">
      使用TensorRT实现模型加速
    </h1>
    <div class="post-meta"><span title='2023-09-05 19:06:48 +0800 CST'>2023-09-05</span>&nbsp;·&nbsp;13 分钟&nbsp;·&nbsp;CKYoung

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">文章目录</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e7%ac%ac%e4%b8%80%e6%ad%a5%e5%85%88%e6%b5%8b%e8%af%95%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%9f%ba%e5%87%86%e9%80%9f%e5%ba%a6" aria-label="第一步：先测试模型的基准速度">第一步：先测试模型的基准速度</a><ul>
                        
                <li>
                    <a href="#%e4%bd%bf%e7%94%a8onnx%e6%b5%8b%e8%af%95%e6%a8%a1%e5%9e%8b%e8%bf%90%e8%a1%8c%e9%80%9f%e5%ba%a6%e5%9f%ba%e5%87%86" aria-label="使用ONNX测试模型运行速度基准">使用ONNX测试模型运行速度基准</a></li>
                <li>
                    <a href="#python%e8%84%9a%e6%9c%ac%e4%bb%a3%e7%a0%81" aria-label="Python脚本代码">Python脚本代码</a></li>
                <li>
                    <a href="#%e6%a8%a1%e5%9e%8b%e9%80%9f%e5%ba%a6%e5%9f%ba%e5%87%86" aria-label="模型速度基准">模型速度基准</a></li></ul>
                </li>
                <li>
                    <a href="#%e7%ac%ac%e4%ba%8c%e6%ad%a5%e4%bd%bf%e7%94%a8tensorrt%e5%ae%9e%e7%8e%b0%e5%8a%a0%e9%80%9f" aria-label="第二步：使用TensorRT实现加速">第二步：使用TensorRT实现加速</a><ul>
                        
                <li>
                    <a href="#tensorrt%e7%8e%af%e5%a2%83%e9%83%a8%e7%bd%b2" aria-label="🌟TensorRT环境部署">🌟TensorRT环境部署</a><ul>
                        
                <li>
                    <a href="#1-%e5%ae%89%e8%a3%85cuda" aria-label="1. 安装CUDA">1. 安装CUDA</a></li>
                <li>
                    <a href="#2-%e5%ae%89%e8%a3%85cudnn" aria-label="2. 安装cuDNN">2. 安装cuDNN</a></li>
                <li>
                    <a href="#3-%e5%ae%89%e8%a3%85tensorrt" aria-label="3. 安装TensorRT">3. 安装TensorRT</a></li></ul>
                </li>
                <li>
                    <a href="#%e4%bd%bf%e7%94%a8trtexec%e5%b7%a5%e5%85%b7%e5%ae%9e%e7%8e%b0%e6%a8%a1%e5%9e%8b%e8%bd%ac%e6%8d%a2" aria-label="使用trtexec工具实现模型转换">使用trtexec工具实现模型转换</a></li>
                <li>
                    <a href="#%e9%aa%8c%e8%af%81engine%e6%a8%a1%e5%9e%8b%e6%98%af%e5%90%a6%e5%8f%af%e7%94%a8" aria-label="验证engine模型是否可用？">验证engine模型是否可用？</a></li></ul>
                </li>
                <li>
                    <a href="#%e7%ac%ac%e4%b8%89%e6%ad%a5%e4%b8%80%e8%88%ac%e7%9a%84%e6%96%b9%e6%b3%95%e4%b8%8d%e8%a1%8c%e6%80%8e%e4%b9%88%e5%8a%9e%e9%9d%9e%e5%b8%b8%e5%85%b3%e9%94%ae" aria-label="第三步：一般的方法不行怎么办？（‼️非常关键）">第三步：一般的方法不行怎么办？（‼️非常关键）</a><ul>
                        
                <li>
                    <a href="#%e4%bd%bf%e7%94%a8%e6%96%b0%e7%89%88%e6%9c%ac%e7%89%b9%e6%80%a7%e6%9d%a5%e8%bd%ac%e6%8d%a2onnx" aria-label="使用新版本特性来转换Onnx">使用新版本特性来转换Onnx</a></li>
                <li>
                    <a href="#%e9%aa%8c%e8%af%81%e6%a8%a1%e5%9e%8b%e5%8f%af%e8%a1%8c%e6%80%a7--%e6%a8%a1%e5%9e%8b%e6%8e%a8%e7%90%86%e8%bf%87%e7%a8%8b" aria-label="验证模型可行性 &amp;amp;&amp;amp; 模型推理过程">验证模型可行性 &amp;&amp; 模型推理过程</a></li></ul>
                </li>
                <li>
                    <a href="#%e6%80%bb%e7%bb%93" aria-label="总结">总结</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
            const id = encodeURI(element.getAttribute('id')).toLowerCase();
            if (element === activeElement){
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
            } else {
                document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
            }
        })
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>

  <div class="post-content"><h2 id="第一步先测试模型的基准速度">第一步：先测试模型的基准速度<a hidden class="anchor" aria-hidden="true" href="#第一步先测试模型的基准速度">#</a></h2>
<h3 id="使用onnx测试模型运行速度基准">使用ONNX测试模型运行速度基准<a hidden class="anchor" aria-hidden="true" href="#使用onnx测试模型运行速度基准">#</a></h3>
<p><strong>目标：</strong>
使用ONNX Runtime来测试机器学习模型的推理时间基准。</p>
<p><strong>步骤：</strong></p>
<ol>
<li>
<p><strong>安装ONNX和ONNX Runtime：</strong> 这两个Python库是测试基准的关键工具。ONNX是一个开放的模型表示标准，它允许在不同的机器学习框架之间移动模型。ONNX Runtime则是一个用于运行ONNX模型的高性能推理引擎。可以通过pip进行安装，命令为<code>pip install onnx onnxruntime-gpu</code>。这里的<code>onnxruntime-gpu</code>版本是支持GPU的ONNX Runtime。</p>
</li>
<li>
<p><strong>加载模型：</strong> 使用ONNX Runtime加载模型，具体做法是创建一个<code>onnxruntime.InferenceSession</code>的实例，并指定模型的文件路径和推理引擎提供商（这里选择的是<code>'CUDAExecutionProvider'</code>，即使用CUDA来利用GPU加速）。</p>
</li>
<li>
<p><strong>获取输入信息并创建输入数据：</strong> 需要知道模型的输入节点的名称和形状，这可以通过<code>session.get_inputs()</code>来获取。有了这些信息后，就可以创建与之匹配的随机输入数据。</p>
</li>
<li>
<p><strong>推理并计时：</strong> 进行100次模型推理，并使用<code>time.time()</code>来计时。注意，这里忽略了第一次推理的时间，因为第一次推理可能会包含一些初始化操作，导致耗时偏大。</p>
</li>
<li>
<p><strong>计算平均推理时间：</strong> 通过将所有推理的时间加起来，然后除以推理次数，就得到了平均推理时间。</p>
</li>
</ol>
<p><strong>问题和解决：</strong></p>
<ol>
<li>
<p><strong>如何创建匹配模型输入的数据：</strong> 模型的输入可能有多个，也可能有各种各样的形状和类型。需要通过<code>session.get_inputs()</code>获取这些信息，然后根据这些信息创建相应的输入数据。</p>
</li>
<li>
<p><strong>如何确保使用了GPU：</strong> 需要在创建<code>onnxruntime.InferenceSession</code>时明确指定使用<code>'CUDAExecutionProvider'</code>。如果没有正确地使用GPU，可能会导致推理速度大幅度降低。</p>
</li>
<li>
<p><strong>如何准确计时：</strong> 选择忽略了第一次推理的时间，因为第一次推理可能会包含一些初始化操作，导致耗时偏大。</p>
</li>
</ol>
<h3 id="python脚本代码">Python脚本代码<a hidden class="anchor" aria-hidden="true" href="#python脚本代码">#</a></h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">onnxruntime</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">time</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 加载模型</span>
</span></span><span class="line"><span class="cl"><span class="n">model_path</span> <span class="o">=</span> <span class="s2">&#34;/home/metoak/Projects/ros_project/src/metoak_camera_driver/models/model_640_384_6125.onnx&#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># session = onnxruntime.InferenceSession(model_path)</span>
</span></span><span class="line"><span class="cl"><span class="n">session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">providers</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;CUDAExecutionProvider&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 获取输入名称和形状</span>
</span></span><span class="line"><span class="cl"><span class="n">input_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">inp</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">session</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl"><span class="n">input_shapes</span> <span class="o">=</span> <span class="p">[</span><span class="n">inp</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">session</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Input names:&#34;</span><span class="p">,</span> <span class="n">input_names</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Input shapes:&#34;</span><span class="p">,</span> <span class="n">input_shapes</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 创建两个随机输入</span>
</span></span><span class="line"><span class="cl"><span class="n">input1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">input_shapes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">input2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">input_shapes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 准备输入数据</span>
</span></span><span class="line"><span class="cl"><span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="n">input_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">input1</span><span class="p">,</span> <span class="n">input_names</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="n">input2</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 推理次数</span>
</span></span><span class="line"><span class="cl"><span class="n">num_inferences</span> <span class="o">=</span> <span class="mi">100</span>
</span></span><span class="line"><span class="cl"><span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 执行推断并计时</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_inferences</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">output</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">inference_time</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">continue</span>
</span></span><span class="line"><span class="cl">    <span class="n">times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inference_time</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Inference </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">, Time: </span><span class="si">{</span><span class="n">inference_time</span> <span class="o">*</span> <span class="mi">1000</span><span class="si">}</span><span class="s2"> ms&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 计算平均推断时间</span>
</span></span><span class="line"><span class="cl"><span class="n">avg_time</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">times</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_inferences</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 打印平均推断时间</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Average inference time: </span><span class="si">{</span><span class="n">avg_time</span> <span class="o">*</span> <span class="mi">1000</span><span class="si">}</span><span class="s2"> ms&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="模型速度基准">模型速度基准<a hidden class="anchor" aria-hidden="true" href="#模型速度基准">#</a></h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">Model name: model_640_384_6125.onnx
</span></span><span class="line"><span class="cl">Input names: <span class="o">[</span><span class="s1">&#39;onnx::Cast_0&#39;</span>, <span class="s1">&#39;onnx::Cast_1&#39;</span><span class="o">]</span>
</span></span><span class="line"><span class="cl">Input shapes: <span class="o">[[</span>1, 3, 384, 640<span class="o">]</span>, <span class="o">[</span>1, 3, 384, 640<span class="o">]]</span>
</span></span><span class="line"><span class="cl">Inference 2, Time: 299.41678047180176 ms
</span></span><span class="line"><span class="cl">Inference 3, Time: 306.2708377838135 ms
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">Inference 98, Time: 332.33070373535156 ms
</span></span><span class="line"><span class="cl">Inference 99, Time: 317.1732425689697 ms
</span></span><span class="line"><span class="cl">Inference 100, Time: 333.3919048309326 ms
</span></span><span class="line"><span class="cl">Average inference time: 322.4095816564078 ms
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="第二步使用tensorrt实现加速">第二步：使用TensorRT实现加速<a hidden class="anchor" aria-hidden="true" href="#第二步使用tensorrt实现加速">#</a></h2>
<blockquote>
<p>如果使用的3080Ti显卡</p>
<p>浮点计算能力：</p>
<p><strong>Single Precision Perf.</strong>: ==34.1 TFLOPS==</p>
<p><strong>Tensor Perf. (FP16)</strong>: ==136 TFLOPS==</p>
<p><strong>Tensor Perf. (FP16-Sparse)</strong>: ==273 TFLOPS==</p>
<p><strong>如果使用Tensor来处理全部精度为16位浮点数的算法，那么理论的运算速率可以达到至少是单精度浮点运算的 136 / 34.1 = 3.99，也就是4倍速度的提升，而如果模型本身就是混合进度进行运算的，那么提升的效果会更大！（如果再考虑上稀疏优化，那么Tensor加速的倍数会更高，达到 273 / 34.1 = 8倍的提升）</strong></p>
</blockquote>
<h3 id="tensorrt环境部署">🌟TensorRT环境部署<a hidden class="anchor" aria-hidden="true" href="#tensorrt环境部署">#</a></h3>
<blockquote>
<p>安装方法对了，安装其实很简单，一句话说明就是：</p>
<p>==cuda、tensorrt都使用dep包，然后使用apt安装==</p>
<p>但是细节很关键！下面的过程涉及安装CUDA、cuDNN、TensorRT</p>
</blockquote>
<p><a href="https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html">官方文档</a></p>
<h4 id="1-安装cuda">1. 安装CUDA<a hidden class="anchor" aria-hidden="true" href="#1-安装cuda">#</a></h4>
<p>CUDA和Nvida Driver这俩个是独立不影响的，所以单独安装CUDA版本即可，与Nvida Driver版本无关，比如安装==cuda 11.8==</p>
<p>Ubuntu20.04:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin
</span></span><span class="line"><span class="cl">sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600
</span></span><span class="line"><span class="cl">wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda-repo-ubuntu2004-11-8-local_11.8.0-520.61.05-1_amd64.deb
</span></span><span class="line"><span class="cl">sudo dpkg -i cuda-repo-ubuntu2004-11-8-local_11.8.0-520.61.05-1_amd64.deb
</span></span><span class="line"><span class="cl">sudo cp /var/cuda-repo-ubuntu2004-11-8-local/cuda-*-keyring.gpg /usr/share/keyrings/
</span></span><span class="line"><span class="cl">sudo apt-get update
</span></span><span class="line"><span class="cl">sudo apt-get -y install cuda
</span></span></code></pre></td></tr></table>
</div>
</div><p>如果使用了runfile的方式安装过cuda了，那么最后一步只需要：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo apt install cuda-libraries-11-8  
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="2-安装cudnn">2. 安装cuDNN<a hidden class="anchor" aria-hidden="true" href="#2-安装cudnn">#</a></h4>
<p>需要注册Nvidia开发者账号，下载对应的CUDA版本的即可，使用如下方式安装：</p>
<p><a href="https://developer.nvidia.com/rdp/cudnn-download">官网</a></p>
<p>下载deb版本即可</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"> sudo dpkg -i cudnn-local-repo-ubuntu2004-8.9.2.26_1.0-1_amd64.deb  
</span></span><span class="line"><span class="cl"> sudo cp /var/cudnn-local-repo-ubuntu2004-8.9.2.26/cudnn-local-6D0A7AE1-keyring.gpg /usr/share/keyrings/ 
</span></span><span class="line"><span class="cl"> sudo apt update
</span></span><span class="line"><span class="cl"> sudo apt-get install libcudnn8
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="3-安装tensorrt">3. 安装TensorRT<a hidden class="anchor" aria-hidden="true" href="#3-安装tensorrt">#</a></h4>
<p><a href="https://developer.nvidia.com/nvidia-tensorrt-download">官网</a></p>
<p>下载最新版本TensorRT 8，同样需要开发者账号；</p>
<p>下载对应系统的对应CUDA的最新版即可</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo dpkg -i nv-tensorrt-local-repo-ubuntu2004-8.6.1-cuda-11.8_1.0-1_amd64.deb
</span></span><span class="line"><span class="cl">sudo cp /var/nv-tensorrt-local-repo-ubuntu2004-8.6.1-cuda-11.8_1.0-1/*-keyring.gpg /usr/share/keyrings/
</span></span><span class="line"><span class="cl">wozate
</span></span><span class="line"><span class="cl">sudo apt install tensorrt
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="使用trtexec工具实现模型转换">使用trtexec工具实现模型转换<a hidden class="anchor" aria-hidden="true" href="#使用trtexec工具实现模型转换">#</a></h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 使用例子</span>
</span></span><span class="line"><span class="cl">trtexec --onnx<span class="o">=</span>/home/metoak/Projects/ros_project/src/metoak_camera_driver/models/model_sim_1280_704.onnx --saveEngine<span class="o">=</span>model_1280_704_fp16.engine --fp16 --avgRuns<span class="o">=</span><span class="m">100</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>可以使用trtexec这个工具来直接转换onnx模型，但实际可能不理想或者遇到一些问题：</p>
<blockquote>
<p>onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.
[07/25/2023-10:29:30] [W] [TRT] onnx2trt_utils.cpp:400: One or more weights outside the range of INT32 was clamped
[07/25/2023-10:29:30] [I] Finished parsing network model. Parse time: 1.3651
[07/25/2023-10:29:30] [I] [TRT] Graph optimization time: 0.282094 seconds.
[07/25/2023-10:29:30] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2630, GPU 3661 (MiB)
[07/25/2023-10:29:30] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 2630, GPU 3671 (MiB)
[07/25/2023-10:29:30] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.</p>
</blockquote>
<ol>
<li>
<p><code>Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.</code>: 这个警告表明你的ONNX模型中包含了INT64类型的权重，而TensorRT原生不支持INT64类型。TensorRT正在尝试将这些权重转换（cast）为INT32类型。这可能会导致一些精度损失，因为INT32的范围小于INT64。</p>
</li>
<li>
<p><code>One or more weights outside the range of INT32 was clamped</code>: 这个警告表明在转换权重类型的过程中，有一个或多个权重的值超出了INT32类型的范围，并被压缩（clamp）到了INT32的范围内。这也可能会导致一些精度损失。</p>
</li>
<li>
<p><code>Finished parsing network model. Parse time: 1.3651</code>: 这表示TensorRT已经完成了对ONNX模型的解析，整个解析过程花费了1.3651秒。</p>
</li>
<li>
<p><code>Graph optimization time: 0.282094 seconds</code>: 这表示TensorRT对模型进行优化的时间是0.282094秒。</p>
</li>
<li>
<p><code>Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2630, GPU 3661 (MiB)</code> 和 <code>Init cuDNN: CPU +0, GPU +10, now: CPU 2630, GPU 3671 (MiB)</code>: 这些行是关于TensorRT初始化cuBLAS和cuDNN库（这些库用于GPU加速的线性代数运算和深度神经网络计算）所需内存的信息。这些行显示了在初始化这些库后，CPU和GPU内存的使用情况。</p>
</li>
<li>
<p><code>Local timing cache in use. Profiling results in this builder pass will not be stored.</code>: 这表示TensorRT正在使用本地的计时缓存进行操作，而这次的分析结果不会被存储。</p>
</li>
</ol>
<p>使用trtexec来转换模型，一般情况下，得到下面的输出时，会成功，但也有可能得到模型无法正常使用：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:13<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> Starting inference
</span></span><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:16<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> Warmup completed <span class="m">12</span> queries over <span class="m">200</span> ms
</span></span><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:16<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> Timing trace has <span class="m">180</span> queries over 3.05014 s
</span></span><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:16<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> 
</span></span><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:16<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">===</span> Trace <span class="nv">details</span> <span class="o">===</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:16<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> Trace averages of <span class="m">100</span> runs:
</span></span><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:16<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> Average on <span class="m">100</span> runs - GPU latency: 16.9045 ms - Host latency: 17.8527 ms <span class="o">(</span>enqueue 11.5822 ms<span class="o">)</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:16<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> 
</span></span><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:16<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> <span class="o">===</span> Performance <span class="nv">summary</span> <span class="o">===</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:16<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> Throughput: 59.0137 qps
</span></span><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:16<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> Latency: <span class="nv">min</span> <span class="o">=</span> 14.8851 ms, <span class="nv">max</span> <span class="o">=</span> 27.6843 ms, <span class="nv">mean</span> <span class="o">=</span> 17.7984 ms, <span class="nv">median</span> <span class="o">=</span> 16.9878 ms, percentile<span class="o">(</span>90%<span class="o">)</span> <span class="o">=</span> 21.1488 ms, percentile<span class="o">(</span>95%<span class="o">)</span> <span class="o">=</span> 23.4043 ms, percentile<span class="o">(</span>99%<span class="o">)</span> <span class="o">=</span> 27.5272 ms
</span></span><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:16<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> Enqueue Time: <span class="nv">min</span> <span class="o">=</span> 3.59753 ms, <span class="nv">max</span> <span class="o">=</span> 22.9529 ms, <span class="nv">mean</span> <span class="o">=</span> 11.5029 ms, <span class="nv">median</span> <span class="o">=</span> 10.7087 ms, percentile<span class="o">(</span>90%<span class="o">)</span> <span class="o">=</span> 13.9353 ms, percentile<span class="o">(</span>95%<span class="o">)</span> <span class="o">=</span> 16.418 ms, percentile<span class="o">(</span>99%<span class="o">)</span> <span class="o">=</span> 20.6213 ms
</span></span><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:16<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> H2D Latency: <span class="nv">min</span> <span class="o">=</span> 0.0994873 ms, <span class="nv">max</span> <span class="o">=</span> 0.790527 ms, <span class="nv">mean</span> <span class="o">=</span> 0.303106 ms, <span class="nv">median</span> <span class="o">=</span> 0.268311 ms, percentile<span class="o">(</span>90%<span class="o">)</span> <span class="o">=</span> 0.458588 ms, percentile<span class="o">(</span>95%<span class="o">)</span> <span class="o">=</span> 0.503906 ms, percentile<span class="o">(</span>99%<span class="o">)</span> <span class="o">=</span> 0.564697 ms
</span></span><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:16<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> GPU Compute Time: <span class="nv">min</span> <span class="o">=</span> 14.0032 ms, <span class="nv">max</span> <span class="o">=</span> 26.499 ms, <span class="nv">mean</span> <span class="o">=</span> 16.8485 ms, <span class="nv">median</span> <span class="o">=</span> 16.0006 ms, percentile<span class="o">(</span>90%<span class="o">)</span> <span class="o">=</span> 19.6926 ms, percentile<span class="o">(</span>95%<span class="o">)</span> <span class="o">=</span> 22.436 ms, percentile<span class="o">(</span>99%<span class="o">)</span> <span class="o">=</span> 26.4436 ms
</span></span><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:16<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> D2H Latency: <span class="nv">min</span> <span class="o">=</span> 0.319824 ms, <span class="nv">max</span> <span class="o">=</span> 1.09839 ms, <span class="nv">mean</span> <span class="o">=</span> 0.646768 ms, <span class="nv">median</span> <span class="o">=</span> 0.621216 ms, percentile<span class="o">(</span>90%<span class="o">)</span> <span class="o">=</span> 0.887512 ms, percentile<span class="o">(</span>95%<span class="o">)</span> <span class="o">=</span> 0.963013 ms, percentile<span class="o">(</span>99%<span class="o">)</span> <span class="o">=</span> 1.05774 ms
</span></span><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:16<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> Total Host Walltime: 3.05014 s
</span></span><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:16<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> Total GPU Compute Time: 3.03273 s
</span></span><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:16<span class="o">]</span> <span class="o">[</span>W<span class="o">]</span> * GPU compute <span class="nb">time</span> is unstable, with coefficient of <span class="nv">variance</span> <span class="o">=</span> 15.1765%.
</span></span><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:16<span class="o">]</span> <span class="o">[</span>W<span class="o">]</span>   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.
</span></span><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:16<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> Explanations of the performance metrics are printed in the verbose logs.
</span></span><span class="line"><span class="cl"><span class="o">[</span>07/25/2023-14:44:16<span class="o">]</span> <span class="o">[</span>I<span class="o">]</span> 
</span></span><span class="line"><span class="cl"><span class="o">&amp;&amp;&amp;&amp;</span> PASSED TensorRT.trtexec <span class="o">[</span>TensorRT v8601<span class="o">]</span> <span class="c1"># /usr/src/tensorrt/bin/trtexec --onnx=/home/metoak/Projects/ros_project/src/metoak_camera_driver/models/model_640_384_6125.onnx --saveEngine=model_640_384.engine --fp16 --avgRuns=100</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>这是通过使用TensorRT工具trtexec的一段测试输出，下面是每个重要部分的解释：</p>
<ul>
<li>
<p><code>[I] Warmup completed 12 queries over 200 ms</code>：在开始正式计时之前，trtexec首先执行了12个预热查询，总耗时200毫秒。</p>
</li>
<li>
<p><code>[I] Timing trace has 180 queries over 3.05014 s</code>：这表示运行了180次推断，总耗时约为3.05秒。</p>
</li>
<li>
<p><code>[I] Average on 100 runs - GPU latency: 16.9045 ms - Host latency: 17.8527 ms</code>：这表示在100次运行中，平均每次推断在GPU上的延迟是16.9045毫秒，在主机上的延迟是17.8527毫秒。</p>
</li>
<li>
<p><code>[I] Throughput: 59.0137 qps</code>：这表示模型的吞吐量为59.0137查询每秒（qps）。</p>
</li>
<li>
<p><code>[I] Latency: min = 14.8851 ms, max = 27.6843 ms, mean = 17.7984 ms, median = 16.9878 ms</code>：这是对延迟的一些统计描述，包括最小值、最大值、平均值和中位数。</p>
</li>
<li>
<p><code>[W] * GPU compute time is unstable, with coefficient of variance = 15.1765%.</code>：这是一个警告，表示GPU计算时间的变化是不稳定的，方差系数为15.1765%，可能需要通过锁定GPU时钟频率或使用<code>--useSpinWait</code>选项来改善这种情况。</p>
</li>
<li>
<p><code>&amp;&amp;&amp;&amp; PASSED TensorRT.trtexec [TensorRT v8601] # /usr/src/tensorrt/bin/trtexec --onnx=/home/metoak/Projects/ros_project/src/metoak_camera_driver/models/model_640_384_6125.onnx --saveEngine=model_640_384.engine --fp16 --avgRuns=100</code>：这表示trtexec测试通过了，并且显示了你用来运行trtexec的命令行参数。</p>
</li>
</ul>
<h3 id="验证engine模型是否可用">验证engine模型是否可用？<a hidden class="anchor" aria-hidden="true" href="#验证engine模型是否可用">#</a></h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">tensorrt</span> <span class="k">as</span> <span class="nn">trt</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">TRT_LOGGER</span> <span class="o">=</span> <span class="n">trt</span><span class="o">.</span><span class="n">Logger</span><span class="p">(</span><span class="n">trt</span><span class="o">.</span><span class="n">Logger</span><span class="o">.</span><span class="n">WARNING</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">trt_runtime</span> <span class="o">=</span> <span class="n">trt</span><span class="o">.</span><span class="n">Runtime</span><span class="p">(</span><span class="n">TRT_LOGGER</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">engine_path</span> <span class="o">=</span> <span class="s2">&#34;/home/metoak/Projects/models/model_resnet101.engine&#34;</span>
</span></span><span class="line"><span class="cl"><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">engine_path</span><span class="p">,</span> <span class="s2">&#34;rb&#34;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">engine_data</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">engine</span> <span class="o">=</span> <span class="n">trt_runtime</span><span class="o">.</span><span class="n">deserialize_cuda_engine</span><span class="p">(</span><span class="n">engine_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Number of bindings: &#34;</span><span class="p">,</span> <span class="n">engine</span><span class="o">.</span><span class="n">num_bindings</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">engine</span><span class="o">.</span><span class="n">num_bindings</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">binding_name</span> <span class="o">=</span> <span class="n">engine</span><span class="o">.</span><span class="n">get_tensor_name</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Binding &#34;</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="s2">&#34;:&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">          <span class="s2">&#34;Binding name: &#34;</span><span class="p">,</span> <span class="n">binding_name</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">          <span class="s2">&#34;Binding shape: &#34;</span><span class="p">,</span> <span class="n">engine</span><span class="o">.</span><span class="n">get_tensor_shape</span><span class="p">(</span><span class="n">binding_name</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">          <span class="s2">&#34;Binding data type: &#34;</span><span class="p">,</span> <span class="n">engine</span><span class="o">.</span><span class="n">get_tensor_dtype</span><span class="p">(</span><span class="n">binding_name</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">          <span class="s2">&#34;Is binding an input? &#34;</span><span class="p">,</span> <span class="n">engine</span><span class="o">.</span><span class="n">get_tensor_mode</span><span class="p">(</span><span class="n">binding_name</span><span class="p">)</span> <span class="o">==</span> <span class="n">trt</span><span class="o">.</span><span class="n">TensorIOMode</span><span class="o">.</span><span class="n">INPUT</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Number of layers: &#34;</span><span class="p">,</span> <span class="n">engine</span><span class="o">.</span><span class="n">num_layers</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>如果所有的打印输出都正常，那么说明转换的模型可以，==并且以正常方法进行推理也是可行的==</p>
<p>之后就是将模型推理部分整合进入项目代码即可。</p>
<h2 id="第三步一般的方法不行怎么办非常关键">第三步：一般的方法不行怎么办？（‼️非常关键）<a hidden class="anchor" aria-hidden="true" href="#第三步一般的方法不行怎么办非常关键">#</a></h2>
<ol>
<li>
<p>使用trtexec提供的转换模型方法，无法直接使用</p>
<ul>
<li>
<p>原因分析：模型比较复杂，TensorRT无法支持 —— 这个问题在安装TensorRT最新版之后解决，但TensorRT增加了新特征（使用第三方插件），无法直接使用trtexec转换后的模型：</p>
</li>
<li>
<blockquote>
<p>In TensorRT 8.6 there are two implementations of InstanceNormalization that may perform differently depending on various parameters. By default the parser will insert an InstanceNormalization plugin layer as it performs best for general use cases. Users that want to benchmark using the native TensorRT implementation of InstanceNorm can set the parser flag <code>kNATIVE_INSTANCENORM</code> prior to parsing the model. For building version compatible or hardware compatible engines, this flag must be set.</p>
</blockquote>
</li>
</ul>
</li>
<li>
<p>使用python脚本来处理，debug功能较弱，尝试使用C++的代码来重写整个工具链</p>
</li>
</ol>
<h3 id="使用新版本特性来转换onnx">使用新版本特性来转换Onnx<a hidden class="anchor" aria-hidden="true" href="#使用新版本特性来转换onnx">#</a></h3>
<p>直接上代码：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">// Usage: ./onnx_to_tensorrt onnx_path
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#include</span> <span class="cpf">&lt;NvInfer.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;NvOnnxParser.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;string&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="c1">// 继承自 nvinfer1::ILogger 以便我们可以修改日志处理方式
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">class</span> <span class="nc">Logger</span> <span class="o">:</span> <span class="k">public</span> <span class="n">nvinfer1</span><span class="o">::</span><span class="n">ILogger</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="kt">void</span> <span class="n">log</span><span class="p">(</span><span class="n">Severity</span> <span class="n">severity</span><span class="p">,</span> <span class="k">const</span> <span class="kt">char</span> <span class="o">*</span><span class="n">msg</span><span class="p">)</span> <span class="k">noexcept</span> <span class="k">override</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// 调试完整信息
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="n">severity</span> <span class="o">!=</span> <span class="n">Severity</span><span class="o">::</span><span class="n">kINFO</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">msg</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">gLogger</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;fstream&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">argc</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Please specify path to the ONNX model.</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">builder</span> <span class="o">=</span> <span class="n">nvinfer1</span><span class="o">::</span><span class="n">createInferBuilder</span><span class="p">(</span><span class="n">gLogger</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="k">auto</span> <span class="n">explicitBatch</span> <span class="o">=</span> <span class="mi">1U</span> <span class="o">&lt;&lt;</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">nvinfer1</span><span class="o">::</span><span class="n">NetworkDefinitionCreationFlag</span><span class="o">::</span><span class="n">kEXPLICIT_BATCH</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">network</span> <span class="o">=</span> <span class="n">builder</span><span class="o">-&gt;</span><span class="n">createNetworkV2</span><span class="p">(</span><span class="n">explicitBatch</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">config</span> <span class="o">=</span> <span class="n">builder</span><span class="o">-&gt;</span><span class="n">createBuilderConfig</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">parser</span> <span class="o">=</span> <span class="n">nvonnxparser</span><span class="o">::</span><span class="n">createParser</span><span class="p">(</span><span class="o">*</span><span class="n">network</span><span class="p">,</span> <span class="n">gLogger</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 注意这里：使用TensorRT自带的InstanceNormalization实现
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">flag</span> <span class="o">=</span> <span class="mi">1U</span> <span class="o">&lt;&lt;</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">nvonnxparser</span><span class="o">::</span><span class="n">OnnxParserFlag</span><span class="o">::</span><span class="n">kNATIVE_INSTANCENORM</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">-&gt;</span><span class="n">setFlags</span><span class="p">(</span><span class="n">flag</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">model_path</span> <span class="o">=</span> <span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">parser</span><span class="o">-&gt;</span><span class="n">parseFromFile</span><span class="p">(</span><span class="n">model_path</span><span class="p">.</span><span class="n">c_str</span><span class="p">(),</span> <span class="mi">1</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Failed to parse the ONNX model.</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">engine</span> <span class="o">=</span> <span class="n">builder</span><span class="o">-&gt;</span><span class="n">buildEngineWithConfig</span><span class="p">(</span><span class="o">*</span><span class="n">network</span><span class="p">,</span> <span class="o">*</span><span class="n">config</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">engine</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Failed to create the TensorRT engine.</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">engine_path</span> <span class="o">=</span> <span class="n">model_path</span><span class="p">.</span><span class="n">substr</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">model_path</span><span class="p">.</span><span class="n">find_last_of</span><span class="p">(</span><span class="sc">&#39;.&#39;</span><span class="p">))</span> <span class="o">+</span> <span class="s">&#34;.trt&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Serializing the TensorRT engine to &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">engine_path</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;...</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">nvinfer1</span><span class="o">::</span><span class="n">IHostMemory</span> <span class="o">*</span><span class="n">serializedModel</span> <span class="o">=</span> <span class="n">engine</span><span class="o">-&gt;</span><span class="n">serialize</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">ofstream</span> <span class="n">engineFile</span><span class="p">(</span><span class="n">engine_path</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">ios</span><span class="o">::</span><span class="n">binary</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">engineFile</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="kt">char</span> <span class="o">*&gt;</span><span class="p">(</span><span class="n">serializedModel</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">()),</span> <span class="n">serializedModel</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="n">engineFile</span><span class="p">.</span><span class="n">close</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">serializedModel</span><span class="o">-&gt;</span><span class="n">destroy</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">-&gt;</span><span class="n">destroy</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">engine</span><span class="o">-&gt;</span><span class="n">destroy</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">config</span><span class="o">-&gt;</span><span class="n">destroy</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">network</span><span class="o">-&gt;</span><span class="n">destroy</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">builder</span><span class="o">-&gt;</span><span class="n">destroy</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>直接使用此代码进行模型转换;</p>
<h3 id="验证模型可行性--模型推理过程">验证模型可行性 &amp;&amp; 模型推理过程<a hidden class="anchor" aria-hidden="true" href="#验证模型可行性--模型推理过程">#</a></h3>
<p>这里直接写一个demo，来实现模型的推理过程，从而验证模型的可行性，这部分直接上代码：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">// tensorrt_image.cpp
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#include</span> <span class="cpf">&lt;NvInfer.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;opencv2/opencv.hpp&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;chrono&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;fstream&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#define TEST_TIME 0
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="c1">// 继承自 nvinfer1::ILogger 以便我们可以修改日志处理方式
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">class</span> <span class="nc">Logger</span> <span class="o">:</span> <span class="k">public</span> <span class="n">nvinfer1</span><span class="o">::</span><span class="n">ILogger</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="kt">void</span> <span class="n">log</span><span class="p">(</span><span class="n">Severity</span> <span class="n">severity</span><span class="p">,</span> <span class="k">const</span> <span class="kt">char</span> <span class="o">*</span><span class="n">msg</span><span class="p">)</span> <span class="k">noexcept</span> <span class="k">override</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Severity::kVERBOSE 或 kINFO
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// 只打印错误信息
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="n">severity</span> <span class="o">==</span> <span class="n">Severity</span><span class="o">::</span><span class="n">kERROR</span> <span class="o">&amp;&amp;</span> <span class="n">severity</span> <span class="o">==</span> <span class="n">Severity</span><span class="o">::</span><span class="n">kINTERNAL_ERROR</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">msg</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">gLogger</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">argc</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Please specify path to the engine file.</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 打开并反序列化 TensorRT engine
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">ifstream</span> <span class="n">engineFile</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">std</span><span class="o">::</span><span class="n">ios</span><span class="o">::</span><span class="n">binary</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">engineFile</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Error opening engine file</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">engineFile</span><span class="p">.</span><span class="n">seekg</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">engineFile</span><span class="p">.</span><span class="n">end</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="kt">long</span> <span class="kt">int</span> <span class="n">fsize</span> <span class="o">=</span> <span class="n">engineFile</span><span class="p">.</span><span class="n">tellg</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">engineFile</span><span class="p">.</span><span class="n">seekg</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">engineFile</span><span class="p">.</span><span class="n">beg</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">&gt;</span> <span class="n">engineData</span><span class="p">(</span><span class="n">fsize</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">engineFile</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="n">engineData</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">fsize</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">engineFile</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Error reading engine file</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">nvinfer1</span><span class="o">::</span><span class="n">IRuntime</span> <span class="o">*</span><span class="n">runtime</span> <span class="o">=</span> <span class="n">nvinfer1</span><span class="o">::</span><span class="n">createInferRuntime</span><span class="p">(</span><span class="n">gLogger</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">nvinfer1</span><span class="o">::</span><span class="n">ICudaEngine</span> <span class="o">*</span><span class="n">engine</span> <span class="o">=</span> <span class="n">runtime</span><span class="o">-&gt;</span><span class="n">deserializeCudaEngine</span><span class="p">(</span><span class="n">engineData</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">fsize</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 创建推理上下文
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">nvinfer1</span><span class="o">::</span><span class="n">IExecutionContext</span> <span class="o">*</span><span class="n">context</span> <span class="o">=</span> <span class="n">engine</span><span class="o">-&gt;</span><span class="n">createExecutionContext</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 读取并预处理图像
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">cv</span><span class="o">::</span><span class="n">Mat</span> <span class="n">img1</span> <span class="o">=</span> <span class="n">cv</span><span class="o">::</span><span class="n">imread</span><span class="p">(</span><span class="s">&#34;/home/metoak/Projects/dependency/InferenceDemo/images/left.png&#34;</span><span class="p">,</span> <span class="n">cv</span><span class="o">::</span><span class="n">IMREAD_COLOR</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cv</span><span class="o">::</span><span class="n">Mat</span> <span class="n">img2</span> <span class="o">=</span> <span class="n">cv</span><span class="o">::</span><span class="n">imread</span><span class="p">(</span><span class="s">&#34;/home/metoak/Projects/dependency/InferenceDemo/images/right.png&#34;</span><span class="p">,</span> <span class="n">cv</span><span class="o">::</span><span class="n">IMREAD_COLOR</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cv</span><span class="o">::</span><span class="n">resize</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span> <span class="n">img1</span><span class="p">,</span> <span class="n">cv</span><span class="o">::</span><span class="n">Size</span><span class="p">(</span><span class="mi">640</span><span class="p">,</span> <span class="mi">384</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">cv</span><span class="o">::</span><span class="n">resize</span><span class="p">(</span><span class="n">img2</span><span class="p">,</span> <span class="n">img2</span><span class="p">,</span> <span class="n">cv</span><span class="o">::</span><span class="n">Size</span><span class="p">(</span><span class="mi">640</span><span class="p">,</span> <span class="mi">384</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">img1</span><span class="p">.</span><span class="n">convertTo</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span> <span class="n">CV_32FC3</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">img2</span><span class="p">.</span><span class="n">convertTo</span><span class="p">(</span><span class="n">img2</span><span class="p">,</span> <span class="n">CV_32FC3</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 创建输入数据
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int</span> <span class="n">num_elements_input</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">384</span> <span class="o">*</span> <span class="mi">640</span><span class="p">;</span> <span class="c1">// 根据你的模型的实际需求进行调整
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 分割通道
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">cv</span><span class="o">::</span><span class="n">Mat</span><span class="o">&gt;</span> <span class="n">channels1</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">cv</span><span class="o">::</span><span class="n">Mat</span><span class="o">&gt;</span> <span class="n">channels2</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cv</span><span class="o">::</span><span class="n">split</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span> <span class="n">channels1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cv</span><span class="o">::</span><span class="n">split</span><span class="p">(</span><span class="n">img2</span><span class="p">,</span> <span class="n">channels2</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">data1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">data2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 将每个通道的数据添加到相应的向量
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="o">&amp;</span><span class="nl">channel</span> <span class="p">:</span> <span class="n">channels1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">channelData</span><span class="p">(</span><span class="n">channel</span><span class="p">.</span><span class="n">begin</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(),</span> <span class="n">channel</span><span class="p">.</span><span class="n">end</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">        <span class="n">data1</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">data1</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="n">channelData</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">channelData</span><span class="p">.</span><span class="n">end</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="o">&amp;</span><span class="nl">channel</span> <span class="p">:</span> <span class="n">channels2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">channelData</span><span class="p">(</span><span class="n">channel</span><span class="p">.</span><span class="n">begin</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(),</span> <span class="n">channel</span><span class="p">.</span><span class="n">end</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">        <span class="n">data2</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">data2</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="n">channelData</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">channelData</span><span class="p">.</span><span class="n">end</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#if TEST_TIME == 1
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="c1">// 打印图像的前10个像素值
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;First 10 pixels of the input image:</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">data1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">&#34; &#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 转换回图像
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">cv</span><span class="o">::</span><span class="n">Mat</span><span class="o">&gt;</span> <span class="n">channels</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">;</span> <span class="o">++</span><span class="n">c</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">channels</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">cv</span><span class="o">::</span><span class="n">Mat</span><span class="p">(</span><span class="mi">384</span><span class="p">,</span> <span class="mi">640</span><span class="p">,</span> <span class="n">CV_32F</span><span class="p">,</span> <span class="n">data1</span><span class="p">.</span><span class="n">data</span><span class="p">()</span> <span class="o">+</span> <span class="n">c</span> <span class="o">*</span> <span class="mi">384</span> <span class="o">*</span> <span class="mi">640</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">cv</span><span class="o">::</span><span class="n">Mat</span> <span class="n">imgData1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cv</span><span class="o">::</span><span class="n">merge</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">imgData1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 归一化并将数据类型转换回 CV_8UC3 以便显示
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">imgData1</span> <span class="o">=</span> <span class="n">imgData1</span> <span class="o">*</span> <span class="mf">255.0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">imgData1</span><span class="p">.</span><span class="n">convertTo</span><span class="p">(</span><span class="n">imgData1</span><span class="p">,</span> <span class="n">CV_8UC3</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cv</span><span class="o">::</span><span class="n">imwrite</span><span class="p">(</span><span class="s">&#34;/home/metoak/Projects/input.png&#34;</span><span class="p">,</span> <span class="n">imgData1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 分配 GPU 内存用于输入
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">void</span> <span class="o">*</span><span class="n">gpuData1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">void</span> <span class="o">*</span><span class="n">gpuData2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpuData1</span><span class="p">,</span> <span class="n">num_elements_input</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpuData2</span><span class="p">,</span> <span class="n">num_elements_input</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 复制输入数据到 GPU
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">gpuData1</span><span class="p">,</span> <span class="n">data1</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">num_elements_input</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">gpuData2</span><span class="p">,</span> <span class="n">data2</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">num_elements_input</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 根据你的模型的输出大小来创建输出数据
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int</span> <span class="n">num_elements_output</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">384</span> <span class="o">*</span> <span class="mi">640</span><span class="p">;</span> <span class="c1">// 你需要根据你的模型的实际输出大小进行调整
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 分配 GPU 内存用于输出
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">void</span> <span class="o">*</span><span class="n">gpuOutput</span><span class="p">[</span><span class="mi">7</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">7</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpuOutput</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">num_elements_output</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 创建输入输出的指针数组
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">void</span> <span class="o">*</span><span class="n">bindings</span><span class="p">[</span><span class="mi">9</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="n">gpuData1</span><span class="p">,</span> <span class="n">gpuData2</span><span class="p">,</span> <span class="n">gpuOutput</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">gpuOutput</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">gpuOutput</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">gpuOutput</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">gpuOutput</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">gpuOutput</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">gpuOutput</span><span class="p">[</span><span class="mi">6</span><span class="p">]};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 创建 CUDA stream
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">cudaStream_t</span> <span class="n">stream</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#if TEST_TIME == 1
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="c1">// 100次推理
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">cudaEvent_t</span> <span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">start</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stop</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">totalTime</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">iteration</span> <span class="o">=</span> <span class="mi">100</span><span class="p">;</span> <span class="c1">// 推理次数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">iteration</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">stream</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// 执行推理
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">context</span><span class="o">-&gt;</span><span class="n">enqueue</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">bindings</span><span class="p">,</span> <span class="n">stream</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Inference iteration: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">i</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">stop</span><span class="p">,</span> <span class="n">stream</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">cudaEventSynchronize</span><span class="p">(</span><span class="n">stop</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="kt">float</span> <span class="n">milliseconds</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">cudaEventElapsedTime</span><span class="p">(</span><span class="o">&amp;</span><span class="n">milliseconds</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">totalTime</span> <span class="o">+=</span> <span class="n">milliseconds</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">avgTime</span> <span class="o">=</span> <span class="n">totalTime</span> <span class="o">/</span> <span class="n">iteration</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Average inference time per image: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">avgTime</span> <span class="o">&lt;&lt;</span> <span class="s">&#34; ms.&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">cudaEventDestroy</span><span class="p">(</span><span class="n">start</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaEventDestroy</span><span class="p">(</span><span class="n">stop</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="cp">#else
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="c1">// 执行推理
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">context</span><span class="o">-&gt;</span><span class="n">enqueue</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">bindings</span><span class="p">,</span> <span class="n">stream</span><span class="p">,</span> <span class="k">nullptr</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 将需要的输出数据从 GPU 内存复制回 CPU 内存
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">outputData</span><span class="p">(</span><span class="n">num_elements_output</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">outputData</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">gpuOutput</span><span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="n">num_elements_output</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="cp">#if TEST_TIME == 1
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="c1">// 打印图像的前10个像素值
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;First 10 pixels of the output image:</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">outputData</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">&#34; &#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>    <span class="c1">// 将一维向量转换为3通道图像
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">cv</span><span class="o">::</span><span class="n">Mat</span> <span class="n">img_out</span><span class="p">(</span><span class="mi">384</span><span class="p">,</span> <span class="mi">640</span><span class="p">,</span> <span class="n">CV_32F</span><span class="p">,</span> <span class="n">outputData</span><span class="p">.</span><span class="n">data</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="n">cv</span><span class="o">::</span><span class="n">Mat</span> <span class="n">img_out_normalized</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cv</span><span class="o">::</span><span class="n">normalize</span><span class="p">(</span><span class="n">img_out</span><span class="p">,</span> <span class="n">img_out_normalized</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="n">cv</span><span class="o">::</span><span class="n">NORM_MINMAX</span><span class="p">,</span> <span class="n">CV_8U</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cv</span><span class="o">::</span><span class="n">Mat</span> <span class="n">img_color_mapped</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cv</span><span class="o">::</span><span class="n">applyColorMap</span><span class="p">(</span><span class="n">img_out_normalized</span><span class="p">,</span> <span class="n">img_color_mapped</span><span class="p">,</span> <span class="n">cv</span><span class="o">::</span><span class="n">COLORMAP_JET</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// cv::resize(img_out, img_out, cv::Size(1920, 1080));
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 保存图像
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">cv</span><span class="o">::</span><span class="n">imwrite</span><span class="p">(</span><span class="s">&#34;/home/metoak/Projects/output.png&#34;</span><span class="p">,</span> <span class="n">img_color_mapped</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Output image size: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">img_out_normalized</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 清理
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">cudaFree</span><span class="p">(</span><span class="n">gpuData1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaFree</span><span class="p">(</span><span class="n">gpuData2</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">7</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">cudaFree</span><span class="p">(</span><span class="n">gpuOutput</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">context</span><span class="o">-&gt;</span><span class="n">destroy</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">engine</span><span class="o">-&gt;</span><span class="n">destroy</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">runtime</span><span class="o">-&gt;</span><span class="n">destroy</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 销毁 CUDA stream
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">cudaStreamDestroy</span><span class="p">(</span><span class="n">stream</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>这里有几点注意：</p>
<ol>
<li>
<p>数据的前处理需要根据的模型的结构来调整，这里是输入两组图像</p>
<ul>
<li>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="c1">// 分配 GPU 内存用于输入
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">void</span> <span class="o">*</span><span class="n">gpuData1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">void</span> <span class="o">*</span><span class="n">gpuData2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpuData1</span><span class="p">,</span> <span class="n">num_elements_input</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpuData2</span><span class="p">,</span> <span class="n">num_elements_input</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 复制输入数据到 GPU
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">gpuData1</span><span class="p">,</span> <span class="n">data1</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">num_elements_input</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">gpuData2</span><span class="p">,</span> <span class="n">data2</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">num_elements_input</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 根据你的模型的输出大小来创建输出数据
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int</span> <span class="n">num_elements_output</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">384</span> <span class="o">*</span> <span class="mi">640</span><span class="p">;</span> <span class="c1">// 你需要根据你的模型的实际输出大小进行调整
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 分配 GPU 内存用于输出
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">void</span> <span class="o">*</span><span class="n">gpuOutput</span><span class="p">[</span><span class="mi">7</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">7</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">gpuOutput</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">num_elements_output</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>这一块就是根据模型的结构来调整输入和输出的</p>
</li>
</ul>
</li>
<li>
<p>这里加入了很多手工调试的信息，比如保存模型输入前后的图像来验证</p>
</li>
<li>
<p>这是一段完整的推理代码，如果这一段代码能够成功运行，那么将这段代码整合到实际的功能中，定义好输入输出的接口，模型也一样可以正常使用</p>
</li>
<li>
<p>这里很多地方是手动申请内存空间的，所以一定注意使用后的销毁（当然也可以使用类来实现析构这种方法）</p>
<ul>
<li>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl">    <span class="c1">// 清理
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">cudaFree</span><span class="p">(</span><span class="n">gpuData1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaFree</span><span class="p">(</span><span class="n">gpuData2</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">7</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">cudaFree</span><span class="p">(</span><span class="n">gpuOutput</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">context</span><span class="o">-&gt;</span><span class="n">destroy</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">engine</span><span class="o">-&gt;</span><span class="n">destroy</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">runtime</span><span class="o">-&gt;</span><span class="n">destroy</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 销毁 CUDA stream
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">cudaStreamDestroy</span><span class="p">(</span><span class="n">stream</span><span class="p">);</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>注意理解这一部分的功能和必要性</p>
</li>
</ul>
</li>
</ol>
<p>有了这两部分后，模型从ONNX到TensorRT，再到TensorRT的推理就打通了！</p>
<p>对于上面的代码编译，直接使用cmake的方法，CMakeLists.txt例子如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cmake" data-lang="cmake"><span class="line"><span class="cl"><span class="nb">cmake_minimum_required</span><span class="p">(</span><span class="s">VERSION</span> <span class="s">3.10</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">project</span><span class="p">(</span><span class="s">OnnxInferenceDemo</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># 调试模式
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="nb">set</span><span class="p">(</span><span class="s">CMAKE_BUILD_TYPE</span> <span class="s">Debug</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">set</span><span class="p">(</span><span class="s">CMAKE_CXX_STANDARD</span> <span class="s">14</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># 设置METOAK的驱动路径
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="nb">set</span><span class="p">(</span><span class="s">MO_SDK_DIR</span> <span class="s">/opt/moak</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">find_package</span><span class="p">(</span><span class="s">OpenCV</span> <span class="s">REQUIRED</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">find_package</span><span class="p">(</span><span class="s">PCL</span> <span class="s">REQUIRED</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">find_package</span><span class="p">(</span><span class="s">CUDA</span> <span class="s">REQUIRED</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">find_path</span><span class="p">(</span><span class="s">TENSORRT_INCLUDE_DIR</span> <span class="s">NvInfer.h</span>
</span></span><span class="line"><span class="cl">          <span class="s">HINTS</span> <span class="o">${</span><span class="nv">TENSORRT_ROOT_DIR</span><span class="o">}</span>
</span></span><span class="line"><span class="cl">          <span class="s">PATH_SUFFIXES</span> <span class="s">include/</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">find_library</span><span class="p">(</span><span class="s">TENSORRT_LIBRARY</span> <span class="s">nvinfer</span>
</span></span><span class="line"><span class="cl">             <span class="s">HINTS</span> <span class="o">${</span><span class="nv">TENSORRT_ROOT_DIR</span><span class="o">}</span>
</span></span><span class="line"><span class="cl">             <span class="s">PATH_SUFFIXES</span> <span class="s">lib</span> <span class="s">lib64</span> <span class="s">lib/x64</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">find_library</span><span class="p">(</span><span class="s">TENSORRT_ONNX_LIBRARY</span> <span class="s">nvonnxparser</span>
</span></span><span class="line"><span class="cl">             <span class="s">HINTS</span> <span class="o">${</span><span class="nv">TENSORRT_ROOT_DIR</span><span class="o">}</span>
</span></span><span class="line"><span class="cl">             <span class="s">PATH_SUFFIXES</span> <span class="s">lib</span> <span class="s">lib64</span> <span class="s">lib/x64</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">set</span><span class="p">(</span><span class="s">TENSORRT_INCLUDE_DIRS</span> <span class="o">${</span><span class="nv">TENSORRT_INCLUDE_DIR</span><span class="o">}</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">set</span><span class="p">(</span><span class="s">TENSORRT_LIBRARIES</span> <span class="o">${</span><span class="nv">TENSORRT_LIBRARY</span><span class="o">}</span> <span class="o">${</span><span class="nv">TENSORRT_ONNX_LIBRARY</span><span class="o">}</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">include_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">TENSORRT_INCLUDE_DIRS</span><span class="o">}</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">include_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">CUDA_INCLUDE_DIRS</span><span class="o">}</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">include_directories</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="s">include</span>
</span></span><span class="line"><span class="cl">    <span class="o">${</span><span class="nv">OpenCV_INCLUDE_DIRS</span><span class="o">}</span>
</span></span><span class="line"><span class="cl">    <span class="o">${</span><span class="nv">MO_SDK_DIR</span><span class="o">}</span><span class="s">/3rdparty/ONNX/x86_64/GPU/include</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">link_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">MO_SDK_DIR</span><span class="o">}</span><span class="s">/3rdparty/ONNX/x86_64/GPU/lib</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">link_directories</span><span class="p">(</span><span class="s">lib</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">add_executable</span><span class="p">(</span><span class="s">onnx_to_tensorrt</span> <span class="s">src/onnx_to_tensorrt.cpp</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">onnx_to_tensorrt</span> <span class="o">${</span><span class="nv">TENSORRT_LIBRARIES</span><span class="o">}</span> <span class="o">${</span><span class="nv">CUDA_LIBRARIES</span><span class="o">}</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">add_executable</span><span class="p">(</span><span class="s">tensorrt_image</span> <span class="s">src/tensorrt_image.cpp</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">tensorrt_image</span> <span class="o">${</span><span class="nv">OpenCV_LIBS</span><span class="o">}</span> <span class="o">${</span><span class="nv">TENSORRT_LIBRARIES</span><span class="o">}</span> <span class="o">${</span><span class="nv">CUDA_LIBRARIES</span><span class="o">}</span><span class="p">)</span><span class="err">
</span></span></span></code></pre></td></tr></table>
</div>
</div><h2 id="总结">总结<a hidden class="anchor" aria-hidden="true" href="#总结">#</a></h2>
<p>使用TensorRT来进行模型加速，如果很凑巧，当前版本无法支持Onnx的模型转换，那可能正常方法就无法使用，就比如这里的模型，在TensorRT 8.6之前的版本，都无法完成正常的转换，并且报出段错误异常，这种情况，可能就是无法支持；</p>
<p><strong>而使用了最新版本发现，模型可以转换，直接无法直接进行模型的推理，那么就要排查一下是哪里的问题了，比如这里的模型，在TensorRT 8.6.1上可以完整转换，但是需要手动指定使用的插件，内建的还是其他的，这就会影响模型对应的推理程序的书写。</strong></p>
<p>另外一点：</p>
<p>TensorRT的安装和环境部署其实很简单，但是因为网络上的搜索出来的教程杂七杂八，导致没有按照统一的方式来安装CUDA、cuDNN、TensorRT，导致apt安装时出现依赖异常，其实只要全部采用dep包管理的方式来安装就不会有问题。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://ahaknow.com/tags/%E7%BC%96%E7%A8%8B%E6%97%A5%E5%B8%B8/">编程日常</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://ahaknow.com/posts/diary/0906%E4%BB%8E%E6%80%A7%E5%AF%92%E6%83%B3%E8%B5%B7/">
    <span class="title">« 上一页</span>
    <br>
    <span>0906 从“性寒”想起……</span>
  </a>
  <a class="next" href="http://ahaknow.com/posts/diary/0904%E4%B9%A0%E6%83%AF%E4%B8%8E%E6%89%93%E7%A0%B4%E4%B9%A0%E6%83%AF/">
    <span class="title">下一页 »</span>
    <br>
    <span>0904 习惯与打破习惯</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="http://ahaknow.com">AhaKnow</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/IHKYoung/CKPaper.git/" rel="noopener" target="_blank">CKPaper</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = '🖨️拷贝';

        function copyingDone() {
            copybutton.innerHTML = '📋已拷贝';
            setTimeout(() => {
                copybutton.innerHTML = '🖨️拷贝';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
