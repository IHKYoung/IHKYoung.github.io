[{"content":"第一步：先测试模型的基准速度 使用ONNX测试模型运行速度基准 目标： 使用ONNX Runtime来测试机器学习模型的推理时间基准。\n步骤：\n安装ONNX和ONNX Runtime： 这两个Python库是测试基准的关键工具。ONNX是一个开放的模型表示标准，它允许在不同的机器学习框架之间移动模型。ONNX Runtime则是一个用于运行ONNX模型的高性能推理引擎。可以通过pip进行安装，命令为pip install onnx onnxruntime-gpu。这里的onnxruntime-gpu版本是支持GPU的ONNX Runtime。\n加载模型： 使用ONNX Runtime加载模型，具体做法是创建一个onnxruntime.InferenceSession的实例，并指定模型的文件路径和推理引擎提供商（这里选择的是'CUDAExecutionProvider'，即使用CUDA来利用GPU加速）。\n获取输入信息并创建输入数据： 需要知道模型的输入节点的名称和形状，这可以通过session.get_inputs()来获取。有了这些信息后，就可以创建与之匹配的随机输入数据。\n推理并计时： 进行100次模型推理，并使用time.time()来计时。注意，这里忽略了第一次推理的时间，因为第一次推理可能会包含一些初始化操作，导致耗时偏大。\n计算平均推理时间： 通过将所有推理的时间加起来，然后除以推理次数，就得到了平均推理时间。\n问题和解决：\n如何创建匹配模型输入的数据： 模型的输入可能有多个，也可能有各种各样的形状和类型。需要通过session.get_inputs()获取这些信息，然后根据这些信息创建相应的输入数据。\n如何确保使用了GPU： 需要在创建onnxruntime.InferenceSession时明确指定使用'CUDAExecutionProvider'。如果没有正确地使用GPU，可能会导致推理速度大幅度降低。\n如何准确计时： 选择忽略了第一次推理的时间，因为第一次推理可能会包含一些初始化操作，导致耗时偏大。\nPython脚本代码 import numpy as np import onnxruntime import time # 加载模型 model_path = \"/home/metoak/Projects/ros_project/src/metoak_camera_driver/models/model_640_384_6125.onnx\" # session = onnxruntime.InferenceSession(model_path) session = onnxruntime.InferenceSession(model_path, providers=['CUDAExecutionProvider']) # 获取输入名称和形状 input_names = [inp.name for inp in session.get_inputs()] input_shapes = [inp.shape for inp in session.get_inputs()] print(\"Input names:\", input_names) print(\"Input shapes:\", input_shapes) # 创建两个随机输入 input1 = np.random.randint(0, 256, size=input_shapes[0]).astype(np.uint8) input2 = np.random.randint(0, 256, size=input_shapes[1]).astype(np.uint8) # 准备输入数据 inputs = {input_names[0]: input1, input_names[1]: input2} # 推理次数 num_inferences = 100 times = [] # 执行推断并计时 for i in range(num_inferences): start = time.time() output = session.run(None, inputs) end = time.time() inference_time = end - start if i == 0: continue times.append(inference_time) print(f\"Inference {i+1}, Time: {inference_time * 1000} ms\") # 计算平均推断时间 avg_time = sum(times) / (num_inferences - 1) # 打印平均推断时间 print(f\"Average inference time: {avg_time * 1000} ms\") 模型速度基准 Model name: model_640_384_6125.onnx Input names: [\u0026#39;onnx::Cast_0\u0026#39;, \u0026#39;onnx::Cast_1\u0026#39;] Input shapes: [[1, 3, 384, 640], [1, 3, 384, 640]] Inference 2, Time: 299.41678047180176 ms Inference 3, Time: 306.2708377838135 ms ... Inference 98, Time: 332.33070373535156 ms Inference 99, Time: 317.1732425689697 ms Inference 100, Time: 333.3919048309326 ms Average inference time: 322.4095816564078 ms 第二步：使用TensorRT实现加速 如果使用的3080Ti显卡\n浮点计算能力：\nSingle Precision Perf.: 34.1 TFLOPS\nTensor Perf. (FP16): 136 TFLOPS\nTensor Perf. (FP16-Sparse): ==273 TFLOPS==\n如果使用Tensor来处理全部精度为16位浮点数的算法，那么理论的运算速率可以达到至少是单精度浮点运算的 136 / 34.1 = 3.99，也就是4倍速度的提升，而如果模型本身就是混合进度进行运算的，那么提升的效果会更大！（如果再考虑上稀疏优化，那么Tensor加速的倍数会更高，达到 273 / 34.1 = 8倍的提升）\n🌟TensorRT环境部署 安装方法对了，安装其实很简单，一句话说明就是：\n==cuda、tensorrt都使用dep包，然后使用apt安装==\n但是细节很关键！下面的过程涉及安装CUDA、cuDNN、TensorRT\n官方文档\n1. 安装CUDA CUDA和Nvida Driver这俩个是独立不影响的，所以单独安装CUDA版本即可，与Nvida Driver版本无关，比如安装==cuda 11.8==\nUbuntu20.04:\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda-repo-ubuntu2004-11-8-local_11.8.0-520.61.05-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu2004-11-8-local_11.8.0-520.61.05-1_amd64.deb sudo cp /var/cuda-repo-ubuntu2004-11-8-local/cuda-*-keyring.gpg /usr/share/keyrings/ sudo apt-get update sudo apt-get -y install cuda 如果使用了runfile的方式安装过cuda了，那么最后一步只需要：\nsudo apt install cuda-libraries-11-8 2. 安装cuDNN 需要注册Nvidia开发者账号，下载对应的CUDA版本的即可，使用如下方式安装：\n官网\n下载deb版本即可\nsudo dpkg -i cudnn-local-repo-ubuntu2004-8.9.2.26_1.0-1_amd64.deb sudo cp /var/cudnn-local-repo-ubuntu2004-8.9.2.26/cudnn-local-6D0A7AE1-keyring.gpg /usr/share/keyrings/ sudo apt update sudo apt-get install libcudnn8 3. 安装TensorRT 官网\n下载最新版本TensorRT 8，同样需要开发者账号；\n下载对应系统的对应CUDA的最新版即可\nsudo dpkg -i nv-tensorrt-local-repo-ubuntu2004-8.6.1-cuda-11.8_1.0-1_amd64.deb sudo cp /var/nv-tensorrt-local-repo-ubuntu2004-8.6.1-cuda-11.8_1.0-1/*-keyring.gpg /usr/share/keyrings/ sudo apt update sudo apt install tensorrt 使用trtexec工具实现模型转换 # 使用例子 trtexec --onnx=/home/metoak/Projects/ros_project/src/metoak_camera_driver/models/model_sim_1280_704.onnx --saveEngine=model_1280_704_fp16.engine --fp16 --avgRuns=100 可以使用trtexec这个工具来直接转换onnx模型，但实际可能不理想或者遇到一些问题：\nonnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32. [07/25/2023-10:29:30] [W] [TRT] onnx2trt_utils.cpp:400: One or more weights outside the range of INT32 was clamped [07/25/2023-10:29:30] [I] Finished parsing network model. Parse time: 1.3651 [07/25/2023-10:29:30] [I] [TRT] Graph optimization time: 0.282094 seconds. [07/25/2023-10:29:30] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2630, GPU 3661 (MiB) [07/25/2023-10:29:30] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 2630, GPU 3671 (MiB) [07/25/2023-10:29:30] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\nYour ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.: 这个警告表明你的ONNX模型中包含了INT64类型的权重，而TensorRT原生不支持INT64类型。TensorRT正在尝试将这些权重转换（cast）为INT32类型。这可能会导致一些精度损失，因为INT32的范围小于INT64。\nOne or more weights outside the range of INT32 was clamped: 这个警告表明在转换权重类型的过程中，有一个或多个权重的值超出了INT32类型的范围，并被压缩（clamp）到了INT32的范围内。这也可能会导致一些精度损失。\nFinished parsing network model. Parse time: 1.3651: 这表示TensorRT已经完成了对ONNX模型的解析，整个解析过程花费了1.3651秒。\nGraph optimization time: 0.282094 seconds: 这表示TensorRT对模型进行优化的时间是0.282094秒。\nInit cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2630, GPU 3661 (MiB) 和 Init cuDNN: CPU +0, GPU +10, now: CPU 2630, GPU 3671 (MiB): 这些行是关于TensorRT初始化cuBLAS和cuDNN库（这些库用于GPU加速的线性代数运算和深度神经网络计算）所需内存的信息。这些行显示了在初始化这些库后，CPU和GPU内存的使用情况。\nLocal timing cache in use. Profiling results in this builder pass will not be stored.: 这表示TensorRT正在使用本地的计时缓存进行操作，而这次的分析结果不会被存储。\n使用trtexec来转换模型，一般情况下，得到下面的输出时，会成功，但也有可能得到模型无法正常使用：\n[07/25/2023-14:44:13] [I] Starting inference [07/25/2023-14:44:16] [I] Warmup completed 12 queries over 200 ms [07/25/2023-14:44:16] [I] Timing trace has 180 queries over 3.05014 s [07/25/2023-14:44:16] [I] [07/25/2023-14:44:16] [I] === Trace details === [07/25/2023-14:44:16] [I] Trace averages of 100 runs: [07/25/2023-14:44:16] [I] Average on 100 runs - GPU latency: 16.9045 ms - Host latency: 17.8527 ms (enqueue 11.5822 ms) [07/25/2023-14:44:16] [I] [07/25/2023-14:44:16] [I] === Performance summary === [07/25/2023-14:44:16] [I] Throughput: 59.0137 qps [07/25/2023-14:44:16] [I] Latency: min = 14.8851 ms, max = 27.6843 ms, mean = 17.7984 ms, median = 16.9878 ms, percentile(90%) = 21.1488 ms, percentile(95%) = 23.4043 ms, percentile(99%) = 27.5272 ms [07/25/2023-14:44:16] [I] Enqueue Time: min = 3.59753 ms, max = 22.9529 ms, mean = 11.5029 ms, median = 10.7087 ms, percentile(90%) = 13.9353 ms, percentile(95%) = 16.418 ms, percentile(99%) = 20.6213 ms [07/25/2023-14:44:16] [I] H2D Latency: min = 0.0994873 ms, max = 0.790527 ms, mean = 0.303106 ms, median = 0.268311 ms, percentile(90%) = 0.458588 ms, percentile(95%) = 0.503906 ms, percentile(99%) = 0.564697 ms [07/25/2023-14:44:16] [I] GPU Compute Time: min = 14.0032 ms, max = 26.499 ms, mean = 16.8485 ms, median = 16.0006 ms, percentile(90%) = 19.6926 ms, percentile(95%) = 22.436 ms, percentile(99%) = 26.4436 ms [07/25/2023-14:44:16] [I] D2H Latency: min = 0.319824 ms, max = 1.09839 ms, mean = 0.646768 ms, median = 0.621216 ms, percentile(90%) = 0.887512 ms, percentile(95%) = 0.963013 ms, percentile(99%) = 1.05774 ms [07/25/2023-14:44:16] [I] Total Host Walltime: 3.05014 s [07/25/2023-14:44:16] [I] Total GPU Compute Time: 3.03273 s [07/25/2023-14:44:16] [W] * GPU compute time is unstable, with coefficient of variance = 15.1765%. [07/25/2023-14:44:16] [W] If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability. [07/25/2023-14:44:16] [I] Explanations of the performance metrics are printed in the verbose logs. [07/25/2023-14:44:16] [I] \u0026amp;\u0026amp;\u0026amp;\u0026amp; PASSED TensorRT.trtexec [TensorRT v8601] # /usr/src/tensorrt/bin/trtexec --onnx=/home/metoak/Projects/ros_project/src/metoak_camera_driver/models/model_640_384_6125.onnx --saveEngine=model_640_384.engine --fp16 --avgRuns=100 这是通过使用TensorRT工具trtexec的一段测试输出，下面是每个重要部分的解释：\n[I] Warmup completed 12 queries over 200 ms：在开始正式计时之前，trtexec首先执行了12个预热查询，总耗时200毫秒。\n[I] Timing trace has 180 queries over 3.05014 s：这表示运行了180次推断，总耗时约为3.05秒。\n[I] Average on 100 runs - GPU latency: 16.9045 ms - Host latency: 17.8527 ms：这表示在100次运行中，平均每次推断在GPU上的延迟是16.9045毫秒，在主机上的延迟是17.8527毫秒。\n[I] Throughput: 59.0137 qps：这表示模型的吞吐量为59.0137查询每秒（qps）。\n[I] Latency: min = 14.8851 ms, max = 27.6843 ms, mean = 17.7984 ms, median = 16.9878 ms：这是对延迟的一些统计描述，包括最小值、最大值、平均值和中位数。\n[W] * GPU compute time is unstable, with coefficient of variance = 15.1765%.：这是一个警告，表示GPU计算时间的变化是不稳定的，方差系数为15.1765%，可能需要通过锁定GPU时钟频率或使用--useSpinWait选项来改善这种情况。\n\u0026amp;\u0026amp;\u0026amp;\u0026amp; PASSED TensorRT.trtexec [TensorRT v8601] # /usr/src/tensorrt/bin/trtexec --onnx=/home/metoak/Projects/ros_project/src/metoak_camera_driver/models/model_640_384_6125.onnx --saveEngine=model_640_384.engine --fp16 --avgRuns=100：这表示trtexec测试通过了，并且显示了你用来运行trtexec的命令行参数。\n验证engine模型是否可用？ import tensorrt as trt TRT_LOGGER = trt.Logger(trt.Logger.WARNING) trt_runtime = trt.Runtime(TRT_LOGGER) engine_path = \u0026#34;/home/metoak/Projects/models/model_resnet101.engine\u0026#34; with open(engine_path, \u0026#34;rb\u0026#34;) as f: engine_data = f.read() engine = trt_runtime.deserialize_cuda_engine(engine_data) print(\u0026#34;Number of bindings: \u0026#34;, engine.num_bindings) for i in range(engine.num_bindings): binding_name = engine.get_tensor_name(i) print(\u0026#34;Binding \u0026#34;, i, \u0026#34;:\u0026#34;, \u0026#34;Binding name: \u0026#34;, binding_name, \u0026#34;Binding shape: \u0026#34;, engine.get_tensor_shape(binding_name), \u0026#34;Binding data type: \u0026#34;, engine.get_tensor_dtype(binding_name), \u0026#34;Is binding an input? \u0026#34;, engine.get_tensor_mode(binding_name) == trt.TensorIOMode.INPUT) print(\u0026#34;Number of layers: \u0026#34;, engine.num_layers) 如果所有的打印输出都正常，那么说明转换的模型可以，并且以正常方法进行推理也是可行的\n之后就是将模型推理部分整合进入项目代码即可。\n第三步：一般的方法不行怎么办？（‼️非常关键） 使用trtexec提供的转换模型方法，无法直接使用\n原因分析：模型比较复杂，TensorRT无法支持 —— 这个问题在安装TensorRT最新版之后解决，但TensorRT增加了新特征（使用第三方插件），无法直接使用trtexec转换后的模型：\nIn TensorRT 8.6 there are two implementations of InstanceNormalization that may perform differently depending on various parameters. By default the parser will insert an InstanceNormalization plugin layer as it performs best for general use cases. Users that want to benchmark using the native TensorRT implementation of InstanceNorm can set the parser flag kNATIVE_INSTANCENORM prior to parsing the model. For building version compatible or hardware compatible engines, this flag must be set.\n使用python脚本来处理，debug功能较弱，尝试使用C++的代码来重写整个工具链\n使用新版本特性来转换Onnx 直接上代码：\n// Usage: ./onnx_to_tensorrt onnx_path #include \u0026lt;NvInfer.h\u0026gt; #include \u0026lt;NvOnnxParser.h\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;iostream\u0026gt; // 继承自 nvinfer1::ILogger 以便我们可以修改日志处理方式 class Logger : public nvinfer1::ILogger { public: void log(Severity severity, const char *msg) noexcept override { // 调试完整信息 if (severity != Severity::kINFO) std::cout \u0026lt;\u0026lt; msg \u0026lt;\u0026lt; std::endl; } } gLogger; #include \u0026lt;fstream\u0026gt; int main(int argc, char *argv[]) { if (argc \u0026lt; 2) { std::cerr \u0026lt;\u0026lt; \u0026#34;Please specify path to the ONNX model.\\n\u0026#34;; return 1; } auto builder = nvinfer1::createInferBuilder(gLogger); const auto explicitBatch = 1U \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(nvinfer1::NetworkDefinitionCreationFlag::kEXPLICIT_BATCH); auto network = builder-\u0026gt;createNetworkV2(explicitBatch); auto config = builder-\u0026gt;createBuilderConfig(); auto parser = nvonnxparser::createParser(*network, gLogger); // 注意这里：使用TensorRT自带的InstanceNormalization实现 auto flag = 1U \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(nvonnxparser::OnnxParserFlag::kNATIVE_INSTANCENORM); parser-\u0026gt;setFlags(flag); std::string model_path = argv[1]; if (!parser-\u0026gt;parseFromFile(model_path.c_str(), 1)) { std::cerr \u0026lt;\u0026lt; \u0026#34;Failed to parse the ONNX model.\\n\u0026#34;; return 1; } auto engine = builder-\u0026gt;buildEngineWithConfig(*network, *config); if (!engine) { std::cerr \u0026lt;\u0026lt; \u0026#34;Failed to create the TensorRT engine.\\n\u0026#34;; return 1; } std::string engine_path = model_path.substr(0, model_path.find_last_of(\u0026#39;.\u0026#39;)) + \u0026#34;.trt\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34;Serializing the TensorRT engine to \u0026#34; \u0026lt;\u0026lt; engine_path \u0026lt;\u0026lt; \u0026#34;...\\n\u0026#34;; nvinfer1::IHostMemory *serializedModel = engine-\u0026gt;serialize(); std::ofstream engineFile(engine_path, std::ios::binary); engineFile.write(reinterpret_cast\u0026lt;const char *\u0026gt;(serializedModel-\u0026gt;data()), serializedModel-\u0026gt;size()); engineFile.close(); serializedModel-\u0026gt;destroy(); parser-\u0026gt;destroy(); engine-\u0026gt;destroy(); config-\u0026gt;destroy(); network-\u0026gt;destroy(); builder-\u0026gt;destroy(); return 0; } 直接使用此代码进行模型转换;\n验证模型可行性 \u0026amp;\u0026amp; 模型推理过程 这里直接写一个demo，来实现模型的推理过程，从而验证模型的可行性，这部分直接上代码：\n// tensorrt_image.cpp #include \u0026lt;NvInfer.h\u0026gt; #include \u0026lt;opencv2/opencv.hpp\u0026gt; #include \u0026lt;chrono\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;fstream\u0026gt; #define TEST_TIME 0 // 继承自 nvinfer1::ILogger 以便我们可以修改日志处理方式 class Logger : public nvinfer1::ILogger { public: void log(Severity severity, const char *msg) noexcept override { // Severity::kVERBOSE 或 kINFO // 只打印错误信息 if (severity == Severity::kERROR \u0026amp;\u0026amp; severity == Severity::kINTERNAL_ERROR) { std::cout \u0026lt;\u0026lt; msg \u0026lt;\u0026lt; std::endl; } } } gLogger; int main(int argc, char *argv[]) { if (argc \u0026lt; 2) { std::cerr \u0026lt;\u0026lt; \u0026#34;Please specify path to the engine file.\\n\u0026#34;; return 1; } // 打开并反序列化 TensorRT engine std::ifstream engineFile(argv[1], std::ios::binary); if (!engineFile) { std::cerr \u0026lt;\u0026lt; \u0026#34;Error opening engine file\\n\u0026#34;; return -1; } engineFile.seekg(0, engineFile.end); long int fsize = engineFile.tellg(); engineFile.seekg(0, engineFile.beg); std::vector\u0026lt;char\u0026gt; engineData(fsize); engineFile.read(engineData.data(), fsize); if (!engineFile) { std::cerr \u0026lt;\u0026lt; \u0026#34;Error reading engine file\\n\u0026#34;; return -1; } nvinfer1::IRuntime *runtime = nvinfer1::createInferRuntime(gLogger); nvinfer1::ICudaEngine *engine = runtime-\u0026gt;deserializeCudaEngine(engineData.data(), fsize, nullptr); // 创建推理上下文 nvinfer1::IExecutionContext *context = engine-\u0026gt;createExecutionContext(); // 读取并预处理图像 cv::Mat img1 = cv::imread(\u0026#34;/home/metoak/Projects/dependency/InferenceDemo/images/left.png\u0026#34;, cv::IMREAD_COLOR); cv::Mat img2 = cv::imread(\u0026#34;/home/metoak/Projects/dependency/InferenceDemo/images/right.png\u0026#34;, cv::IMREAD_COLOR); cv::resize(img1, img1, cv::Size(640, 384)); cv::resize(img2, img2, cv::Size(640, 384)); img1.convertTo(img1, CV_32FC3, 1 / 255.0); img2.convertTo(img2, CV_32FC3, 1 / 255.0); // 创建输入数据 int num_elements_input = 1 * 3 * 384 * 640; // 根据你的模型的实际需求进行调整 // 分割通道 std::vector\u0026lt;cv::Mat\u0026gt; channels1(3); std::vector\u0026lt;cv::Mat\u0026gt; channels2(3); cv::split(img1, channels1); cv::split(img2, channels2); std::vector\u0026lt;float\u0026gt; data1; std::vector\u0026lt;float\u0026gt; data2; // 将每个通道的数据添加到相应的向量 for (auto \u0026amp;channel : channels1) { std::vector\u0026lt;float\u0026gt; channelData(channel.begin\u0026lt;float\u0026gt;(), channel.end\u0026lt;float\u0026gt;()); data1.insert(data1.end(), channelData.begin(), channelData.end()); } for (auto \u0026amp;channel : channels2) { std::vector\u0026lt;float\u0026gt; channelData(channel.begin\u0026lt;float\u0026gt;(), channel.end\u0026lt;float\u0026gt;()); data2.insert(data2.end(), channelData.begin(), channelData.end()); } #if TEST_TIME == 1 // 打印图像的前10个像素值 std::cout \u0026lt;\u0026lt; \u0026#34;First 10 pixels of the input image:\\n\u0026#34;; for (int i = 0; i \u0026lt; 10; ++i) { std::cout \u0026lt;\u0026lt; data1[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } #endif // 转换回图像 std::vector\u0026lt;cv::Mat\u0026gt; channels(3); for (int c = 0; c \u0026lt; 3; ++c) { channels[c] = cv::Mat(384, 640, CV_32F, data1.data() + c * 384 * 640); } cv::Mat imgData1; cv::merge(channels, imgData1); // 归一化并将数据类型转换回 CV_8UC3 以便显示 imgData1 = imgData1 * 255.0; imgData1.convertTo(imgData1, CV_8UC3); cv::imwrite(\u0026#34;/home/metoak/Projects/input.png\u0026#34;, imgData1); // 分配 GPU 内存用于输入 void *gpuData1; void *gpuData2; cudaMalloc(\u0026amp;gpuData1, num_elements_input * sizeof(float)); cudaMalloc(\u0026amp;gpuData2, num_elements_input * sizeof(float)); // 复制输入数据到 GPU cudaMemcpy(gpuData1, data1.data(), num_elements_input * sizeof(float), cudaMemcpyHostToDevice); cudaMemcpy(gpuData2, data2.data(), num_elements_input * sizeof(float), cudaMemcpyHostToDevice); // 根据你的模型的输出大小来创建输出数据 int num_elements_output = 1 * 3 * 384 * 640; // 你需要根据你的模型的实际输出大小进行调整 // 分配 GPU 内存用于输出 void *gpuOutput[7]; for (int i = 0; i \u0026lt; 7; ++i) { cudaMalloc(\u0026amp;gpuOutput[i], num_elements_output * sizeof(float)); } // 创建输入输出的指针数组 void *bindings[9] = {gpuData1, gpuData2, gpuOutput[0], gpuOutput[1], gpuOutput[2], gpuOutput[3], gpuOutput[4], gpuOutput[5], gpuOutput[6]}; // 创建 CUDA stream cudaStream_t stream; cudaStreamCreate(\u0026amp;stream); #if TEST_TIME == 1 // 100次推理 cudaEvent_t start, stop; cudaEventCreate(\u0026amp;start); cudaEventCreate(\u0026amp;stop); float totalTime = 0; int iteration = 100; // 推理次数 for (int i = 0; i \u0026lt; iteration; i++) { cudaEventRecord(start, stream); // 执行推理 context-\u0026gt;enqueue(1, bindings, stream, nullptr); std::cout \u0026lt;\u0026lt; \u0026#34;Inference iteration: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; cudaEventRecord(stop, stream); cudaEventSynchronize(stop); float milliseconds = 0; cudaEventElapsedTime(\u0026amp;milliseconds, start, stop); totalTime += milliseconds; } float avgTime = totalTime / iteration; std::cout \u0026lt;\u0026lt; \u0026#34;Average inference time per image: \u0026#34; \u0026lt;\u0026lt; avgTime \u0026lt;\u0026lt; \u0026#34; ms.\u0026#34; \u0026lt;\u0026lt; std::endl; cudaEventDestroy(start); cudaEventDestroy(stop); #else // 执行推理 context-\u0026gt;enqueue(1, bindings, stream, nullptr); #endif // 将需要的输出数据从 GPU 内存复制回 CPU 内存 std::vector\u0026lt;float\u0026gt; outputData(num_elements_output); cudaMemcpy(outputData.data(), gpuOutput[6], num_elements_output * sizeof(float), cudaMemcpyDeviceToHost); #if TEST_TIME == 1 // 打印图像的前10个像素值 std::cout \u0026lt;\u0026lt; \u0026#34;First 10 pixels of the output image:\\n\u0026#34;; for (int i = 0; i \u0026lt; 10; ++i) { std::cout \u0026lt;\u0026lt; outputData[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } #endif // 将一维向量转换为3通道图像 cv::Mat img_out(384, 640, CV_32F, outputData.data()); cv::Mat img_out_normalized; cv::normalize(img_out, img_out_normalized, 0, 255, cv::NORM_MINMAX, CV_8U); cv::Mat img_color_mapped; cv::applyColorMap(img_out_normalized, img_color_mapped, cv::COLORMAP_JET); // cv::resize(img_out, img_out, cv::Size(1920, 1080)); // 保存图像 cv::imwrite(\u0026#34;/home/metoak/Projects/output.png\u0026#34;, img_color_mapped); std::cout \u0026lt;\u0026lt; \u0026#34;Output image size: \u0026#34; \u0026lt;\u0026lt; img_out_normalized.size() \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; // 清理 cudaFree(gpuData1); cudaFree(gpuData2); for (int i = 0; i \u0026lt; 7; ++i) { cudaFree(gpuOutput[i]); } context-\u0026gt;destroy(); engine-\u0026gt;destroy(); runtime-\u0026gt;destroy(); // 销毁 CUDA stream cudaStreamDestroy(stream); return 0; } 这里有几点注意：\n数据的前处理需要根据的模型的结构来调整，这里是输入两组图像\n// 分配 GPU 内存用于输入 void *gpuData1; void *gpuData2; cudaMalloc(\u0026amp;gpuData1, num_elements_input * sizeof(float)); cudaMalloc(\u0026amp;gpuData2, num_elements_input * sizeof(float)); // 复制输入数据到 GPU cudaMemcpy(gpuData1, data1.data(), num_elements_input * sizeof(float), cudaMemcpyHostToDevice); cudaMemcpy(gpuData2, data2.data(), num_elements_input * sizeof(float), cudaMemcpyHostToDevice); // 根据你的模型的输出大小来创建输出数据 int num_elements_output = 1 * 3 * 384 * 640; // 你需要根据你的模型的实际输出大小进行调整 // 分配 GPU 内存用于输出 void *gpuOutput[7]; for (int i = 0; i \u0026lt; 7; ++i) { cudaMalloc(\u0026amp;gpuOutput[i], num_elements_output * sizeof(float)); } 这一块就是根据模型的结构来调整输入和输出的\n这里加入了很多手工调试的信息，比如保存模型输入前后的图像来验证\n这是一段完整的推理代码，如果这一段代码能够成功运行，那么将这段代码整合到实际的功能中，定义好输入输出的接口，模型也一样可以正常使用\n这里很多地方是手动申请内存空间的，所以一定注意使用后的销毁（当然也可以使用类来实现析构这种方法）\n// 清理 cudaFree(gpuData1); cudaFree(gpuData2); for (int i = 0; i \u0026lt; 7; ++i) { cudaFree(gpuOutput[i]); } context-\u0026gt;destroy(); engine-\u0026gt;destroy(); runtime-\u0026gt;destroy(); // 销毁 CUDA stream cudaStreamDestroy(stream); 注意理解这一部分的功能和必要性\n有了这两部分后，模型从ONNX到TensorRT，再到TensorRT的推理就打通了！\n对于上面的代码编译，直接使用cmake的方法，CMakeLists.txt例子如下：\ncmake_minimum_required(VERSION 3.10) project(OnnxInferenceDemo) # 调试模式 set(CMAKE_BUILD_TYPE Debug) set(CMAKE_CXX_STANDARD 14) # 设置METOAK的驱动路径 set(MO_SDK_DIR /opt/moak) find_package(OpenCV REQUIRED) find_package(PCL REQUIRED) find_package(CUDA REQUIRED) find_path(TENSORRT_INCLUDE_DIR NvInfer.h HINTS ${TENSORRT_ROOT_DIR} PATH_SUFFIXES include/) find_library(TENSORRT_LIBRARY nvinfer HINTS ${TENSORRT_ROOT_DIR} PATH_SUFFIXES lib lib64 lib/x64) find_library(TENSORRT_ONNX_LIBRARY nvonnxparser HINTS ${TENSORRT_ROOT_DIR} PATH_SUFFIXES lib lib64 lib/x64) set(TENSORRT_INCLUDE_DIRS ${TENSORRT_INCLUDE_DIR}) set(TENSORRT_LIBRARIES ${TENSORRT_LIBRARY} ${TENSORRT_ONNX_LIBRARY}) include_directories(${TENSORRT_INCLUDE_DIRS}) include_directories(${CUDA_INCLUDE_DIRS}) include_directories( include ${OpenCV_INCLUDE_DIRS} ${MO_SDK_DIR}/3rdparty/ONNX/x86_64/GPU/include ) link_directories(${MO_SDK_DIR}/3rdparty/ONNX/x86_64/GPU/lib) link_directories(lib) add_executable(onnx_to_tensorrt src/onnx_to_tensorrt.cpp) target_link_libraries(onnx_to_tensorrt ${TENSORRT_LIBRARIES} ${CUDA_LIBRARIES}) add_executable(tensorrt_image src/tensorrt_image.cpp) target_link_libraries(tensorrt_image ${OpenCV_LIBS} ${TENSORRT_LIBRARIES} ${CUDA_LIBRARIES}) 总结 使用TensorRT来进行模型加速，如果很凑巧，当前版本无法支持Onnx的模型转换，那可能正常方法就无法使用，就比如这里的模型，在TensorRT 8.6之前的版本，都无法完成正常的转换，并且报出段错误异常，这种情况，可能就是无法支持；\n而使用了最新版本发现，模型可以转换，直接无法直接进行模型的推理，那么就要排查一下是哪里的问题了，比如这里的模型，在TensorRT 8.6.1上可以完整转换，但是需要手动指定使用的插件，内建的还是其他的，这就会影响模型对应的推理程序的书写。\n另外一点：\nTensorRT的安装和环境部署其实很简单，但是因为网络上的搜索出来的教程杂七杂八，导致没有按照统一的方式来安装CUDA、cuDNN、TensorRT，导致apt安装时出现依赖异常，其实只要全部采用dep包管理的方式来安装就不会有问题。\n","permalink":"http://ahaknow.com/posts/know/%E6%B5%8B%E8%AF%95/","summary":"第一步：先测试模型的基准速度 使用ONNX测试模型运行速度基准 目标： 使用ONNX Runtime来测试机器学习模型的推理时间基准。 步骤： 安装ON","title":"测试"}]