[{"content":" 好记性不如烂笔头！\n学一点记一点，常常温习，温故而有新知！\n经常会用到的，文件查找🔍 在Linux操作系统中，查找文件是日常工作的一个重要部分。Linux提供了多种强大的命令行工具来帮助用户快速定位文件和目录。这里将详细介绍几种常用的查找方法，包括find、grep、locate命令，以及使用这些工具的高级技巧。\n1. 使用find命令 find命令是最强大的文件搜索工具之一，它可以在目录树中查找符合条件的文件和目录。\n基本语法： 1 find [路径] [选项] [动作] [路径]：指定开始查找的目录路径。例如，/代表根目录，.代表当前目录。 [选项]：指定查找条件，如按名称、大小、类型、修改时间等。 [动作]：对找到的文件执行的操作，如打印文件名、删除文件等。 示例： 查找当前目录及子目录下所有名为example.txt的文件：\n1 find . -name example.txt 查找/home目录下所有修改时间在7天内的文件：\n1 find /home -mtime -7 查找/var/log目录下大于50MB的文件并删除它们：\n1 find /var/log -size +50M -exec rm {} \\; 2. 使用grep命令 grep命令用于搜索文件内容，查找并打印匹配指定模式的行。\n基本语法： 1 grep [选项] [模式] [文件...] [选项]：控制搜索行为的选项，如-i（忽略大小写）、-r（递归搜索）等。 [模式]：是一个正则表达式，用于描述要匹配的文本。 [文件...]：指定要搜索的文件列表。 示例： 在example.txt文件中查找包含\u0026quot;hello\u0026quot;的行：\n1 grep \u0026#34;hello\u0026#34; example.txt 递归搜索当前目录下所有.py文件中包含\u0026quot;import os\u0026quot;的行：\n1 grep -r \u0026#34;import os\u0026#34; *.py 3. 使用locate命令 locate命令通过搜索一个内置的数据库来查找文件，这个数据库包含了系统上大部分文件的路径。locate的优势在于速度非常快，但它依赖于数据库定期更新。\n基本用法： 1 locate [选项] [模式] [模式]：指定要搜索的文件名模式。 示例： 快速查找名为example.txt的文件：\n1 locate example.txt 高级技巧： 结合使用find和grep：你可以使用find命令找到特定的文件，然后用grep搜索这些文件中的内容。例如，查找当前目录及子目录下所有.php文件中包含\u0026quot;mysqli\u0026quot;的行：\n1 find . -name \u0026#34;*.php\u0026#34; -exec grep \u0026#34;mysqli\u0026#34; {} \\; 使用find命令的高级选项：例如，查找15天前被访问的并且大于100KB的.jpg文件：\n1 find /path/to/search -type f -name \u0026#34;*.jpg\u0026#34; -atime +15 -size +100k 总结： Linux提供了多种强大的查找工具，每个工具都有其独特的用途和优势。find命令非常适合基于文件属性进行深度搜索，grep命令擅长搜索文件内容，而locate命令在搜索整个文件系统时速度非常快。\n授之以渔，学会使用Linux自带的手册📒 在Linux中，当你想要了解命令的详细使用方法、可用选项以及参数时，可以使用man命令来访问该命令的手册页（manual pages）。手册页提供了关于Linux命令和程序的详细信息，包括它们的功能、选项、参数以及一些示例。这是一种非常实用的学习和查询工具。\n使用man命令查看手册页 基本语法：\n1 man [命令或程序名] 示例： 查看find命令的手册页：\n1 man find 查看grep命令的手册页：\n1 man grep 查看locate命令的手册页：\n1 man locate 浏览手册页 手册页通常很长，你可以使用以下按键来浏览：\n空格键：向下翻页。 b：向上翻页。 上下箭头键：逐行滚动。 /关键字：搜索关键字。例如，/选项会搜索包含“选项”的文本。重复按下n键可以跳到下一个匹配项。 q：退出手册页。 其他获取帮助的方法 除了man命令，还有一些其他方法可以获取命令的使用帮助：\n--help选项：大多数Linux命令支持--help选项，该选项会显示命令的基本用法和选项。这通常是快速获取命令概览的好方法。\n1 2 3 find --help grep --help locate --help info命令：某些命令还有info页，这是另一种文档，可能提供比man页更详细的信息。\n1 info grep ","permalink":"http://ahaknow.com/posts/know/linux%E6%97%A5%E5%B8%B8%E7%B2%BE%E8%BF%9B/","summary":"好记性不如烂笔头！ 学一点记一点，常常温习，温故而有新知！ 经常会用到的，文件查找🔍 在Linux操作系统中，查找文件是日常工作的一个重要部分。L","title":"Linux日常精进🚀"},{"content":"","permalink":"http://ahaknow.com/posts/know/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91-%E6%89%93%E6%80%AA%E5%8D%87%E7%BA%A7/","summary":"","title":"软件开发 打怪升级"},{"content":"","permalink":"http://ahaknow.com/posts/know/%E4%BD%BF%E7%94%A8latex%E4%B9%A6%E5%86%99%E7%AE%97%E6%B3%95%E4%BC%AA%E4%BB%A3%E7%A0%81/","summary":"","title":"使用Latex书写算法伪代码"},{"content":"主要就是想说明：\n买家因为觉得卖家一直不发货，心里很不爽，想要急着退货。\n卖家则是，某些情况导致发不了货，一直不发货的话，平台对商家的评分会降低，所以会求着买家退货。\n","permalink":"http://ahaknow.com/posts/think/%E4%B8%80%E5%9C%BA%E5%9B%A0%E4%B8%BA%E4%B8%8D%E5%8F%91%E8%B4%A7%E8%80%8C%E9%80%80%E8%B4%A7%E7%9A%84%E5%8D%9A%E5%BC%88/","summary":"主要就是想说明： 买家因为觉得卖家一直不发货，心里很不爽，想要急着退货。 卖家则是，某些情况导致发不了货，一直不发货的话，平台对商家的评分会降低","title":"一场因为不发货而退货的博弈"},{"content":" 引言：\n今天，2024年2月20日，又一次经历了Word，就是微软Office的那个Word自己发病，把我论文里面的公式全变成了乱码\u0026amp;……%¥#@！\n心里瞬间不是个滋味了啊！\n上一次，是Word没有保存（自动保存需要Office365会员，平时都Latex、Markdown的，平均3年才会正经用一次Word，所以没有开会员，自然也没有它提供的自动保存），一个程序崩溃卡死，直接回到了解放前，结果是，凭借着记忆，花了一天时间重新写了出来。\n所以，自打上一次的“教训”之后，我学“聪明”了一点，写了一个自动运行脚本，每个10分钟自动保存一次，这个是借助Mac提供的脚本支持，通过命令实现键盘输入。\n而这一次呢，直接给我干懵了。而且因为之前每次覆盖保存的操作，使得我很难退回到正常状态。这也就意味着所有乱码的公式都要重新打一遍。。。。\n如果，不只是覆盖保存，还是渐进保存呢？每一天保存一个，那至少只会毁掉一天的工作量，弥补起来很算快，而不是一下子回到了一周前，这就要命了！\n所以，为啥“狡兔三窟”，本质的意义在于，多一个副本，多一手应对不确定的准备。\n","permalink":"http://ahaknow.com/posts/think/%E7%8B%A1%E5%85%94%E4%B8%89%E7%AA%9F%E7%9A%84%E5%AD%98%E5%9C%A8%E6%84%8F%E4%B9%89/","summary":"引言： 今天，2024年2月20日，又一次经历了Word，就是微软Office的那个Word自己发病，把我论文里面的公式全变成了乱码\u0026amp;","title":"“狡兔三窟”的存在意义"},{"content":" 引子：\n做了一个实时展示的小程序，一开始做的比较粗糙，每次呢，也就是每天的早上，都需要手动去执行，然后用鼠标去一点点调整展示窗口的大小和排列，一共三个窗口，一大两小，排列方式是固定的，但是由于每次都是一点点拖动摆放，因此每一次自己都需要对排列后的样子有一个预期，尽可能调整成看上去还不错的样子，尽管如此这般，也很难做到每次的排列都相同。做到这里，虽然没用到多少时间，几分钟吧，但耗费了不少注意力，因为每天都要想着：早上的第一件事是做这个，而且很有可能哪一天就忘了。晚上的话，还要想着去关掉它，手动关闭，费不了多少时间，但同时消耗了不少注意力，原因和早上的一样：一整天的潜意识里都会挂念着，晚上了得把它关掉。如此，一日复一天，天天如此，重复着同样的工作，而且似乎并没有意识到什么问题……\n都2024年了啊！还是个会敲代码的，天天就干这些事？不知道让这个小程序自己打开再自己关掉嘛？时间一直往前走，怎么写代码的意识还不如从前了呢！\n是啊，已经不知不觉陷入了一种“忙忙碌碌，不思进取，每天做着同样的事，为了活着而活着的”行尸走肉之中了。一天，两天，一周，一个月，大半年，就是这么混过来的，有进步嘛？有一点，但仔细瞧瞧只不过是错觉，很可能还是个负方向的。有反思嘛？好像有过，但借口一句没时间啊就过去了，正如此时敲击着键盘，如果现在不写，那么可能又是，一天过去，一周过去，一个月过去，于是乎，2025年2月20日啦。确实很打脸，因为上一次想要写写文字反思的时候，是2023年2月18日，而当下此刻，是2024年2月19日。。。\n什么时候种树都不迟，只要你意识到了就抡起铲子！也许当下的形势让你觉得自己的所学一文不值，那么就立刻开始，多学习点，想学的就去学，从自己的所爱出发，而不是犹豫、惆怅，还徘徊……\n说回来了，原本这一篇记录是想偏向于“自学的总结”，会涉及一些比较“专业”的东西，一些代码啊，一些指令啥的，昨晚借口一句：“困了先睡觉了”，就没写。欺骗自己说早上醒来写，实际上自己心里很清楚，大概率不可能。人的惰性太自然了，躺着多舒服啊，但如果一直躺着，那不如成为一具尸首。\n正篇：意识到了就立刻行动 我知道此前的我，也是成了“每天重复做着同样工作”的讨厌样子了。\n新的一年，那就由此改变吧！\n实时展示的小程序是吧，加一个自动启动和退出的功能咯！怎么做呢？\n我最开始的想法是将这个小程序软件化，也就是使用QT的可视化图形界面来改写，无奈，尝试了大半天，代码编译看着没问题了，但用不起来，这很大原因和自己没有系统学习有关。现在的chatGPT啊，写代码很厉害了，可能目前我的水平还比较低，用不好它，以至于说是让它辅助我改写，实际上我很多时候并不过脑子，直接复制粘贴，那哪行哦！这不就成了工具的奴隶了，连工具都不如了，这不更容易被食物链中的腐食者给吃了嘛。\n于是，我开始想，能不能通过一些技巧来实现我的需求呢？这些技巧背后依赖的功能应该是存在的，但是我不知道，此时我就是可以通过chatGPT协助了。\n现在是三个窗口（此前实现的时候，使用了多线程，因此同时展示的问题是解决的），我能不能把窗口数量减少，比如两个小窗口直接合成成一个？ 之前每次手动打开都需要手动调整窗口大小和位置，能不能直接在代码中实现预先的设置呢？ 自动打开，Linux上似乎有守护进程可以？但是没有在这里尝试过（这个也是很古老的知识了，至少六年前我就用过），想到过，但是没有行动起来！！ 自动关闭呢？之前设置了通过键盘输入q实现程序的退出，能不能让Linux自动模拟键入q呢？ 按照这个思路我就去做了～\n合并窗口：将思维打开，不要局限 一开始，我是用两个窗口来同时显示两个图像的，最简单也是最笨的操作。为啥之前我就没有想到将两个图像放在一个窗口里呢？\n可能是思维惯性，之前怎么做的，就跟着怎么做了。 也可能就是偷懒，从而导致的不动脑子，以及相应会产生的拖延。 认识到自己的思维存在僵化之后，开始改变：\n首先想到，直接拼接两张图像，上下拼接就能完成。但是在显示上不友好，上下拼接后变成一个长方形了，但是由于显示器很大，把窗口拉长之后上下图居中在一起，上下有空白的，显得很丑，从美观的角度来看的话，如果两个拼接的图像在上下位置能够保证对称，比如上方、两图中间、下方各自留出一块空间，这样就看着舒服了。按照这个思路，那就改～\n直接拼接不成，那就先考虑创建一个大的窗口，然后将两张图像分别搁进去，具体操作起来特别像前端的UI设计，要考虑padding的填充。这样的方法可行，然后还需要加入图像的标题，也采用往大窗口里搁置的思路，同样的，还是考虑了要设置padding的填充，使得标题能够局中。之后就是根据实际的情况，不断地调整窗口的尺寸，padding的大小，以达到自认为满意的比例了。至此，合并窗口完成了，现在的小程序只需要两个窗口就能满足了。\n代码备忘录📝 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 cv::Mat merged_image; if (left_image_color_global \u0026amp;\u0026amp; !left_image_color_global-\u0026gt;empty() \u0026amp;\u0026amp; disparity_color_global \u0026amp;\u0026amp; !disparity_color_global-\u0026gt;empty()) { int space = 25; // 标题和图像之间的空间 int test_height = 50; // 文本标题的高度（大致估计） cv::Scalar textColor(255, 255, 255); // 文本颜色 int font_face = cv::FONT_HERSHEY_SIMPLEX; double font_scale = 1; int thickness = 2; int top_padding = 20; int between_padding = 60; int bottom_padding = 20; int merged_height = left_image_color_global-\u0026gt;rows + disparity_color_global-\u0026gt;rows + 3 * space + 2 * test_height + top_padding + bottom_padding + between_padding; int merged_width = std::max(left_image_color_global-\u0026gt;cols, disparity_color_global-\u0026gt;cols); merged_image = cv::Mat::zeros(merged_height, merged_width, left_image_color_global-\u0026gt;type()); // 绘制第一个图像和标题 int title1_x = (merged_width - cv::getTextSize(\u0026#34;Left Image\u0026#34;, font_face, font_scale, thickness, nullptr).width) / 2; cv::putText(merged_image, \u0026#34;Left Image\u0026#34;, cv::Point(title1_x, test_height), font_face, font_scale, textColor, thickness); left_image_color_global-\u0026gt;copyTo(merged_image(cv::Rect(0, top_padding + test_height + space, left_image_color_global-\u0026gt;cols, left_image_color_global-\u0026gt;rows))); // 绘制第二个图像和标题 int title2_x = (merged_width - cv::getTextSize(\u0026#34;Dense Disparity\u0026#34;, font_face, font_scale, thickness, nullptr).width) / 2; int merged_mid = left_image_color_global-\u0026gt;rows + 2 * space + test_height + top_padding; cv::putText(merged_image, \u0026#34;Dense Disparity\u0026#34;, cv::Point(title2_x, merged_mid + test_height), font_face, font_scale, textColor, thickness); disparity_color_global-\u0026gt;copyTo(merged_image(cv::Rect(0, merged_mid + bottom_padding + test_height + space, disparity_color_global-\u0026gt;cols, disparity_color_global-\u0026gt;rows))); } 提前设置窗口位置：不知道的不代表没有，以前是搜索，现在是“问” 设置好窗口大小之后，还需要能够实现打开小程序后，两个窗口能够自动“站”到合适的位置。**应该是有这样的方法的，只不过是我不知道，在没有chatGPT之前，可以用Google进行搜索，现在有了chatGPT，只要将需求描述清楚，一般互联网上已有的信息就能够获取到。**因此我就直接“问”了有没有这样的方法，果然，有的，而且还很简洁。\n忽然想到一点，这个不就是信息差嘛！以前靠信息差吃某一种职位铁饭碗的想法应该在此之后自己消亡了吧？也许不会，因为好逸恶劳，因为懒惰，因为路径依赖？总之，现在自己意识到了还不算迟。\n所以，很可能以后还是很缺“产品经理、项目经理”这种明确知道需求是什么的，能够用精炼的语言将其描述清楚的总揽全局的人，而不缺重复的、低级的、复制粘贴的所谓“程序员”码农。\n代码备忘录📝 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cv::namedWindow(\u0026#34;Color \u0026amp; Disparity\u0026#34;, cv::WINDOW_NORMAL); cv::resizeWindow(\u0026#34;Color \u0026amp; Disparity\u0026#34;, 1548, 2160); cv::moveWindow(\u0026#34;Color \u0026amp; Disparity\u0026#34;, 0, 0); // 将窗口移动到左上角 // ---------------------------------------------------------- // 创建点云可视化器 pcl::visualization::PCLVisualizer::Ptr viewer(new pcl::visualization::PCLVisualizer(\u0026#34;Dense PointCloud Viewer\u0026#34;)); viewer-\u0026gt;setBackgroundColor(0, 0, 0); viewer-\u0026gt;addPointCloud\u0026lt;pcl::PointXYZRGB\u0026gt;(point_cloud_ptr_global, \u0026#34;Dense PointCloud\u0026#34;); viewer-\u0026gt;setPointCloudRenderingProperties(pcl::visualization::PCL_VISUALIZER_POINT_SIZE, 3, \u0026#34;Dense PointCloud\u0026#34;); viewer-\u0026gt;initCameraParameters(); viewer-\u0026gt;setCameraPosition( 0, 0, -1.5, // 控制相机的XYZ位置 0, 0.25, 1, 0, 1, 0 // 后面点云转了旋转，这里Y为1 ); viewer-\u0026gt;setSize(2220, 2160); // 因为使用了VTK的窗口 vtkSmartPointer\u0026lt;vtkRenderWindow\u0026gt; renderWindow = viewer-\u0026gt;getRenderWindow(); renderWindow-\u0026gt;SetPosition(1900, 0); 自启动：不懂的就多探究，不要是一个“半瓶水响叮当” 之前知道一种方法是用守护进程：通过crontab -e进行设置，我也用这个方法尝试了，发现不起作用。分析原因得到：\n因为，我启动的小程序需要调用显示窗口，OpenCV提供的简易窗口和VTK提供的可视化窗口，而守护进程启动命令时，是一种“文本”方式，无法唤起显示的窗口。\n**以前在服务器上用crontab自启动一些命令，因为只会有文本输出，比较简易，那会儿使用远程服务器，自然也不会涉及显示，也就忽略了这个问题。**所以，我此前对于守护进程的理解还只是皮毛，今天多了一些使用的经验。但更重要的是从这次的经历中学会一些“遇到问题、分析现象、思考解决”的思维。\n发现这条路走不通，于是，将自己的需求重新整理了一遍：\n“我希望在当前显示器下，当前的命令行终端下，定时地执行某一个命令”\n在这个思路下，找到了一个可行的方式：\n在终端中，持续运行一个判断：获取当前时间，判断是否等于设定的需要自动打开的时间，如果是就执行命令，然后之后一段时间里不再执行。\n在Linux环境下，这个判断可以比较直接的实现：通过Bash脚本的方式。（本质就是Bash脚本自动化）\n这里明白了一点，一条路走不通很可能是这路条本身就不对，因为很多时候对一项事物也是处于一种了解皮毛的状态，所以遇到问题了要学会打破“自以为是”，从整体的层面来进行分析，对于这里的问题就是：“守护进程不行，为啥不行，那我把运行的日志打印出来，然后分析一下错误信息，再考虑这种方法是不是本身就不支持。”\n代码备忘录📝 1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash TARGET_TIME=\u0026#34;09:00\u0026#34; while true; do CURRENT_TIME=$(date +%H:%M) if [ \u0026#34;$CURRENT_TIME\u0026#34; == \u0026#34;$TARGET_TIME\u0026#34; ]; then /home/metoak/Projects/RealTimeDensePointCloud/C++/build/tensorrt_inference_one_camera /home/metoak/Projects/RealTimeDensePointCloud/C++/models/model_mobilev2_960_512.trt 960 512 sleep 60 # 避免在目标时间多次执行 fi sleep 30 # 每30秒检查一次时间 done 自关闭：脑子是越用越灵光的，多想想，不要固化，不要陷入路径依赖 在这里遇到的问题和自启动时差不多。在最开始实现时，也想到了要实现手动关闭，所以就设置了一个通过键盘关闭的方法。我在想能不能自动关闭时，也就顺着这个思路走下去了：“能不能用指令来模拟键盘输入，来达到跟手动按下键盘一样的效果呢？\n想法是好的，也通过chatGPT进行“询问”和尝试，是有这种方法的，但是无法应用在这个问题的解决上。原因简单分析如下：\n通过一个指令或者一个程序模拟键盘输入，它所生效的位置是这个指令或者程序运行的空间；而我通过键盘控制关闭时，实际上是做了两件事：一、先选中窗口，二、按下键盘上的q键；模拟键盘输入可以完成第二步，但是完成不了第一步，因此这种方法在这里不起作用。\n至少，我在思考问题上，会多想一步了。但也陷进了路径依赖中！\n想让程序自己关闭的最快捷方式难道不应该是在程序内部实现嘛？而且很方便的只需要一个判断就行了，不然要你编码做何用？\n是啊，我忘记了，整个演示程序的代码都是我写的，那我想控制什么时候关闭，不是很容易嘛！（什么时候开启不太行，但用了上面的持续运行判断是可以的，因为开启需要从外界开启。断电的电脑怎么自动开机呢？但开机的电脑可以设置定时关闭，如此常识，自己之前咋就忘了呢。。。）\n所以，有时候可以让自己适当放空，然后重新梳理，一步一步来分析解决。\n路径依赖，说到底也是因为懒，能不能打破懒，还得看自己够不够狠心和韧性了，愈挫愈勇，这个方法不行，大不了从头来，用新的思路来解决！\n代码备忘录📝 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // 获取当前时间 auto now = std::chrono::system_clock::now(); std::time_t now_c = std::chrono::system_clock::to_time_t(now); std::tm now_tm = *std::localtime(\u0026amp;now_c); // 检查时间是否超过19:00 if (now_tm.tm_hour \u0026gt;= 19) { std::cout \u0026lt;\u0026lt; \u0026#34;Current time is \u0026#34; \u0026lt;\u0026lt; std::put_time(\u0026amp;now_tm, \u0026#34;%F %T\u0026#34;) \u0026lt;\u0026lt; \u0026#34;, the machine need to rest, See you tomorrow!\u0026#34; \u0026lt;\u0026lt; std::endl; running = false; } // 原来的手动关闭操作 int key = cv::waitKey(1); if (key == \u0026#39;q\u0026#39; || key == \u0026#39;Q\u0026#39;) { running = false; } 结束语：继续坚持，从1到2到3，到100，到一直能坚持 人类最可贵的，不就是会思考嘛，机器智能只会越来越强盛的今后，尤其显得珍贵！\n继续坚持，明天再继续～\n2024年2月19日\n","permalink":"http://ahaknow.com/posts/think/%E8%AE%A9%E5%B7%A5%E4%BD%9C%E8%87%AA%E5%8A%A8%E5%AE%8C%E6%88%90or%E6%AF%8F%E5%A4%A9%E9%87%8D%E5%A4%8D%E5%90%8C%E6%A0%B7%E7%9A%84%E5%B7%A5%E4%BD%9C/","summary":"引子： 做了一个实时展示的小程序，一开始做的比较粗糙，每次呢，也就是每天的早上，都需要手动去执行，然后用鼠标去一点点调整展示窗口的大小和排列，","title":"让工作自动完成or每天重复同样的工作？"},{"content":" ✨背景：\n实时显示彩色图像、深度图像和点云，目前使用三个单独的窗口来实现可视化，感觉很LOW，应该可以将它们整合都一个窗口中展示，这样的话，布局就不需要每次打开都进行调整了，还能够做到定时的启动和关闭。\n想到了用QT来改写～（虽然尝试失败了，采用了另一种策略😄）\n下面的部分记录一下QT的安装。\nQT相关的依赖 安装QT 1 2 sudo apt update sudo apt install qt5-default qtcreator 验证安装 安装完成后，可以通过运行Qt Creator来验证Qt是否已正确安装。在终端中输入qtcreator，Qt Creator应该会启动。\n1 qtcreator 安装VTK 这里需要手动编译安装！\n下载VTK源代码：访问VTK的GitHub页面或VTK官方网站来找到源代码。\n使用git克隆VTK仓库的示例命令：\n1 2 git clone https://github.com/Kitware/VTK.git cd VTK 创建构建目录：在VTK源代码目录中创建一个构建目录并进入该目录。\n1 2 mkdir build cd build 配置构建：使用CMake配置构建，启用VTK的Qt支持，可能需要指定Qt的安装路径\n1 cmake .. -DVTK_GROUP_ENABLE_Qt=YES -DCMAKE_BUILD_TYPE=Release -DQt5Widgets_DIR=/usr/lib/x86_64-linux-gnu/cmake/Qt5Widgets 如果不知道Qt5Widgets在哪里，可以用find查找：\n1 find /usr -name \u0026#34;Qt5Widgets\u0026#34; 也可以使用pkg-config：\n1 pkg-config --cflags Qt5Widgets 这应该会输出类似-I/usr/include/x86_64-linux-gnu/qt5/...的路径，这是头文件的位置。\n1 pkg-config --libs Qt5Widgets 这将提供链接器需要的库路径和名称，如-L/usr/lib/x86_64-linux-gnu -lQt5Widgets。\n编译VTK：编译VTK。这可能需要一些时间。\n1 make -j$(nproc) -j$(nproc)选项会使用所有可用的CPU核心来加速编译过程。\n安装VTK（可选）：完成编译后，可以选择安装VTK。这将安装所有必要的头文件、库和其他文件到你的系统中。\n1 sudo make install ‼️重要说明 如果在编译安装VTK时出现下面的错误：\n1 2 3 4 5 6 7 8 9 10 during RTL pass: fwprop2 In file included from /home/metoak/Projects/RealTimeDensePointCloud/VTK/build/ThirdParty/exprtk/vtk_exprtk.h:12, from /home/metoak/Projects/RealTimeDensePointCloud/VTK/Common/Misc/vtkExprTkFunctionParser.cxx:15: /home/metoak/Projects/RealTimeDensePointCloud/VTK/ThirdParty/exprtk/vtkexprtk/exprtk.hpp: In member function ‘T vtkexprtk::details::vec_binop_vecval_node\u0026lt;T, Operation\u0026gt;::value() const [with T = double; Operation = vtkexprtk::details::xor_op\u0026lt;double\u0026gt;]’: /home/metoak/Projects/RealTimeDensePointCloud/VTK/ThirdParty/exprtk/vtkexprtk/exprtk.hpp:11386:10: internal compiler error: Segmentation fault 11386 | } | ^ 0x7feb09c1108f ??? /build/glibc-wuryBv/glibc-2.31/signal/../sysdeps/unix/sysv/linux/x86_64/sigaction.c:0 0x7feb09bf2082 __libc_start_main 具体的错误分析是：\n内部编译器错误（internal compiler error，简称ICE），具体是在执行RTL（寄存器传输语言）传递时的fwprop2阶段发生了段错误（Segmentation fault）。这通常指示编译器本身的一个bug，而不是代码或其依赖项的问题。\n一般可以通过升级编译器来解决，比如报错时候的编译器版本是GCC-9和G++-9，可以将其升级为GCC-10和G++-10，Ubuntu20.04环境可以直接通过apt进行安装：\n1 2 sudo apt update sudo apt install gcc-10 g++-10 删除现有的损坏链接（按需使用） 如果在第2步进行软链接时进行错误操作，比如没有安装gcc-10就进行了软链接，那么就会报错，比如：\n1 update-alternatives: warning: forcing reinstallation of alternative /usr/bin/gcc-10 because link group gcc is broken 这时候就可以通过下面的方法先删除损坏的链接：\n1 2 sudo update-alternatives --remove gcc /usr/bin/gcc-10 sudo update-alternatives --remove g++ /usr/bin/g++-10 注意：这个命令可能会因为链接已经损坏而失败，如果是这样的话，你可以直接进入下一步。\n重新安装链接 将gcc-10作为默认版本，可以使用：\n1 sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 100 --slave /usr/bin/g++ g++ /usr/bin/g++-10 这条命令会将gcc命令链接到gcc-10，并将g++命令链接到g++-10。\n如果安装了多个版本的GCC，可以通过update-alternatives --config gcc来选择默认的版本。\n1 sudo update-alternatives --config gcc 验证安装 1 2 gcc --version g++ --version 目前的VTK安装似乎有点问题，导致使用QT+VTK时，CMake发生内存错误，暂时不使用QT了。\n感觉需要系统学习一下。\n","permalink":"http://ahaknow.com/posts/know/qt%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","summary":"✨背景： 实时显示彩色图像、深度图像和点云，目前使用三个单独的窗口来实现可视化，感觉很LOW，应该可以将它们整合都一个窗口中展示，这样的话，布","title":"QT学习笔记"},{"content":"","permalink":"http://ahaknow.com/posts/diary/0207%E6%8A%BD%E5%8D%A1%E5%85%BB%E5%8F%B7%E4%B8%8E%E6%B4%BB%E5%BE%97%E8%87%AA%E5%BE%8B/","summary":"","title":"「0207」抽卡养号与活得自律"},{"content":"","permalink":"http://ahaknow.com/posts/diary/0206%E6%8A%8A%E6%B8%B8%E6%88%8F%E5%8F%B7%E5%8D%96%E4%BA%86%E5%90%A7%E4%B8%8D%E7%84%B6%E5%85%BB%E7%9D%80%E4%B9%88%E5%85%BB%E7%9D%80%E8%83%BD%E4%B8%8D%E8%83%BD%E6%8E%A7%E5%88%B6%E4%BD%8F%E8%87%AA%E5%B7%B1%E5%91%A2/","summary":"","title":"「0206」把游戏号卖了吧，不然养着么？养着能不能控制住自己呢？！"},{"content":"","permalink":"http://ahaknow.com/posts/diary/0205%E4%BB%8Eword%E8%87%AA%E5%B7%B1%E5%8D%A1%E6%AD%BB%E6%B2%A1%E6%9C%89%E4%BF%9D%E5%AD%98%E5%A4%B1%E5%8E%BB%E4%B8%80%E5%91%A8%E5%86%99%E4%BD%9C%E6%88%90%E6%9E%9C%E6%83%B3%E5%88%B0/","summary":"","title":"「0205」从word自己卡死，没有保存，失去一周写作成果想到"},{"content":" 开始学习Docker的使用，怎么学呢？边用边学，边学边用！\n1. 进入Docker，需要同时映射本地的路径，使得在Docker中可以直接访问 举例如下：\n1 sudo docker run -it --net=host --runtime nvidia --name yck -e DISPLAY=$DISPLAY -v /home/titan/YCK:/opt/yck nvcr.io/nvidia/l4t-jetpack:r35.3.1 具体讲解 如果希望在Docker容器内访问本地目录，需要在运行容器时使用 -v 或 --volume 参数来挂载目录\n1 docker run -v /本地/路径:/容器/路径 -it \u0026lt;镜像名称\u0026gt; /bin/bash 2. 在Docker中的apt更新国内源（偷懒的方法） 因为没有尝试进行科学上网🚀，所以直接使用了更换国内源的方法（很久远的方法了）\n1 2 3 4 5 # ports对应的ARM架构 sudo sed -i \u0026#39;s/ports.ubuntu.com/mirrors.aliyun.com/g\u0026#39; /etc/apt/sources.list # x86 sudo sed -i \u0026#39;s/archive.ubuntu.com/mirrors.aliyun.com/g\u0026#39; /etc/apt/sources.list 同时安装一些必要的工具\n1 sudo apt-get install bash-completion cmake ","permalink":"http://ahaknow.com/posts/know/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","summary":"开始学习Docker的使用，怎么学呢？边用边学，边学边用！ 1. 进入Docker，需要同时映射本地的路径，使得在Docker中可以直接访问 举例如","title":"Docker进阶指南"},{"content":"","permalink":"http://ahaknow.com/posts/diary/0203%E4%B8%BA%E4%BB%80%E4%B9%88%E8%87%AA%E5%B7%B1%E4%BC%9A%E6%B2%89%E8%BF%B7%E6%B8%B8%E6%88%8F/","summary":"","title":"「0203」为什么自己会沉迷游戏？"},{"content":"","permalink":"http://ahaknow.com/posts/diary/0201%E5%85%88%E5%B0%9D%E8%AF%95%E5%81%9A%E5%A5%BD%E4%B8%80%E4%BB%B6%E4%BA%8B/","summary":"","title":"「0201」先尝试做好一件事！"},{"content":"","permalink":"http://ahaknow.com/posts/diary/0202%E5%BA%94%E8%AF%A5%E5%BB%BA%E7%AB%8B%E8%B5%B7%E6%9D%A5%E8%87%AA%E5%B7%B1%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93%E9%82%A3%E5%B0%B1%E4%BB%8E%E7%8E%B0%E5%9C%A8%E5%BC%80%E5%A7%8B%E5%BD%93%E4%B8%8B%E7%AB%8B%E5%8D%B3/","summary":"","title":"「0202」应该建立起来自己的知识库，那就从现在开始，当下，立即！"},{"content":"","permalink":"http://ahaknow.com/posts/diary/0131%E4%BB%8E%E6%B8%B8%E6%88%8F%E4%B8%AD%E6%80%9D%E8%80%83%E5%81%9A%E5%A4%9A%E4%B8%8D%E5%A6%82%E5%81%9A%E7%B2%BE/","summary":"","title":"「0131」从游戏中思考：做多不如做精"},{"content":"","permalink":"http://ahaknow.com/posts/diary/0130%E6%B2%A1%E6%9C%89%E8%BF%9B%E6%AD%A5%E8%BF%B7%E8%8C%AB%E4%B9%9F%E4%B8%8D%E7%9F%A5%E9%81%93%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88%E6%B2%A1%E6%9C%89%E5%8A%A8%E5%8A%9B%E8%87%AA%E9%97%AD%E7%94%9A%E8%87%B3%E8%BF%98%E4%BC%9A%E6%B2%89%E6%B2%A6%E6%B8%B8%E6%88%8F/","summary":"","title":"「0130」没有进步，迷茫，也不知道该做什么，没有动力，自闭，甚至还会沉沦游戏"},{"content":"","permalink":"http://ahaknow.com/posts/diary/0129%E6%88%91%E4%BC%BC%E4%B9%8E%E6%9B%B4%E5%96%9C%E6%AC%A2%E6%94%B6%E6%8B%BE%E7%83%82%E5%B1%80/","summary":"","title":"「0129」我似乎更喜欢收拾烂局？"},{"content":"Linux使用NFS（Network File System）作为远程连接的网盘 目的:\nNFS是一种允许远程主机通过网络访问文件系统的协议。\n通过ssh、mount的方法就能共享文件路径\n比如，ssh连接开发板，直接共享本地的文件路径，有啥操作本地修改就行，开发板上只需要执行命令即可，而不是每次都scp，就很高效！\n操作步骤 安装 NFS 服务器:\n1 2 sudo apt-get update sudo apt-get install nfs-kernel-server 配置 NFS 共享: 编辑 /etc/exports 文件来添加您想共享的目录。\n1 sudo vim /etc/exports 添加类似以下内容（共享 /home/young/CKData 目录）：\n1 2 /home/young/CKData *(rw,sync,no_subtree_check,no_root_squash) /home/username/shared client-ip(rw,sync,no_subtree_check,no_root_squash) 其中 client-ip 是客户端的 IP 地址或子网。\n其中的通配符表示适对所有 IP 开放\n启动 NFS 服务:\n1 sudo systemctl restart nfs-kernel-server 在客户端挂载 NFS 共享: 在客户端，您需要安装 NFS 客户端工具，然后挂载远程文件系统：\n1 sudo mount server-ip:/home/username/shared /mnt 需要确保/mnt路径存在。\n特别情况 在使用 mount 命令挂载 NFS 文件系统时，-o nolock 是一个选项，用于控制文件锁定的行为。\n-o nolock 参数 含义：-o nolock 选项用于禁用 NFS 的文件锁定机制。文件锁定是一种机制，用于防止多个客户端同时写入或更改同一个文件，这有助于防止数据损坏或一致性问题。\n使用场景：\n当 NFS 客户端不能使用标准的文件锁定机制时，比如在一些较老的系统或者特定的网络环境下。 在一些简单的应用场景中，比如只读挂载或者不需要文件锁定的环境下。 当 NFS 服务器或网络不支持或不稳定时，使用 nolock 可以避免挂载操作因无法获得锁而失败。 其他常见的 mount 参数 rw / ro：分别表示挂载文件系统为可读写 (rw) 或只读 (ro)。\nvers：指定 NFS 协议的版本，如 vers=3 或 vers=4。\nsoft / hard：\nsoft：在网络问题导致 NFS 请求无响应时，客户端会在尝试一定次数后放弃，可能会导致数据损坏。 hard：在网络问题导致 NFS 请求无响应时，客户端会无限期地重试，直到服务器响应。 intr：允许 NFS 请求可以被中断，特别是在 hard 挂载模式下。\nrsize / wsize：指定读写操作的数据块大小。\ntimeo：设置 NFS 请求超时时间，通常以十分之一秒为单位。\nretrans：在请求失败时，设置 NFS 重试次数。\nsec：指定用于认证的安全模式，如 sec=krb5。\nasync / sync：\nasync：允许系统在写入操作完成前返回响应，可以提高性能，但在系统崩溃的情况下可能会导致数据丢失。 sync：确保写入操作完成后才返回响应，保证数据的一致性。 ","permalink":"http://ahaknow.com/posts/know/linux%E4%BD%BF%E7%94%A8nfs%E4%BD%9C%E4%B8%BA%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5%E7%9A%84%E7%BD%91%E7%9B%98/","summary":"Linux使用NFS（Network File System）作为远程连接的网盘 目的: NFS是一种允许远程主机通过网络访问文件系统的协议。 通过ssh、","title":"Linux使用NFS（Network File System）作为远程连接的网盘"},{"content":"","permalink":"http://ahaknow.com/posts/diary/0128%E5%A4%A9%E4%B8%8B%E6%B2%A1%E6%9C%89%E5%85%8D%E8%B4%B9%E7%9A%84%E5%8D%88%E9%A4%90%E7%BE%8A%E6%AF%9B%E6%80%BB%E6%98%AF%E5%87%BA%E5%9C%A8%E7%BE%8A%E8%BA%AB%E4%B8%8A%E5%95%8A/","summary":"","title":"「0128」天下没有免费的午餐，羊毛总是出在羊身上啊"},{"content":"","permalink":"http://ahaknow.com/posts/diary/0127%E5%A6%82%E6%9E%9C%E6%AF%8F%E5%A4%A9%E9%83%BD%E8%83%BD%E8%BF%9B%E6%AD%A5%E4%B8%80%E7%82%B9/","summary":"","title":"「0127」如果每天都能进步一点？"},{"content":"话说，重新拾起要再次记录的念头，应该是一年前！\n最早开始想要记录，应该是六年前！六年啊！一恍惚就过去了？并且最可怕的事情，对比现在的自己和六年前的自己，似乎，还不如了？！！！\n六年，2190天，每天退步一点点，0.99的2190次方： $$ 0.99^{6\\times365} = 2.7610621 \\times 10^{-10} $$ 真的是，退步的不能再退步了，也真的太可怕了。\n时间无情的走了，而我呢，日复一日，年复一年，忙忙碌碌，重重复复，一路滑向深渊。\n能不能再做到，每天思考并记录呢？以前做到过，但是中断了，能不能在捡起来，再继续做下去！\n一直对自己说，自己是内驱的，自己是自律，结果事实却在打自己脸。\n一样会沉迷于电视剧，一样会沉沦在概率游戏中。\n","permalink":"http://ahaknow.com/posts/diary/01262024%E5%B9%B4%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E8%AE%B0%E5%BD%95/","summary":"话说，重新拾起要再次记录的念头，应该是一年前！ 最早开始想要记录，应该是六年前！六年啊！一恍惚就过去了？并且最可怕的事情，对比现在的自己和六年","title":"「0126」2024年的第一篇记录"},{"content":" 一转眼，2024年了！博客的更新又停滞了一年？！！！\n好记性不如烂笔头，不管以前知道的、学过的、时间久了都容易忘，时常记录下来，而不只是一个形式。\n博客的第一要义 写文章 在archetypes/default.md先创建每次new文章的模版，然后使用hugo new创建，会在content目录下生成。\n使用hugo -F --cleanDestinationDir命令可以每次生成全新的public，但是要注意，每次都是清空后再生成，因此需要一直保留的文件可以移动到static文件夹下面，比如域名的配置文件CNAME。\n","permalink":"http://ahaknow.com/posts/know/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BAckpaper%E8%AE%B0%E5%BD%95/","summary":"一转眼，2024年了！博客的更新又停滞了一年？！！！ 好记性不如烂笔头，不管以前知道的、学过的、时间久了都容易忘，时常记录下来，而不只是一个形","title":"🧭博客搭建｜CKPaper记录"},{"content":" 我在想……\n有点意思！\n那这是为什么啊？\n有的时候活得太浑浑噩噩了，听的是啥就是啥，这里听到一次新词，那里就给它显摆出去（哇！好厉害啊！）可是，再多问一句为什么呢？……于是，尴尬的场面来了，“嗯……这个嘛……是这样……”（当然“胡说的表演开始了”）\n就比如说：\n不小心受凉了，但不能不吃东西吧，那吃什么好呢？\n—— 这时可能有长辈会说：“这个你这段时间可不能多吃！性寒，越吃身体越糟糕”\n是呀，可是，我怎么知道这个东西它性寒，那个东西它不性寒（性热？还是性啥？）这么一想，感觉确实缺乏一些生活的基本常识：\n性寒、性热的本质是什么？\n我又怎么凭感觉来区分寒热？（就像感冒了傻傻分不清风寒还是风热🤣）\n从基本的认知出发吧，中医的观点，强调人身体的“阴阳平衡”，自然对于寒和热也应该是相互制约，不多不少的。\n如果体内寒气多了（从阴阳角度就是，阳气偏弱），表现出来的就是“体寒”，进而从逻辑层面的想到，**体寒会有什么样的表现？是不是很自然就会想到，体寒肯定更怕寒啊！**再由此想到，风寒感冒的症状里应该也会有一条：畏寒；而导致风寒感冒的原因呢，就容易联想到：偶感风寒，直白说就是受凉了。\n既然身体已经不平衡了，阴盛阳衰，那怎么来弥补呢？记得小时候感冒了（那会儿其实并不会思考是风寒还是风热，当然也不会知道细菌还是病毒），妈妈都会熬一碗姜汤，再放一勺红糖，喝完后，晚上睡觉再捂一晚被子，第二天基本就好了。现在再回想起来，一下子就能明白是为什么了：\n红糖 性温、味甘\n生姜 微温、味辛\n两者都能很好地调节寒弱，寒热互抵，自然能够达到让身体重新恢复平衡的状态～（当然如果是风热感冒的话，这样操作可能就不会见效了。）\n写在这，忽然想到，既然正常的身体是阴阳平衡的，那么我们生活的自然环境也一样充满了“阴和阳的关系”。\n如果说睡晚了，觉没睡够，那很可能就会让身体再次打破阴阳平衡，长此以往，身体逐渐变差，甚至体弱多病起来。为什么？直觉来看：\n白天，有太阳的时候，算作“阳”，但是人基本要在白天干很多事，一天中的几乎所有事都是白天干的，此时的“阳”也就会被消耗；当黑夜降临，我们所处的环境很自然地变成了“阴”，既然白天已经过多消耗了“阳气”，晚上的“阴气”又很重，那比较明智的做法就是：天黑了，就赶紧休息，补充好身体的“阳气”，等到新一天的光明……\n但是！自从有了电灯，这种日出而作，日落而息就被打破了，人在黑夜里待的是越来越长，晚睡早起、晚睡晚起，总之，**能够让身体恢复阴阳平衡的机制被打破了，身体越来越糟糕、气色越来越差，一天不如一天但依旧恶性循环。**尽可能还是早睡早起吧，日出而作，看看明早太阳几点起来，然后先睡它个7小时！\n再回到吃，吃的东西，肯定也会分不同的性质，夏天可喜欢吃西瓜了，吃西瓜解渴，凉快！可是，为什么呢？\n还是直觉的角度出来，西瓜要长熟，肯定要很多水，水越多，与“凉”、与“寒”也就联系在一起了。\n但实际上了，凭感觉有什么并不能准确判断，甚至完全搞反，比如：\n螃蟹🦀是寒性的；\n虾🦐是温性的；\n香蕉🍌是大寒的（就是特别寒）；\n桃子🍑是温性的；\n……\n所以啊，可以凭自然的感觉，比如，夏天生长需要很多水分的植物性偏寒；水生植物一般性偏寒；但也不一定处处皆适应。\n真要考量一番的时候，还是多查查资料，百科全书最好，吾将上下而求索，然后发现还真的挺有意思～\n","permalink":"http://ahaknow.com/posts/diary/0906%E4%BB%8E%E6%80%A7%E5%AF%92%E6%83%B3%E8%B5%B7/","summary":"我在想…… 有点意思！ 那这是为什么啊？ 有的时候活得太浑浑噩噩了，听的是啥就是啥，这里听到一次新词，那里就给它显摆出去（哇！好厉害啊！）可是，再","title":"0906 从“性寒”想起……"},{"content":"第一步：先测试模型的基准速度 使用ONNX测试模型运行速度基准 目标： 使用ONNX Runtime来测试机器学习模型的推理时间基准。\n步骤：\n安装ONNX和ONNX Runtime： 这两个Python库是测试基准的关键工具。ONNX是一个开放的模型表示标准，它允许在不同的机器学习框架之间移动模型。ONNX Runtime则是一个用于运行ONNX模型的高性能推理引擎。可以通过pip进行安装，命令为pip install onnx onnxruntime-gpu。这里的onnxruntime-gpu版本是支持GPU的ONNX Runtime。\n加载模型： 使用ONNX Runtime加载模型，具体做法是创建一个onnxruntime.InferenceSession的实例，并指定模型的文件路径和推理引擎提供商（这里选择的是'CUDAExecutionProvider'，即使用CUDA来利用GPU加速）。\n获取输入信息并创建输入数据： 需要知道模型的输入节点的名称和形状，这可以通过session.get_inputs()来获取。有了这些信息后，就可以创建与之匹配的随机输入数据。\n推理并计时： 进行100次模型推理，并使用time.time()来计时。注意，这里忽略了第一次推理的时间，因为第一次推理可能会包含一些初始化操作，导致耗时偏大。\n计算平均推理时间： 通过将所有推理的时间加起来，然后除以推理次数，就得到了平均推理时间。\n问题和解决：\n如何创建匹配模型输入的数据： 模型的输入可能有多个，也可能有各种各样的形状和类型。需要通过session.get_inputs()获取这些信息，然后根据这些信息创建相应的输入数据。\n如何确保使用了GPU： 需要在创建onnxruntime.InferenceSession时明确指定使用'CUDAExecutionProvider'。如果没有正确地使用GPU，可能会导致推理速度大幅度降低。\n如何准确计时： 选择忽略了第一次推理的时间，因为第一次推理可能会包含一些初始化操作，导致耗时偏大。\nPython脚本代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import numpy as np import onnxruntime import time # 加载模型 model_path = \u0026#34;/home/metoak/Projects/ros_project/src/metoak_camera_driver/models/model_640_384_6125.onnx\u0026#34; # session = onnxruntime.InferenceSession(model_path) session = onnxruntime.InferenceSession(model_path, providers=[\u0026#39;CUDAExecutionProvider\u0026#39;]) # 获取输入名称和形状 input_names = [inp.name for inp in session.get_inputs()] input_shapes = [inp.shape for inp in session.get_inputs()] print(\u0026#34;Input names:\u0026#34;, input_names) print(\u0026#34;Input shapes:\u0026#34;, input_shapes) # 创建两个随机输入 input1 = np.random.randint(0, 256, size=input_shapes[0]).astype(np.uint8) input2 = np.random.randint(0, 256, size=input_shapes[1]).astype(np.uint8) # 准备输入数据 inputs = {input_names[0]: input1, input_names[1]: input2} # 推理次数 num_inferences = 100 times = [] # 执行推断并计时 for i in range(num_inferences): start = time.time() output = session.run(None, inputs) end = time.time() inference_time = end - start if i == 0: continue times.append(inference_time) print(f\u0026#34;Inference {i+1}, Time: {inference_time * 1000} ms\u0026#34;) # 计算平均推断时间 avg_time = sum(times) / (num_inferences - 1) # 打印平均推断时间 print(f\u0026#34;Average inference time: {avg_time * 1000} ms\u0026#34;) 模型速度基准 1 2 3 4 5 6 7 8 9 10 Model name: model_640_384_6125.onnx Input names: [\u0026#39;onnx::Cast_0\u0026#39;, \u0026#39;onnx::Cast_1\u0026#39;] Input shapes: [[1, 3, 384, 640], [1, 3, 384, 640]] Inference 2, Time: 299.41678047180176 ms Inference 3, Time: 306.2708377838135 ms ... Inference 98, Time: 332.33070373535156 ms Inference 99, Time: 317.1732425689697 ms Inference 100, Time: 333.3919048309326 ms Average inference time: 322.4095816564078 ms 第二步：使用TensorRT实现加速 如果使用的3080Ti显卡\n浮点计算能力：\nSingle Precision Perf.: 34.1 TFLOPS\nTensor Perf. (FP16): 136 TFLOPS\nTensor Perf. (FP16-Sparse): ==273 TFLOPS==\n如果使用Tensor来处理全部精度为16位浮点数的算法，那么理论的运算速率可以达到至少是单精度浮点运算的 136 / 34.1 = 3.99，也就是4倍速度的提升，而如果模型本身就是混合进度进行运算的，那么提升的效果会更大！（如果再考虑上稀疏优化，那么Tensor加速的倍数会更高，达到 273 / 34.1 = 8倍的提升）\n🌟TensorRT环境部署 安装方法对了，安装其实很简单，一句话说明就是：\n==cuda、tensorrt都使用dep包，然后使用apt安装==\n但是细节很关键！下面的过程涉及安装CUDA、cuDNN、TensorRT\n官方文档\n1. 安装CUDA CUDA和Nvida Driver这俩个是独立不影响的，所以单独安装CUDA版本即可，与Nvida Driver版本无关，比如安装==cuda 11.8==\nUbuntu20.04:\n1 2 3 4 5 6 7 wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda-repo-ubuntu2004-11-8-local_11.8.0-520.61.05-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu2004-11-8-local_11.8.0-520.61.05-1_amd64.deb sudo cp /var/cuda-repo-ubuntu2004-11-8-local/cuda-*-keyring.gpg /usr/share/keyrings/ sudo apt-get update sudo apt-get -y install cuda 如果使用了runfile的方式安装过cuda了，那么最后一步只需要：\n1 sudo apt install cuda-libraries-11-8 2. 安装cuDNN 需要注册Nvidia开发者账号，下载对应的CUDA版本的即可，使用如下方式安装：\n官网\n下载deb版本即可\n1 2 3 4 sudo dpkg -i cudnn-local-repo-ubuntu2004-8.9.2.26_1.0-1_amd64.deb sudo cp /var/cudnn-local-repo-ubuntu2004-8.9.2.26/cudnn-local-6D0A7AE1-keyring.gpg /usr/share/keyrings/ sudo apt update sudo apt-get install libcudnn8 3. 安装TensorRT 官网\n下载最新版本TensorRT 8，同样需要开发者账号；\n下载对应系统的对应CUDA的最新版即可\n1 2 3 4 sudo dpkg -i nv-tensorrt-local-repo-ubuntu2004-8.6.1-cuda-11.8_1.0-1_amd64.deb sudo cp /var/nv-tensorrt-local-repo-ubuntu2004-8.6.1-cuda-11.8_1.0-1/*-keyring.gpg /usr/share/keyrings/ sudo apt update sudo apt install tensorrt 使用trtexec工具实现模型转换 1 2 # 使用例子 trtexec --onnx=/home/metoak/Projects/ros_project/src/metoak_camera_driver/models/model_sim_1280_704.onnx --saveEngine=model_1280_704_fp16.engine --fp16 --avgRuns=100 可以使用trtexec这个工具来直接转换onnx模型，但实际可能不理想或者遇到一些问题：\nonnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32. [07/25/2023-10:29:30] [W] [TRT] onnx2trt_utils.cpp:400: One or more weights outside the range of INT32 was clamped [07/25/2023-10:29:30] [I] Finished parsing network model. Parse time: 1.3651 [07/25/2023-10:29:30] [I] [TRT] Graph optimization time: 0.282094 seconds. [07/25/2023-10:29:30] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2630, GPU 3661 (MiB) [07/25/2023-10:29:30] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 2630, GPU 3671 (MiB) [07/25/2023-10:29:30] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\nYour ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.: 这个警告表明你的ONNX模型中包含了INT64类型的权重，而TensorRT原生不支持INT64类型。TensorRT正在尝试将这些权重转换（cast）为INT32类型。这可能会导致一些精度损失，因为INT32的范围小于INT64。\nOne or more weights outside the range of INT32 was clamped: 这个警告表明在转换权重类型的过程中，有一个或多个权重的值超出了INT32类型的范围，并被压缩（clamp）到了INT32的范围内。这也可能会导致一些精度损失。\nFinished parsing network model. Parse time: 1.3651: 这表示TensorRT已经完成了对ONNX模型的解析，整个解析过程花费了1.3651秒。\nGraph optimization time: 0.282094 seconds: 这表示TensorRT对模型进行优化的时间是0.282094秒。\nInit cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2630, GPU 3661 (MiB) 和 Init cuDNN: CPU +0, GPU +10, now: CPU 2630, GPU 3671 (MiB): 这些行是关于TensorRT初始化cuBLAS和cuDNN库（这些库用于GPU加速的线性代数运算和深度神经网络计算）所需内存的信息。这些行显示了在初始化这些库后，CPU和GPU内存的使用情况。\nLocal timing cache in use. Profiling results in this builder pass will not be stored.: 这表示TensorRT正在使用本地的计时缓存进行操作，而这次的分析结果不会被存储。\n使用trtexec来转换模型，一般情况下，得到下面的输出时，会成功，但也有可能得到模型无法正常使用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [07/25/2023-14:44:13] [I] Starting inference [07/25/2023-14:44:16] [I] Warmup completed 12 queries over 200 ms [07/25/2023-14:44:16] [I] Timing trace has 180 queries over 3.05014 s [07/25/2023-14:44:16] [I] [07/25/2023-14:44:16] [I] === Trace details === [07/25/2023-14:44:16] [I] Trace averages of 100 runs: [07/25/2023-14:44:16] [I] Average on 100 runs - GPU latency: 16.9045 ms - Host latency: 17.8527 ms (enqueue 11.5822 ms) [07/25/2023-14:44:16] [I] [07/25/2023-14:44:16] [I] === Performance summary === [07/25/2023-14:44:16] [I] Throughput: 59.0137 qps [07/25/2023-14:44:16] [I] Latency: min = 14.8851 ms, max = 27.6843 ms, mean = 17.7984 ms, median = 16.9878 ms, percentile(90%) = 21.1488 ms, percentile(95%) = 23.4043 ms, percentile(99%) = 27.5272 ms [07/25/2023-14:44:16] [I] Enqueue Time: min = 3.59753 ms, max = 22.9529 ms, mean = 11.5029 ms, median = 10.7087 ms, percentile(90%) = 13.9353 ms, percentile(95%) = 16.418 ms, percentile(99%) = 20.6213 ms [07/25/2023-14:44:16] [I] H2D Latency: min = 0.0994873 ms, max = 0.790527 ms, mean = 0.303106 ms, median = 0.268311 ms, percentile(90%) = 0.458588 ms, percentile(95%) = 0.503906 ms, percentile(99%) = 0.564697 ms [07/25/2023-14:44:16] [I] GPU Compute Time: min = 14.0032 ms, max = 26.499 ms, mean = 16.8485 ms, median = 16.0006 ms, percentile(90%) = 19.6926 ms, percentile(95%) = 22.436 ms, percentile(99%) = 26.4436 ms [07/25/2023-14:44:16] [I] D2H Latency: min = 0.319824 ms, max = 1.09839 ms, mean = 0.646768 ms, median = 0.621216 ms, percentile(90%) = 0.887512 ms, percentile(95%) = 0.963013 ms, percentile(99%) = 1.05774 ms [07/25/2023-14:44:16] [I] Total Host Walltime: 3.05014 s [07/25/2023-14:44:16] [I] Total GPU Compute Time: 3.03273 s [07/25/2023-14:44:16] [W] * GPU compute time is unstable, with coefficient of variance = 15.1765%. [07/25/2023-14:44:16] [W] If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability. [07/25/2023-14:44:16] [I] Explanations of the performance metrics are printed in the verbose logs. [07/25/2023-14:44:16] [I] \u0026amp;\u0026amp;\u0026amp;\u0026amp; PASSED TensorRT.trtexec [TensorRT v8601] # /usr/src/tensorrt/bin/trtexec --onnx=/home/metoak/Projects/ros_project/src/metoak_camera_driver/models/model_640_384_6125.onnx --saveEngine=model_640_384.engine --fp16 --avgRuns=100 这是通过使用TensorRT工具trtexec的一段测试输出，下面是每个重要部分的解释：\n[I] Warmup completed 12 queries over 200 ms：在开始正式计时之前，trtexec首先执行了12个预热查询，总耗时200毫秒。\n[I] Timing trace has 180 queries over 3.05014 s：这表示运行了180次推断，总耗时约为3.05秒。\n[I] Average on 100 runs - GPU latency: 16.9045 ms - Host latency: 17.8527 ms：这表示在100次运行中，平均每次推断在GPU上的延迟是16.9045毫秒，在主机上的延迟是17.8527毫秒。\n[I] Throughput: 59.0137 qps：这表示模型的吞吐量为59.0137查询每秒（qps）。\n[I] Latency: min = 14.8851 ms, max = 27.6843 ms, mean = 17.7984 ms, median = 16.9878 ms：这是对延迟的一些统计描述，包括最小值、最大值、平均值和中位数。\n[W] * GPU compute time is unstable, with coefficient of variance = 15.1765%.：这是一个警告，表示GPU计算时间的变化是不稳定的，方差系数为15.1765%，可能需要通过锁定GPU时钟频率或使用--useSpinWait选项来改善这种情况。\n\u0026amp;\u0026amp;\u0026amp;\u0026amp; PASSED TensorRT.trtexec [TensorRT v8601] # /usr/src/tensorrt/bin/trtexec --onnx=/home/metoak/Projects/ros_project/src/metoak_camera_driver/models/model_640_384_6125.onnx --saveEngine=model_640_384.engine --fp16 --avgRuns=100：这表示trtexec测试通过了，并且显示了你用来运行trtexec的命令行参数。\n验证engine模型是否可用？ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import tensorrt as trt TRT_LOGGER = trt.Logger(trt.Logger.WARNING) trt_runtime = trt.Runtime(TRT_LOGGER) engine_path = \u0026#34;/home/metoak/Projects/models/model_resnet101.engine\u0026#34; with open(engine_path, \u0026#34;rb\u0026#34;) as f: engine_data = f.read() engine = trt_runtime.deserialize_cuda_engine(engine_data) print(\u0026#34;Number of bindings: \u0026#34;, engine.num_bindings) for i in range(engine.num_bindings): binding_name = engine.get_tensor_name(i) print(\u0026#34;Binding \u0026#34;, i, \u0026#34;:\u0026#34;, \u0026#34;Binding name: \u0026#34;, binding_name, \u0026#34;Binding shape: \u0026#34;, engine.get_tensor_shape(binding_name), \u0026#34;Binding data type: \u0026#34;, engine.get_tensor_dtype(binding_name), \u0026#34;Is binding an input? \u0026#34;, engine.get_tensor_mode(binding_name) == trt.TensorIOMode.INPUT) print(\u0026#34;Number of layers: \u0026#34;, engine.num_layers) 如果所有的打印输出都正常，那么说明转换的模型可以，==并且以正常方法进行推理也是可行的==\n之后就是将模型推理部分整合进入项目代码即可。\n第三步：一般的方法不行怎么办？（‼️非常关键） 使用trtexec提供的转换模型方法，无法直接使用\n原因分析：模型比较复杂，TensorRT无法支持 —— 这个问题在安装TensorRT最新版之后解决，但TensorRT增加了新特征（使用第三方插件），无法直接使用trtexec转换后的模型：\nIn TensorRT 8.6 there are two implementations of InstanceNormalization that may perform differently depending on various parameters. By default the parser will insert an InstanceNormalization plugin layer as it performs best for general use cases. Users that want to benchmark using the native TensorRT implementation of InstanceNorm can set the parser flag kNATIVE_INSTANCENORM prior to parsing the model. For building version compatible or hardware compatible engines, this flag must be set.\n使用python脚本来处理，debug功能较弱，尝试使用C++的代码来重写整个工具链\n使用新版本特性来转换Onnx 直接上代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 // Usage: ./onnx_to_tensorrt onnx_path #include \u0026lt;NvInfer.h\u0026gt; #include \u0026lt;NvOnnxParser.h\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;iostream\u0026gt; // 继承自 nvinfer1::ILogger 以便我们可以修改日志处理方式 class Logger : public nvinfer1::ILogger { public: void log(Severity severity, const char *msg) noexcept override { // 调试完整信息 if (severity != Severity::kINFO) std::cout \u0026lt;\u0026lt; msg \u0026lt;\u0026lt; std::endl; } } gLogger; #include \u0026lt;fstream\u0026gt; int main(int argc, char *argv[]) { if (argc \u0026lt; 2) { std::cerr \u0026lt;\u0026lt; \u0026#34;Please specify path to the ONNX model.\\n\u0026#34;; return 1; } auto builder = nvinfer1::createInferBuilder(gLogger); const auto explicitBatch = 1U \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(nvinfer1::NetworkDefinitionCreationFlag::kEXPLICIT_BATCH); auto network = builder-\u0026gt;createNetworkV2(explicitBatch); auto config = builder-\u0026gt;createBuilderConfig(); auto parser = nvonnxparser::createParser(*network, gLogger); // 注意这里：使用TensorRT自带的InstanceNormalization实现 auto flag = 1U \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(nvonnxparser::OnnxParserFlag::kNATIVE_INSTANCENORM); parser-\u0026gt;setFlags(flag); std::string model_path = argv[1]; if (!parser-\u0026gt;parseFromFile(model_path.c_str(), 1)) { std::cerr \u0026lt;\u0026lt; \u0026#34;Failed to parse the ONNX model.\\n\u0026#34;; return 1; } auto engine = builder-\u0026gt;buildEngineWithConfig(*network, *config); if (!engine) { std::cerr \u0026lt;\u0026lt; \u0026#34;Failed to create the TensorRT engine.\\n\u0026#34;; return 1; } std::string engine_path = model_path.substr(0, model_path.find_last_of(\u0026#39;.\u0026#39;)) + \u0026#34;.trt\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34;Serializing the TensorRT engine to \u0026#34; \u0026lt;\u0026lt; engine_path \u0026lt;\u0026lt; \u0026#34;...\\n\u0026#34;; nvinfer1::IHostMemory *serializedModel = engine-\u0026gt;serialize(); std::ofstream engineFile(engine_path, std::ios::binary); engineFile.write(reinterpret_cast\u0026lt;const char *\u0026gt;(serializedModel-\u0026gt;data()), serializedModel-\u0026gt;size()); engineFile.close(); serializedModel-\u0026gt;destroy(); parser-\u0026gt;destroy(); engine-\u0026gt;destroy(); config-\u0026gt;destroy(); network-\u0026gt;destroy(); builder-\u0026gt;destroy(); return 0; } 直接使用此代码进行模型转换;\n验证模型可行性 \u0026amp;\u0026amp; 模型推理过程 这里直接写一个demo，来实现模型的推理过程，从而验证模型的可行性，这部分直接上代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 // tensorrt_image.cpp #include \u0026lt;NvInfer.h\u0026gt; #include \u0026lt;opencv2/opencv.hpp\u0026gt; #include \u0026lt;chrono\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;fstream\u0026gt; #define TEST_TIME 0 // 继承自 nvinfer1::ILogger 以便我们可以修改日志处理方式 class Logger : public nvinfer1::ILogger { public: void log(Severity severity, const char *msg) noexcept override { // Severity::kVERBOSE 或 kINFO // 只打印错误信息 if (severity == Severity::kERROR \u0026amp;\u0026amp; severity == Severity::kINTERNAL_ERROR) { std::cout \u0026lt;\u0026lt; msg \u0026lt;\u0026lt; std::endl; } } } gLogger; int main(int argc, char *argv[]) { if (argc \u0026lt; 2) { std::cerr \u0026lt;\u0026lt; \u0026#34;Please specify path to the engine file.\\n\u0026#34;; return 1; } // 打开并反序列化 TensorRT engine std::ifstream engineFile(argv[1], std::ios::binary); if (!engineFile) { std::cerr \u0026lt;\u0026lt; \u0026#34;Error opening engine file\\n\u0026#34;; return -1; } engineFile.seekg(0, engineFile.end); long int fsize = engineFile.tellg(); engineFile.seekg(0, engineFile.beg); std::vector\u0026lt;char\u0026gt; engineData(fsize); engineFile.read(engineData.data(), fsize); if (!engineFile) { std::cerr \u0026lt;\u0026lt; \u0026#34;Error reading engine file\\n\u0026#34;; return -1; } nvinfer1::IRuntime *runtime = nvinfer1::createInferRuntime(gLogger); nvinfer1::ICudaEngine *engine = runtime-\u0026gt;deserializeCudaEngine(engineData.data(), fsize, nullptr); // 创建推理上下文 nvinfer1::IExecutionContext *context = engine-\u0026gt;createExecutionContext(); // 读取并预处理图像 cv::Mat img1 = cv::imread(\u0026#34;/home/metoak/Projects/dependency/InferenceDemo/images/left.png\u0026#34;, cv::IMREAD_COLOR); cv::Mat img2 = cv::imread(\u0026#34;/home/metoak/Projects/dependency/InferenceDemo/images/right.png\u0026#34;, cv::IMREAD_COLOR); cv::resize(img1, img1, cv::Size(640, 384)); cv::resize(img2, img2, cv::Size(640, 384)); img1.convertTo(img1, CV_32FC3, 1 / 255.0); img2.convertTo(img2, CV_32FC3, 1 / 255.0); // 创建输入数据 int num_elements_input = 1 * 3 * 384 * 640; // 根据你的模型的实际需求进行调整 // 分割通道 std::vector\u0026lt;cv::Mat\u0026gt; channels1(3); std::vector\u0026lt;cv::Mat\u0026gt; channels2(3); cv::split(img1, channels1); cv::split(img2, channels2); std::vector\u0026lt;float\u0026gt; data1; std::vector\u0026lt;float\u0026gt; data2; // 将每个通道的数据添加到相应的向量 for (auto \u0026amp;channel : channels1) { std::vector\u0026lt;float\u0026gt; channelData(channel.begin\u0026lt;float\u0026gt;(), channel.end\u0026lt;float\u0026gt;()); data1.insert(data1.end(), channelData.begin(), channelData.end()); } for (auto \u0026amp;channel : channels2) { std::vector\u0026lt;float\u0026gt; channelData(channel.begin\u0026lt;float\u0026gt;(), channel.end\u0026lt;float\u0026gt;()); data2.insert(data2.end(), channelData.begin(), channelData.end()); } #if TEST_TIME == 1 // 打印图像的前10个像素值 std::cout \u0026lt;\u0026lt; \u0026#34;First 10 pixels of the input image:\\n\u0026#34;; for (int i = 0; i \u0026lt; 10; ++i) { std::cout \u0026lt;\u0026lt; data1[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } #endif // 转换回图像 std::vector\u0026lt;cv::Mat\u0026gt; channels(3); for (int c = 0; c \u0026lt; 3; ++c) { channels[c] = cv::Mat(384, 640, CV_32F, data1.data() + c * 384 * 640); } cv::Mat imgData1; cv::merge(channels, imgData1); // 归一化并将数据类型转换回 CV_8UC3 以便显示 imgData1 = imgData1 * 255.0; imgData1.convertTo(imgData1, CV_8UC3); cv::imwrite(\u0026#34;/home/metoak/Projects/input.png\u0026#34;, imgData1); // 分配 GPU 内存用于输入 void *gpuData1; void *gpuData2; cudaMalloc(\u0026amp;gpuData1, num_elements_input * sizeof(float)); cudaMalloc(\u0026amp;gpuData2, num_elements_input * sizeof(float)); // 复制输入数据到 GPU cudaMemcpy(gpuData1, data1.data(), num_elements_input * sizeof(float), cudaMemcpyHostToDevice); cudaMemcpy(gpuData2, data2.data(), num_elements_input * sizeof(float), cudaMemcpyHostToDevice); // 根据你的模型的输出大小来创建输出数据 int num_elements_output = 1 * 3 * 384 * 640; // 你需要根据你的模型的实际输出大小进行调整 // 分配 GPU 内存用于输出 void *gpuOutput[7]; for (int i = 0; i \u0026lt; 7; ++i) { cudaMalloc(\u0026amp;gpuOutput[i], num_elements_output * sizeof(float)); } // 创建输入输出的指针数组 void *bindings[9] = {gpuData1, gpuData2, gpuOutput[0], gpuOutput[1], gpuOutput[2], gpuOutput[3], gpuOutput[4], gpuOutput[5], gpuOutput[6]}; // 创建 CUDA stream cudaStream_t stream; cudaStreamCreate(\u0026amp;stream); #if TEST_TIME == 1 // 100次推理 cudaEvent_t start, stop; cudaEventCreate(\u0026amp;start); cudaEventCreate(\u0026amp;stop); float totalTime = 0; int iteration = 100; // 推理次数 for (int i = 0; i \u0026lt; iteration; i++) { cudaEventRecord(start, stream); // 执行推理 context-\u0026gt;enqueue(1, bindings, stream, nullptr); std::cout \u0026lt;\u0026lt; \u0026#34;Inference iteration: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; cudaEventRecord(stop, stream); cudaEventSynchronize(stop); float milliseconds = 0; cudaEventElapsedTime(\u0026amp;milliseconds, start, stop); totalTime += milliseconds; } float avgTime = totalTime / iteration; std::cout \u0026lt;\u0026lt; \u0026#34;Average inference time per image: \u0026#34; \u0026lt;\u0026lt; avgTime \u0026lt;\u0026lt; \u0026#34; ms.\u0026#34; \u0026lt;\u0026lt; std::endl; cudaEventDestroy(start); cudaEventDestroy(stop); #else // 执行推理 context-\u0026gt;enqueue(1, bindings, stream, nullptr); #endif // 将需要的输出数据从 GPU 内存复制回 CPU 内存 std::vector\u0026lt;float\u0026gt; outputData(num_elements_output); cudaMemcpy(outputData.data(), gpuOutput[6], num_elements_output * sizeof(float), cudaMemcpyDeviceToHost); #if TEST_TIME == 1 // 打印图像的前10个像素值 std::cout \u0026lt;\u0026lt; \u0026#34;First 10 pixels of the output image:\\n\u0026#34;; for (int i = 0; i \u0026lt; 10; ++i) { std::cout \u0026lt;\u0026lt; outputData[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } #endif // 将一维向量转换为3通道图像 cv::Mat img_out(384, 640, CV_32F, outputData.data()); cv::Mat img_out_normalized; cv::normalize(img_out, img_out_normalized, 0, 255, cv::NORM_MINMAX, CV_8U); cv::Mat img_color_mapped; cv::applyColorMap(img_out_normalized, img_color_mapped, cv::COLORMAP_JET); // cv::resize(img_out, img_out, cv::Size(1920, 1080)); // 保存图像 cv::imwrite(\u0026#34;/home/metoak/Projects/output.png\u0026#34;, img_color_mapped); std::cout \u0026lt;\u0026lt; \u0026#34;Output image size: \u0026#34; \u0026lt;\u0026lt; img_out_normalized.size() \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; // 清理 cudaFree(gpuData1); cudaFree(gpuData2); for (int i = 0; i \u0026lt; 7; ++i) { cudaFree(gpuOutput[i]); } context-\u0026gt;destroy(); engine-\u0026gt;destroy(); runtime-\u0026gt;destroy(); // 销毁 CUDA stream cudaStreamDestroy(stream); return 0; } 这里有几点注意：\n数据的前处理需要根据的模型的结构来调整，这里是输入两组图像\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // 分配 GPU 内存用于输入 void *gpuData1; void *gpuData2; cudaMalloc(\u0026amp;gpuData1, num_elements_input * sizeof(float)); cudaMalloc(\u0026amp;gpuData2, num_elements_input * sizeof(float)); // 复制输入数据到 GPU cudaMemcpy(gpuData1, data1.data(), num_elements_input * sizeof(float), cudaMemcpyHostToDevice); cudaMemcpy(gpuData2, data2.data(), num_elements_input * sizeof(float), cudaMemcpyHostToDevice); // 根据你的模型的输出大小来创建输出数据 int num_elements_output = 1 * 3 * 384 * 640; // 你需要根据你的模型的实际输出大小进行调整 // 分配 GPU 内存用于输出 void *gpuOutput[7]; for (int i = 0; i \u0026lt; 7; ++i) { cudaMalloc(\u0026amp;gpuOutput[i], num_elements_output * sizeof(float)); } 这一块就是根据模型的结构来调整输入和输出的\n这里加入了很多手工调试的信息，比如保存模型输入前后的图像来验证\n这是一段完整的推理代码，如果这一段代码能够成功运行，那么将这段代码整合到实际的功能中，定义好输入输出的接口，模型也一样可以正常使用\n这里很多地方是手动申请内存空间的，所以一定注意使用后的销毁（当然也可以使用类来实现析构这种方法）\n1 2 3 4 5 6 7 8 9 10 11 12 // 清理 cudaFree(gpuData1); cudaFree(gpuData2); for (int i = 0; i \u0026lt; 7; ++i) { cudaFree(gpuOutput[i]); } context-\u0026gt;destroy(); engine-\u0026gt;destroy(); runtime-\u0026gt;destroy(); // 销毁 CUDA stream cudaStreamDestroy(stream); 注意理解这一部分的功能和必要性\n有了这两部分后，模型从ONNX到TensorRT，再到TensorRT的推理就打通了！\n对于上面的代码编译，直接使用cmake的方法，CMakeLists.txt例子如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 cmake_minimum_required(VERSION 3.10) project(OnnxInferenceDemo) # 调试模式 set(CMAKE_BUILD_TYPE Debug) set(CMAKE_CXX_STANDARD 14) # 设置METOAK的驱动路径 set(MO_SDK_DIR /opt/moak) find_package(OpenCV REQUIRED) find_package(PCL REQUIRED) find_package(CUDA REQUIRED) find_path(TENSORRT_INCLUDE_DIR NvInfer.h HINTS ${TENSORRT_ROOT_DIR} PATH_SUFFIXES include/) find_library(TENSORRT_LIBRARY nvinfer HINTS ${TENSORRT_ROOT_DIR} PATH_SUFFIXES lib lib64 lib/x64) find_library(TENSORRT_ONNX_LIBRARY nvonnxparser HINTS ${TENSORRT_ROOT_DIR} PATH_SUFFIXES lib lib64 lib/x64) set(TENSORRT_INCLUDE_DIRS ${TENSORRT_INCLUDE_DIR}) set(TENSORRT_LIBRARIES ${TENSORRT_LIBRARY} ${TENSORRT_ONNX_LIBRARY}) include_directories(${TENSORRT_INCLUDE_DIRS}) include_directories(${CUDA_INCLUDE_DIRS}) include_directories( include ${OpenCV_INCLUDE_DIRS} ${MO_SDK_DIR}/3rdparty/ONNX/x86_64/GPU/include ) link_directories(${MO_SDK_DIR}/3rdparty/ONNX/x86_64/GPU/lib) link_directories(lib) add_executable(onnx_to_tensorrt src/onnx_to_tensorrt.cpp) target_link_libraries(onnx_to_tensorrt ${TENSORRT_LIBRARIES} ${CUDA_LIBRARIES}) add_executable(tensorrt_image src/tensorrt_image.cpp) target_link_libraries(tensorrt_image ${OpenCV_LIBS} ${TENSORRT_LIBRARIES} ${CUDA_LIBRARIES}) 总结 使用TensorRT来进行模型加速，如果很凑巧，当前版本无法支持Onnx的模型转换，那可能正常方法就无法使用，就比如这里的模型，在TensorRT 8.6之前的版本，都无法完成正常的转换，并且报出段错误异常，这种情况，可能就是无法支持；\n而使用了最新版本发现，模型可以转换，直接无法直接进行模型的推理，那么就要排查一下是哪里的问题了，比如这里的模型，在TensorRT 8.6.1上可以完整转换，但是需要手动指定使用的插件，内建的还是其他的，这就会影响模型对应的推理程序的书写。\n另外一点：\nTensorRT的安装和环境部署其实很简单，但是因为网络上的搜索出来的教程杂七杂八，导致没有按照统一的方式来安装CUDA、cuDNN、TensorRT，导致apt安装时出现依赖异常，其实只要全部采用dep包管理的方式来安装就不会有问题。\n","permalink":"http://ahaknow.com/posts/know/%E4%BD%BF%E7%94%A8tensorrt%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E5%8A%A0%E9%80%9F/","summary":"第一步：先测试模型的基准速度 使用ONNX测试模型运行速度基准 目标： 使用ONNX Runtime来测试机器学习模型的推理时间基准。 步骤： 安装ON","title":"使用TensorRT实现模型加速"},{"content":" 慌慌张张、匆匆忙忙，\n一天一天日子久了，\n好像什么都会忘掉……\n这大半年！！！！\n一种状态，习惯它久了，便心安理得了；\n日记写得是断断续续，但能明确记得的：那些每天晚上写过日记的日子，每一天都很充实，不后悔！\n每天锻炼真是个好东西啊！记得自己感慨过：“跑步是良药，药可真不能停啊！”\n最要命的一句借口可能就是：“我没时间……”；真的是没时间？打死我都不信！\n每天跑个步，每天记个总结，不需要多，只要是有，就已经完全不同；忙忙碌碌，一天又一天，就这样过去的话，确实，只要不想起来，就好像过得很舒服，但只要稍微一停下来，稍微想一想：\n“我要做什么？我想要什么？我在干什么？哎，等一下，怎么突然就空虚了？”\n庆幸自己还能意识到，想到了就去改一改：\n从晚睡晚起到早睡早起；\n容易忘记那就再设个提醒；\n好久没锻炼了，恢复起来每天继续；\n对了！\n现在人工智能这么会输出，我再不自己想想，写写东西，真的连做机器都不配了！\n","permalink":"http://ahaknow.com/posts/diary/0904%E4%B9%A0%E6%83%AF%E4%B8%8E%E6%89%93%E7%A0%B4%E4%B9%A0%E6%83%AF/","summary":"慌慌张张、匆匆忙忙， 一天一天日子久了， 好像什么都会忘掉…… 这大半年！！！！ 一种状态，习惯它久了，便心安理得了； 日记写得是断断续续，但能明确记","title":"0904 习惯与打破习惯"},{"content":"已经停滞学习快两年了！\n有一个很痛心的坏毛病：\n知道有很多好的课程可以学习，可以快速成长，但是，知道是知道，网页也点开了，可是……，一堆积，一篇也没有完整看完过！\n完全失去了考研之前的那种学习的渴望！\n‼️身边资源变多了，反而不知道把握了！！\n很遗憾！很可惜！\n其实有很多想说，想要感慨的！ –\u0026gt; 研究生还没毕业工作呢！！可是，却已然摆出了一副停滞学习的念头！担心GPT？担心AGI？完全没必要！！老老实实把能找到的好课都认真学习完！！\n记住！\n一个一个学！\n贪多必失！！\n","permalink":"http://ahaknow.com/posts/diary/0712%E5%81%9C%E6%BB%9E%E5%AD%A6%E4%B9%A0/","summary":"已经停滞学习快两年了！ 有一个很痛心的坏毛病： 知道有很多好的课程可以学习，可以快速成长，但是，知道是知道，网页也点开了，可是……，一堆积，一篇","title":"0712 停滞学习"},{"content":"就是遇到了那种代码写的跟狗屎一样的，看的眼睛都疼。\n警醒自己，要保持好写好规范代码的习惯。\n……\n一晃就是一个月！！\n","permalink":"http://ahaknow.com/posts/diary/0223%E6%95%B4%E6%B4%81%E4%BB%A3%E7%A0%81md/","summary":"就是遇到了那种代码写的跟狗屎一样的，看的眼睛都疼。 警醒自己，要保持好写好规范代码的习惯。 …… 一晃就是一个月！！","title":"0223 整洁代码"},{"content":" 先说今天发生的：\n一直以来，都传闻，电商平台会卖二手，也就是被退货的重新再卖给其他顾客。\n话说，这个传闻，今天被证实了！\n是什么原因呢？（如果显示器也有心智，那遇到这事儿，心里应该也很纠结吧。）\n今天买了一台显示器，送到之后看包装，明显感觉是故意处理过的一样，自带包装有破损，外面裹了一层塑料膜；好不容易拆开，打开看，看到显示器背后的支架处有使用过的痕迹，这时我心里一凉，白天的担心成真了？！显示器很重，好不容易组装好，开机、连接电脑，显示器启动画面出来了，然后……没然后了，跳出来一个不是英文的提示窗口，不是英文！仔细一看，像是法语，走到这里，我几乎笃定了我的怀疑，我买到的是一台二手退货机。\n联系客服，说明情况，这台显示器无法正常连接使用，有使用痕迹，并且无法连接电脑后显示非英文提示窗。客服一般都倾向于推卸责任，一个劲说让我拔了在插，我说这种方法没有用，你帮我换一台机吧，那这个时候客服说要给我打技术支持的电话，我说可以，等等电话，不行还是得换。\n电话来了，没跟我这种情况的原因是啥，一个劲的说让我手动切换输入源，我说，这个显示器默认选项不就是自动获取输入源嘛，怎么还要手动选了，而且这个提示语还是法语？你见过哪个正经显示器出厂后设置成非英语的，这种情况的话我有理由换一台新的，然后，电话那会嘀咕了一句：“换一台全新未拆分的吧”。这下好了！我反问到：“全新未拆分啥意思，现在从网上店铺买，不是全新的，难道还有其他选项可以选？”\n这个时候，我明白了，商家背后的操作可能比你想得还可怕。现在很多商品都有七天无理由，那退回去了怎么处理呢，不二次销售那不得亏死了，所以，如果“幸运”，那很可能买东西就会买到被退回去的二手，因为商家也侥幸，觉得有些顾客会嫌麻烦，不计较，毕竟这部分还是有一些人的。（写到这，突然想到一个思细恐极的事，如果购物行为足够可以分析一个人的性格，比如他不在乎到手的包装如何，对一点小毛病就认忍了不退了，那商家会不会就拿这些人开刀，专门给他们发二手呢！）\n事情在往后就有趣了，客服在app上给我留言说：售后给您选一个全新未拆分的，我当时就乐了，很想说：“你知不知道这种说法很容易让人产生误解：你们就是会卖二手啊！”\n再之后换货，看到退换须知，明确提到了会二次销售，但真的是二次嘛？很可能是三次四次吧，或者包装重新包一个，再卖出去，顾客收到了没考虑那么多，于是，又一次侥幸成功了。\n哎……有点乱糟糟的。\n等换货来再看看是什么情况吧！\n","permalink":"http://ahaknow.com/posts/diary/0222%E4%B8%80%E5%8F%B0%E6%98%BE%E7%A4%BA%E5%99%A8%E7%9A%84%E5%BF%83%E8%B7%AF%E5%8E%86%E7%A8%8B/","summary":"先说今天发生的： 一直以来，都传闻，电商平台会卖二手，也就是被退货的重新再卖给其他顾客。 话说，这个传闻，今天被证实了！ 是什么原因呢？（如果显示","title":"0222 一台显示器的心路历程"},{"content":" 还真的忘记了昨天发生了时候 —— 写在2023年2月22日晚\n记得昨天晚上提醒自己说：\n快要睡觉了，一定要赶在睡觉前把今天的日记写了！—— 谁料，一忙就忙过了头，倒床就睡了。\n现在让我回想，昨天印象最深刻的事，我还真的回忆不起来了，今天有今天的特别记忆，但似乎保鲜期也只有一觉之隔，估计明早一起，也就忘了一半，等明晚再要记录时，估计已经是支零破碎，想也想不起来，或者直接想岔劈了吧。\n算是一次教训！\n今天打好样，先记录日记，再继续学习，学习结束了，看有啥补充的！\n🌟今天会尤克里里的扫弦了！\n晚上做饭的时候，听到一首歌，旋律似曾相识，很像很像，但就快要想到的时候卡住了。后来，索性不想了，又仔细听了听这首歌，忽然明白：\n为什么会有“万能和弦”，为什么会有“节奏型”，这不就是一种重复和组合，让原本直接的唱歌变得更生动，所以仔细听，很多歌的伴奏旋律都有相似之处，那些似曾相识也就不觉得什么了。\n","permalink":"http://ahaknow.com/posts/diary/0221%E6%98%A8%E5%A4%A9%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/","summary":"还真的忘记了昨天发生了时候 —— 写在2023年2月22日晚 记得昨天晚上提醒自己说： 快要睡觉了，一定要赶在睡觉前把今天的日记写了！—— 谁料，一忙","title":"0221 昨天发生了什么！"},{"content":" 周末，龙哥来家里做客，闲聊之余，被他发现了久挂在门后的尤克里里，龙哥很会弹吉他，在它熟悉了尤克里里的几种和弦的按法后，调好了音，便弹唱了起来，一首《南方》随心而来……\n这把尤克里里我很久之前就买了，从本科毕业带回了家，又从家带到了研究生的现在，几经波折，似乎总还有些侥幸的情愫在，甚至去年某个夜深人静的时候还在想，明年新一年，定一个计划，那不如从练好尤克里里开始，如果沿着这段记忆再往前，很有更早的印象，似乎是；在自己生日之前，学会用它弹唱一场生日快乐歌。几经周转，还是放弃了。\n直到这一次，身临其境地体会到了尤克里里在自己面前弹奏，自己内心的那点小火苗忽然就会点燃了，也可以是，瞬间就“悟”了！\n听着龙哥弹唱结束，知道了几个名词：“万能和弦”、“节奏型”、“1645”，听他的描绘，我如果能把这几个掌握了，那一半的歌，自己都能弹唱了。\n借着这股劲，开始搜索和学习……\n弹琴，肯定没有速成，只不过你是知道了更快的路径，路，还是要一步一步走的，哪怕是捷径，也是要一步步走完的。\n这其中最重要的还是练习，就像练习打字一样，熟能生巧罢了。而人的大脑呢，会自己进行学习，只需要给他刺激和反馈（让我想起了，第一次走铁轨，那真的是，走一步看一眼，生怕崴了脚，但时隔多日自己再去走的时候发现，自己竟然能在铁轨上跑起来了，你是大脑能不厉害嘛！）\n练琴这里也是，每天练一点，比如，今天先练习按和弦，练习拨弦，哪怕很僵硬，很生疏，也没事，都是第一次嘛！适度练好了，就可以去睡觉了。\n—— 第二天再练习的时候，明显感受到，手指要灵活多了。 —— 2023.2.21记。\n","permalink":"http://ahaknow.com/posts/diary/0220%E5%BC%80%E5%A7%8B%E5%AD%A6%E4%B9%A0%E5%B0%A4%E5%85%8B%E9%87%8C%E9%87%8C/","summary":"周末，龙哥来家里做客，闲聊之余，被他发现了久挂在门后的尤克里里，龙哥很会弹吉他，在它熟悉了尤克里里的几种和弦的按法后，调好了音，便弹唱了起来","title":"0220 开始学习尤克里里"},{"content":" — 这是什么牌子的啊？\n— 哦，大疆的啊，那炸机的时候有趣啊\n— …… （我心里很想骂他，这种看着热闹说闲话的！）\n— 诶诶额！撞树上了！赶紧操控一下\n— 诶诶额！！过来接一下！\n— …… 咚！没接住，重重摔在地上，四个胳膊断了三根。。。\n从来没有想过这件事会发生在自己身上。\n就是怪有人在旁边说闲话？\n如果是自己一个人就肯定不会发生这种事！\n当时我在想啥？—— 我想骂他，所以一不留神……撞上去了。\n但要反思的还是自己：\n为什么自己不够小心！原本就应该在完全空旷的场地上飞啊！\n起飞之后，我其实并没有看着它，心里还在嘀咕，这种看热闹说风凉话的咋这么恶心人，结果，我的完全自信变成了那种人口中真正玩笑。\n想想生活中的其他事也是，常常说，要做自己，管别人说什么呢，也许有的时候还真不是这样，被说什么就来什么，一语成谶！\n还是要多反思反思自己：\n做任何事情的时候，都要足够仔细小心，没有考虑周全的过度自信就是极端地欺骗自己！\n高手过招，总是一步算几步甚至几十步，即使这样，也还是步步为营，不然必定走不远。\n所以对自己而言，这一次算是买了十足的教训（基本上是摔断了，维修的话，算上成本和维修费，估计要大几百甚至上千了）：\n自己想好了就仔细做，你要专注的还是你当下在做的事情；\n上网搜了一下其他网友是咋么炸机的，看到最多的就是：\n没注意，撞到树上了！\n想想也是，只留意正前方看到的画面了，这个飞机如果没有其他方向的避障，就比如侧向和上向，自己要是不留神的话，遇到树了，那必然是死死纠缠上的结果。\n希望这个教训能让自己多反省反省！\n—— 夜里做了一场大噩梦，记得梦醒一半，感觉脑袋要炸掉一样，非常杂乱，很多真实与扭曲的混合，甚至有些害怕。 —— 2023.2.20记。\n","permalink":"http://ahaknow.com/posts/diary/0219%E5%88%9A%E8%B5%B7%E9%A3%9E%E5%B0%B1%E7%82%B8%E4%BA%86/","summary":"— 这是什么牌子的啊？ — 哦，大疆的啊，那炸机的时候有趣啊 — …… （我心里很想骂他，这种看着热闹说闲话的！） — 诶诶额！撞树上了！赶紧操控一下 — 诶诶","title":"0219 刚起飞就炸了"},{"content":" 脑海里突然蹦出这样一些话：\n“确实书读得太少了，GPT知道的东西这么多！\n人工智能真的有点‘可怕’了！\n书读多了，也不如会用这些工具啊！”\n突然就想写写东西了，整理整理自己的一些思考吧！\n现在的人工智能语言模型，本质上讲是“几乎所”有高质量电子形式资料的集合体，它“学习”的东西要比一个普通学生（正常升学到毕业）要多得多多，所以它“几乎”无所不知。\n所以，作为一个普通学生，面对它，会显得“弱小无知”很多。初见之时会有“害怕”，但熟悉之后更多的应该是一种欣慰：\n终于有一个工具，能够帮自己来快速学习了！以前，可能需要老师辅导（何为老师呢？因为他知道的比我多，我要向他学习），现在，通过人工智能语言模型就能自己加速自己的学习了，哪些遇到问题了，请教一下他，说不定就有了思路，通过这个思路再探索下去，继续阅读相关书籍、课程，不就事半功倍了！\n为啥准备写这段文字之前，心里感觉怪怪的呢？\n有一点确实挺可叹的，就是人的思考能力在不经意的退化，**忘记了要怎样提问，甚至忘记了要怎样思考一个问题，**如果一个学生只知道老师讲的，自己不理解也不问为什么，那确实，等他们从小学、中学，一路读到大学之后，很容易就会被人工智能颠覆了！\n我觉得，还是要庆幸的，至少现在能用上这么好用的工具，来加速自己的认知。我记得以前（四五年前吧，那会儿AlphaGo刚出来，但我迟钝了），以会使用Google搜索为荣，为什么呢？因为用Google搜索能够更准确得获得自己需要的信息。现在呢，我倒是觉得，有这么好的工具不会用，那就真的是太亏了！当然自己认知的脚步也不能停，有了好的工具，更能得心应手才是！\n所以，不是什么人工智能的末日恐慌论，反而是在警醒自己：\n时刻保持自己的思考能力\n遇到问题，先理清楚头绪，一步步来解决，哪些是自己认知范围外的，这一部分可以通过提问的方式来获得启发；哪些是自己可以进行补缺的，有针对性地对课程和资料进行学习\n有了好工具了，一定要先学会使用！能用机器解决的，就让机器来辅助自己\n先写这么多吧，可以像记流水账一样来记录。\n","permalink":"http://ahaknow.com/posts/diary/0218chatgpt%E7%9A%84%E6%80%9D%E8%80%83/","summary":"脑海里突然蹦出这样一些话： “确实书读得太少了，GPT知道的东西这么多！ 人工智能真的有点‘可怕’了！ 书读多了，也不如会用这些工具啊！” 突然就想","title":"0218 chatGPT的思考"}]