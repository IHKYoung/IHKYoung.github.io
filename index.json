[{"content":" 又干了一件：“为了一碟醋，包了一锅饺子”的事。。。\n就是重新开始学习Strang老爷子的线性代数课了，然后下载了之前国内字幕组的版本（多年不好好学，英语水平明显下降了！！！），这个版本的老问题一直存在，就是某些视频音轨缺失，只有右耳机有声音（这对于一个常年降噪耳机离耳只是为了补充电量的人来说，简直不能忍。。。）\n于是……\n都是4202年了，有什么问题是写个代码解决不了的呢？\n于是Python闪亮登场✨\n废话不多说，直接上代码：\nfrom moviepy.editor import VideoFileClip, AudioFileClip from pydub import AudioSegment # 加载视频文件 video_clip = VideoFileClip(\u0026#34;./麻省理工-线性代数/[P01]Lec01_方程组的几何解释.mp4\u0026#34;) # 提取音频并保存为临时文件 temp_audio_path = \u0026#34;temp_audio.mp3\u0026#34; video_clip.audio.write_audiofile(temp_audio_path) # 使用pydub处理音频 audio_segment = AudioSegment.from_file(temp_audio_path) # 如果原始音频是立体声，分割为单声道；否则直接复制为左右两个声道 if audio_segment.channels \u0026gt; 1: mono_channels = audio_segment.split_to_mono() right_channel = mono_channels[1] # 右声道 else: right_channel = audio_segment stereo_audio = AudioSegment.from_mono_audiosegments(right_channel, right_channel) # 将处理后的音频保存为另一个临时文件 stereo_audio_path = \u0026#34;temp_stereo_audio.mp3\u0026#34; stereo_audio.export(stereo_audio_path, format=\u0026#34;mp3\u0026#34;) # 使用moviepy将新音频设置回视频 new_audio_clip = AudioFileClip(stereo_audio_path) video_clip.audio = new_audio_clip # 输出处理后的视频文件 video_clip.write_videofile(\u0026#34;./麻省理工-线性代数/[P01]Lec01_方程组的几何解释（修复）.mp4\u0026#34;) # 清理临时文件 import os os.remove(temp_audio_path) os.remove(stereo_audio_path) 这里用到两个Python的工具：pydub、moviepy，直接安装就好。\nmoviepy可以同时处理视频和音频；pydub`主要处理音频部分，这个库提供了更灵活的音频处理功能，包括声道的操作。\npip install moviepy pip install pydub 同时确保系统中也安装了ffmpeg，因为pydub依赖于ffmpeg来处理音频文件：\nbrew install ffmpeg ","permalink":"https://ahaknow.com/posts/know/techmp4-audio-track-repair/","summary":"又干了一件：“为了一碟醋，包了一锅饺子”的事。。。 就是重新开始学习Strang老爷子的线性代数课了，然后下载了之前国内字幕组的版本（多年不好","title":"技术：MP4的音轨修复"},{"content":" 博客里使用了自定义的字体，最开始直接采用ttf文件加载的方式，因为博客内容全部是静态存在Github上的，这种直接加载多少有点慢，所以想了一些策略。\n压缩字体 先显示后加载 压缩字体 TrueType Font (TTF) 常见的字体文件格式，由Apple和Microsoft在上世纪80年代末共同开发。TTF使用二次贝塞尔曲线来描述字符形状，这种曲线可以精确地表示复杂形状，并且在放大时保持平滑。\nWeb Open Font Format (WOFF) WOFF是专门为Web设计的字体格式，2009年成为W3C的推荐标准。WOFF是基于TTF和OpenType字体格式的，但提供了更好的压缩和额外的元数据支持。WOFF存在两个版本：WOFF 1.0和WOFF 2.0。WOFF 2.0使用了更高效的压缩算法，文件大小比WOFF 1.0更小。\n总结 TTF是一种通用的字体格式，广泛用于各种操作系统和设备，提供了高质量的矢量字体展示。而WOFF是为Web设计的字体格式，它优化了文件大小和加载性能，特别适合在线使用。选择哪种格式取决于你的具体需求：如果你需要确保字体在各种环境中的广泛兼容性，TTF可能是更好的选择；如果你主要关注网页性能和加载速度，WOFF将是更优的选择。\n在MacOS上直接本地转换ttf字体。\n安装woff2工具：\nbrew install woff2 使用以下命令将TTF字体文件转换为WOFF2格式：\nwoff2_compress path/to/your/font.ttf 这会生成一个与原始TTF文件同名，但扩展名为.woff2的文件。例如，如果源文件名为font.ttf，输出文件将会是font.woff2。\n先显示后加载 这里主要是CSS的工作：\n在CSS文件中使用@font-face规则引入转换后的WOFF2字体文件，并通过font-display: swap;属性设置字体显示策略。这样做可以确保文本在字体文件下载完成之前使用回退字体显示，从而提升页面的可用性和性能。\n@font-face { font-family: \u0026#39;MyCustomFont\u0026#39;; src: url(\u0026#39;MyFont.woff2\u0026#39;) format(\u0026#39;woff2\u0026#39;); font-display: swap; /* 使用回退字体直到自定义字体加载完毕 */ } 这里，font-display: swap;指示浏览器在自定义字体加载期间使用回退字体，一旦自定义字体加载完毕立即切换。这有助于改善首次内容渲染时间（FCP）和最大内容绘制（LCP）等性能指标。\n也可以使用Web Font Loader，这里就是JavaScript的工作了：\nWeb Font Loader允许添加事件回调和控制字体加载的行为，简单的使用方式如下（需要配合CSS中的@font-face）：\n\u0026lt;script src=\u0026#34;https://ajax.googleapis.com/ajax/libs/webfont/1.6.28/webfont.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; WebFont.load({ custom: { families: [\u0026#39;MyCustomFont\u0026#39;], urls: [\u0026#39;path/to/your/css/file.css\u0026#39;] // CSS文件的路径（可以不特别设置） } }); \u0026lt;/script\u0026gt; ","permalink":"https://ahaknow.com/posts/know/blogfont-display-acceleration/","summary":"博客里使用了自定义的字体，最开始直接采用ttf文件加载的方式，因为博客内容全部是静态存在Github上的，这种直接加载多少有点慢，所以想了一","title":"博客优化：自定义字体加速显示"},{"content":"","permalink":"https://ahaknow.com/posts/know/slam14-ch2/","summary":"","title":"SLAM十四讲第二讲：初识SLAM"},{"content":"第一讲里没有具体需要好好理解的部分，不过自测题目题目值得过一遍。\n1.线性方程Ax=b的求解 有线性方程$Ax=b$，若已知$A,b$，需要求解$x$，该如何求解？这对$A$和$b$有哪些要求？\n（提示：从$A$的维度和秩的角度来分析）\n这个问题很简单，但是值得全面地分析和理解：\n首先假设$A$是一个$m \\times n$矩阵，$x$是一个$n \\times 1$的列向量，$b$是一个$m \\times 1$的列向量。这里的$m$表示方程的数量，$n$表示未知数的数量。\n先从矩阵的秩出发来理解，以列的角度看，矩阵的秩表示了所有列向量线性组合所能达到的空间范围，对于方程$Ax=b$，如果向量$b$在矩阵$A$的列空间中（$b$可以被$A$的列向量线性表示），那么存在至少一个解$x$使得$Ax=b$，因此就有了下面的判断性质（比较矩阵$A$和增广矩阵$A|b$秩的关系）：\n$rank(A) = rank(A|b)$：表明向量$b$在矩阵$A$的列空间中，因此至少存在一个解$x$使得$Ax=b$。 $rank(A) \u0026lt; rank(A|b)$：表明$b$不在$A$的列空间中，因此没有解。 然后再分析$rank(A)$来确定解的情况，是有唯一解还是存在多个解：\n如果$rank(A)=n$（未知数的数量，也就是列的数量），则$A$的所有列向量都是线性独立的，意味着对于给定的$b$，存在唯一的$x$满足$Ax=b$。 如果$rank(A)\u0026lt;n$，则$A$中的某些列向量是其他列向量的线性组合，意味着给定的$b$时，这些可以被其他列向量表示的向量不参与线性组合出$b$，也就可以有任意值作为系数，导致存在无限多个解。 线性方程$Ax=b$的求解方法 ‼️以下的回答来自GPT，自己还不是很理解，需要重新系统学习线性代数的知识后再进行自己的消化吸收！\n超定系统：$m \u0026gt; n$，方程数量多于未知数。 欠定系统：$m \u0026lt; n$，方程数量少于未知数。 适定系统：$m = n$，方程数量等于未知数。 LU分解 原理：将矩阵$A$分解为两个特殊的矩阵乘积，一个下三角矩阵$L$和一个上三角矩阵$U$，即$A=LU$。这样，原始问题$Ax=b$变为求解两个更简单的线性方程组$L(Ux)=b$。 求解过程：首先解$L\\mathbf{y}=b$找到$\\mathbf{y}$，然后解$Ux=\\mathbf{y}$找到$x$。 适用性：特别适用于适定系统，也可用于某些类型的超定和欠定系统，假设$A$可以进行有效的LU分解。 QR分解 原理：将矩阵$A$分解为一个正交矩阵$Q$和一个上三角矩阵$R$，即$A=QR$。正交矩阵具有性质$Q^TQ=I$（$I$是单位矩阵）。 求解过程：利用$Q$的正交性质，原方程$Ax=b$变为$QRx=b$，进一步化简为$Rx=Q^Tb$，然后可以通过回代求解上三角方程组$Rx=Q^Tb$来找到$x$。 适用性：适用于所有类型的系统，尤其是超定系统，因为QR分解提供了一种求解最小二乘问题的自然方法。 奇异值分解（SVD） 原理：SVD将矩阵$A$分解为三个矩阵的乘积，即$A=U\\Sigma V^T$，其中$U$和$V$是正交矩阵，$\\Sigma$是对角矩阵，对角线上的元素是所谓的奇异值。 求解过程：对于方程$Ax=b$，可以转换为$\\Sigma y=U^Tb$（这里$y=V^Tx$），然后通过求解$\\Sigma y=U^Tb$来找到$y$，进一步求得$x=Vy$。 适用性：SVD是一种非常强大的方法，特别是对于奇异矩阵或矩阵秩不满的情况，因为它允许计算伪逆矩阵$A^+$，即使在$A$不可逆的情况下也能找到最小二乘解或最小范数解。 迭代方法求解 原理：迭代方法是从一个初始估计开始，通过重复应用迭代公式来逐步逼近方程的解。常见的迭代方法包括雅可比方法、高斯-赛德尔方法和共轭梯度法等。 求解过程：根据具体的迭代公式，每一步都基于前一步的结果来更新解的估计值，直至满足某个终止条件（如解的变化小于某个阈值）。 适用性：迭代方法特别适用于大规模稀疏矩阵的系统，因为它们通常不需要矩阵分解，而矩阵分解在大规模问题上可能非常昂贵。 2.高斯分布 高斯分布是什么？它的一维形式是什么样子？它的高维形式是什么样子？\n高斯分布，也称为正态分布，高斯分布可以被视为描述自然和人为现象中随机误差的理想模型，其普遍性来源于中心极限定理（Central Limit Theorem, CLT），这个定理说明了许多小效应的累积可以产生高斯分布的现象，具体而言就是：“大量相互独立且分布相同的随机变量之和趋向于服从正态分布，无论原始随机变量的分布如何。”\n高斯分布的一维形式 一维高斯分布（或简称为正态分布）的数学表达式为：\n$$ f(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) $$\n其中，$\\mu$是分布的均值，$\\sigma^2$是方差，$\\sigma$是标准差。这个公式描述了在给定均值$\\mu$和方差$\\sigma^2$的情况下，随机变量$X$取特定值$x$的概率密度。图形上，一维高斯分布是一个对称的钟形曲线，其中心位于均值$\\mu$，曲线的宽度由标准差$\\sigma$决定。\n高斯分布的高维形式 高维高斯分布，或多变量高斯分布，是一维高斯分布在多维空间中的推广。其数学表达式为：\n$$ f(\\mathbf{x} | \\boldsymbol{\\mu}, \\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^k |\\Sigma|}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T \\Sigma^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})\\right) $$\n其中，$\\mathbf{x}$是一个$k$维随机向量，$\\boldsymbol{\\mu}$是均值向量，$\\Sigma$是协方差矩阵，$|\\Sigma|$是协方差矩阵的行列式。高维高斯分布的图形是在多维空间中的一个“山峰”，其中心位于均值向量$\\boldsymbol{\\mu}$，“山峰”的形状和方向由协方差矩阵$\\Sigma$决定。\n协方差是衡量两个随机变量联合变化趋势的度量。如果两个变量的增减趋势相同（即一个变量增加时，另一个也增加），则它们的协方差为正；如果一个变量增加时另一个减少，则协方差为负。\n对于两个随机变量$X$和$Y$，协方差定义为$Cov(X, Y) = E[(X - \\mu_X)(Y - \\mu_Y)]$，其中$\\mu_X$和$\\mu_Y$分别是$X$和$Y$的均值，$E$表示期望值操作。\n","permalink":"https://ahaknow.com/posts/know/slam14-ch1/","summary":"第一讲里没有具体需要好好理解的部分，不过自测题目题目值得过一遍。 1.线性方程Ax=b的求解 有线性方程$Ax=b$，若已知$A,b$，需要求解","title":"SLAM十四讲第一讲：预备知识"},{"content":" 引子：\n我们生活的世界是三维的，在三维世界中对物体的感知拥有明确的距离感，比如键盘在手的前面，显示器在键盘的前面，那么显示器就在手的更前面（这里的“前”或者“后”属于自定义的界定，也可以认为键盘在手的后面，显示器在手的更后面），而如果通过相机将这个三维世界中的场景拍成一张照片，此时三维空间中的所有距离层次就一起被“拍”进了二维图像中，形象地比喻就是将立体的空间“压”成一张了平面的表达，从此表达空间深度的维度就丢失了，并且这个过程在自然状态下是不可逆的。\n而当我们谈到三维空间（Three-dimensional space，以下称3D）的视觉感知时，则必然与深度（距离层次感）脱不开关系，也就是说，当我们用视觉（人可以用眼睛+大脑，机器可以用图像传感器+软件算法）去感知周围的环境时，只有同时获取到了深度，才有资格去讨论3D的视觉感知，否则还是停留在2D平面图像的层次。\n因此在讨论“3D视觉感知的发展”时可以将关注点聚焦在两处：\n如何提高机器视觉系统获取深度信息的精度和可靠性？（如何获得更高精度的深度信息？） 在融合图像和深度信息后，能够推动哪些3D视觉感知技术的发展？（在拥有图像和深度信息后，我们能继续做什么？） 机器视觉的深度获取 首先，我们讨论的是机器的视觉系统，关于机器视觉的定义，简言之，就是通过“某些设备”让机器能够“看到”周围的环境，在不具体追溯这些设备的软硬件实现时，我们以“相机”这个更为广泛的概念进行代替，也就是说，通过相机（相机内部还需要算法和软件支持）可以让机器“看到”这个世界，而相机实现的不同，则可以让机器看待这个世界的方式也发生改变。\n因此，比较直观地获取深度的方式是直接通过相机软硬件这个载体来实现，在这里只进行简要的介绍，因为以下的每一种相机在具体讨论时都需要大篇幅的内容进行结构、原理和方法的说明。\n从相机的角度来看，获取图像深度的方式可以分为两大类型：\n一种是通过硬件的物理测量，比如向三维空间中发射特殊的光源，这个光源接触到物体发生反射后可以回到接收装置，通过测量光源的来回传播时间获得距离信息的飞行时间原理（Time of Flight，TOF）的深度相机，以及向三维空间中投射具有特殊形状的光源，通过测量计算这些光源在物体表面发生的形变来获得距离信息的结构光原理（Structured Light）的深度相机； 另一种则是通过图像中的几何学关系计算得到，专业的表达是多视图几何（Multiple View Geometry），具体的实现可以是一个相机拍摄的多个连续场景图像，或者是两个或多个相机拍摄的同一个场景图像，其中较为成熟的是通过两个平行相机实现的双目立体视觉（Binocular Stereo Vision）相机。 具体到每一种类型的深度相机，都有其各自的关注点来提高相机获取深度的精度，在此暂不深究。从另一个角度出发，如果没有这些额外的软硬件支持，只能通过相机拍摄二维图像，怎么得到图像的深度信息呢？\n在当下能够通过数据驱动解决复杂问题的大背景下，利用机器学习（更具体一点是深度学习）直接从二维图像中估计出深度是一种技术趋势，比如在paperswithcode.com上检索“Depth Estimation”可以看到很多开源的算法和模型，并且所依赖的数据也不再限制，利用单张图像或者多张图像都有相应的方法。因此更直观的想法就是在相机中嵌入深度学习技术直接从图像中估计出深度信息，从而也就避免了采用额外硬件设备所需要的校准、标定等繁杂的维护工作。\n在深度学习里有一条基本认知是：“数据决定了深度学习的上限，而模型只是逼近这个上限而已”。也就是说，想要通过深度学习完善地解决直接的图像深度估计问题，一个好的模型固然重要，但更为关键是拥有驱动这个模型完好运转的数据燃料，并且数据的质量决定了模型效果，那么怎样获得这些高质量的数据呢？\n上文介绍的通过硬件物理测量和通过图像几何学关系获取深度的两种相机，这些在市场中已经有成熟的产品投入应用，如果用它们得到的深度数据来驱动深度学习，从理论上来讲，最理想的状态也不过是达到了深度相机的最高精度效果，并且物理测量或者几何计算本身就具有的偏差还会对模型的效果产生负面影响。\n因此，从数据驱动的深度学习方法估计图像深度的这一方向切入，如果想要让模型的结果更精确从而获取更可靠的深度信息，还需要从数据上下功夫，具体可实践的方式是使用3D仿真，通过模拟相机和构建3D场景产生所需求的且完全准确的深度信息，可以采用的工具和软件包括：\n3D建模和动画软件：Blender； 游戏物理引擎：Unity 3D，Unreal Engine； 机器人仿真器（开源）：Gazebo Sim； 自动驾驶仿真器（开源）：CARLA。 而随着人工智能的发展，特别是生成式人工智能（Generative AI）的技术，将人工智能自己生成内容的技术融入到3D仿真生成也将成为一种新趋势。\n3D视觉感知技术 有了图像深度（用Z表示）之后，我们能做什么呢？\n首先能够从二维图像恢复出三维的空间关系，图像中每一个像素点(x,y)可以通过投影关系恢复到三维空间中的点(X,Y,Z)，将这些三维的点组合起来就构成了点云（Point Cloud）。 通过点云可以分析出哪些是可以移动的区域，哪些区域不平坦；以及哪些是空间中的阻挡自身运动的障碍物，这些障碍物与自身的实时距离等，这个过程体现的就是三维空间的感知。 由于从三维空间“拍”进二维图像的过程中，距离相机更近的物体会挡住其身后的物体，因此在从二维图像恢复到三维空间时，那些被挡住的部分自然也不会呈现出来，而想要从图像中恢复出一个空间的完整样貌，就需要很多张拍摄到这个空间各个角落的图像一起“组合”，共同“拼接”来还原，这个过程就叫做三维空间的场景重建。 以上的过程在具体实现中会涉及到较多的数学原理推导，在这里没有具体展开说明，但从本质的理解出发，在拥有图像深度之后，视觉的感知就完整了，下面从移动机器人和智能驾驶两个领域来谈一谈3D视觉感知技术的具体应用。\n移动机器人领域 一般而言，移动机器人是在一个区域内运行的，也就是说，移动机器人需要拥有这个区域的地图信息，然后在已有地图的基础上再完成感知、定位、路径规划和导航，甚至在实际应用时还需要考虑对地图的更新。机器人的定位和建图可以通过SLAM（Simultaneous Localization And Mapping）技术来实现，SLAM中也需要知道深度信息，比如视觉SLAM在只提供图像的情况下时通过几何学关系来计算出深度，而如果能够同时提供足够精确的深度信息，那么SLAM的建图和定位精度也会更加准确。\n在3D视觉下，机器人可以做到对物体更加准确和丰富的感知，不仅是对物体实现简单的位置测距，在一些算法的支持下，还能够对特定的物体实现姿态的估计，在同时拥有物体的位置和姿态后，就可以展开定位、抓取等后续的操作。同时利用三维空间的场景重建技术，还能够将机器人所处的三维空间模型恢复出来，以这个三维场景为基础，可以继续构建更丰富的地图形式，以及结合3D检测分割等技术实现更具体的环境感知。\n智能驾驶领域 行驶状态中的车辆是实时运动的，并且车辆所在的道路环境也是实时变化，因此即使没有预先加载的高精度地图，智能驾驶的车辆也应该能够通过视觉或者其他传感器感知到周围环境中其他车辆或者物体的状况变化从而调整自身，这是业界常说的“轻地图，重感知”。\n智能驾驶的车辆，不同于移动机器人在一个区域内运行，也不会像移动机器人一样为了补全视野的盲区而在一个范围内来回打转，车辆的运行轨迹基本是持续向前的，因此对于3D视觉的感知更倾向于实时的呈现，通过相机持续拍摄车辆周围可以获得视频流，而同时拥有了图像深度后，这个平面图像的视频流就可以转换为三维空间的点云运动流，这个转换的过程不涉及深度学习。对点云空间也可以同样进行检测、分割等处理，从而获得周围环境的实时道路状况信息。\n近几年兴起的Occupancy Networks（占用网络）是上述描绘场景的一种近似表达，特别是MonoScene，通过一个网络模型实现了从单张图像中获取深度和语义信息再以三维栅格网络方式呈现的流程，从一定意义上掀起了占用网络的热潮。而从本质出发，如果拥有图像中每一个像素对应的精确深度，那么不管是高密度的三维空间点云还是数据量更低的栅格化网格，都可以顺畅地实现。\n3D视觉感知的本质前提是拥有足够精确的第三维度信息，也就是深度，而后再开展以3D视觉为主导的感知技术才能如鱼得水。现如今虽然说人工智能是大趋势，深度学习方法可以解决很多问题，但要驱动深度学习方法完好运转还需要充足且高质量的数据驱动，不管是先获得精确深度信息还是说直接的3D视觉感知，当应用深度学习方法时，都离不开数据。\n总的来说，3D视觉的核心是先恢复出了准确可靠的深度信息，而后再进行更具体的感知任务。一种思路是从相机本身考虑，但可能目前几种深度相机有理论的上限或者实际应用的瓶颈难以继续突破深度测量的精度和可靠性，另一种思路是完全的数据驱动，首先拥有足够量级的精准深度数据，而后不断优化深度学习的模型来达到最佳效果。3D视觉感知技术的应用也是一样，在拥有准确深度后，很多传统的感知任务都会锦上添花，当然也可以将3D视觉感知作为一个整体，在只有图像输入的情况下实现3D视觉的感知，这里就回到了应用深度学习方法解决的思路，模型优化固然重要，更必要还是高质量数据驱动。\n","permalink":"https://ahaknow.com/posts/know/3d-visual-perception/","summary":"引子： 我们生活的世界是三维的，在三维世界中对物体的感知拥有明确的距离感，比如键盘在手的前面，显示器在键盘的前面，那么显示器就在手的更前面（这","title":"3D视觉感知"},{"content":" 问题的描述是这样的（用我理解后的语言来表达）：\n以搭载2D激光雷达（2D-Lidar）的扫地机器人为例，只考虑在二维平面下的关系。世界坐标系$O_W$，机器人坐标系$O_C$，雷达坐标系$O_L$，雷达坐标系的方向轴与机器人坐标系一致，雷达搭载在机器人上，用机器人坐标系$O_C$的坐标表达，雷达的位置为$(xlc, ylc)$，现在空间中有一个物体点P，通过2D雷达对其测量（雷达旋转方向为逆时针），得到对应的角度$angle$和距离$dis$，现在假设扫地机器人运动的角速度是$w_c$，线速度是$v_c$，在$t_0$时刻时，机器人坐标系$O_C$与世界坐标系$O_W$重合，这里的角度单位都是弧度，距离单位都是米。\n现在给这样一组数据，问在$t_1$时刻时，物体点P在世界坐标系$O_W$下的坐标：\nangle、dis、xlc、ylc、 vc、wc、t1\n需要理解的前提 首先，对于输入数据的理解：可以将测试数据作为一个验证情况，因为，当世界坐标系$O_W$，机器人坐标系$O_C$，雷达坐标系$O_L$之间的关系表达明确时，不管带入什么测试数据，都应该是满足的。\n另外，有一个常识需要补充：在C++环境以及处理物体学、数学分析等进行三角函数计算时，使用的都是弧度制， 1弧度 $\\frac{180}{\\pi}$度，1 度 = $\\frac{\\pi}{180}$弧度。\ndouble degrees = 90.0; double radians = degrees * (M_PI / 180.0); double sinValue = sin(radians); // 使用弧度 🌟进行一步一步拆解 t时间后机器人运动的位置 首先需要理解，扫地机器人是怎么运动的，在这里知道机器人运动同时拥有角速度$w_c$，线速度$v_c$。\n一般情况下，移动机器人是通过控制两个并行轮子的速度差（差速）进行旋转的。两个轮子速度相同时，移动机器人将直线运动，两个轮子速度不同时，移动机器人将绕着某一点进行旋转运动，这个点就是即时转动中心（Instantaneous Center of Rotation, 简称 ICC）。\n也就是说，机器人此时的运动模型是一个圆弧运动，运动的半径$R = \\frac{v_c}{w_c}$，运动的角度可以通过$\\theta = w \\cdot t$得到，而对于机器人中心点的位置情况则可以通过极坐标的关系来得到，也就是机器人中心为一点，绕着ICC为原点，进行半径为R的圆周运动，转动了$\\theta = w \\cdot t$角度，那么转动$t$时间后机器人中心点的位置以XY坐标系（笛卡尔坐标系）表达就是：\n$X_{t} = R \\sin(w_c \\cdot t)$ $Y_{t} = R (1 - \\cos(w_c \\cdot t))$ 由此假设以机器人坐标系$O_C$原点来计算，那么在t时间后，$O_C$原点的坐标在世界坐标系$O_W$上的表达为：\n$X_{W_C} = R \\sin(w_c \\cdot t)$ $Y_{W_C} = R (1 - \\cos(w_c \\cdot t))$ 即时转动中心（Instantaneous Center of Rotation） 在任意给定瞬间，一个在平面上进行复合运动（即同时包含平移和旋转）的物体看起来是围绕一个假想点进行旋转，这个点就是即时转动中心。\n这个点可能位于物体内部，也可能位于物体外部（取决于左右轮的速度），甚至可能无限远（这种情况下，物体的运动可以看作是纯平移）。\n假设一个通过两个并行轮子进行差速驱动的移动机器人，左右轮子的速度分别为$V_l$和$V_r$，两轮中心之间距离为$L$，那么可以定义机器人的线速度$v_c$和角速度$w_c$如下：\n线速度$v_c$，表示机器人移动时机器人中心点的速度，通过左右轮速度的平均值来估计： $$ v_c = \\frac{V_r + V_l}{2} $$ 角速度$w_c$描述了机器人绕ICC的旋转速度，可以通过左右轮速度差与轮距的比值来计算（通过$v = \\omega \\cdot r$得到）： $$ w_c = \\frac{V_r - V_l}{L} $$ 机器人绕ICC的圆弧运动半径$R$（从ICC到机器人中心的距离），根据$v = \\omega \\cdot r$可以得到： $$ R = \\frac{v_c}{w_c} $$\n将上述定义的$v_c$和$w_c$代入到$R$的公式中可以得到： $$ R = \\frac{\\frac{V_r + V_l}{2}}{\\frac{V_r - V_l}{L}} $$ $$ R = \\frac{L}{2} \\cdot \\frac{V_r + V_l}{V_r - V_l} $$\n另一种数学解释 对于ICC的计算，在任意瞬间，机器人的左右轮到ICC的距离固不变，形成一个圆周运动。假设右轮比左轮快，那么ICC位于机器人左侧，ICC距离机器人的中心点为$R$。\n由于机器人的左右两个轮子绕ICC的旋转半径不同，但是它们完成一次完整旋转所需的时间相同，因此有：\n左轮的圆周速度是$V_l$，旋转半径是$R - \\frac{L}{2}$。 右轮的圆周速度是$V_r$，旋转半径是$R + \\frac{L}{2}$。 根据圆周运动的速度公式$v = \\omega \\cdot r$，得到：\n$V_l = w_c \\cdot (R - \\frac{L}{2})$ $V_r = w_c \\cdot (R + \\frac{L}{2})$ 将$w_c$带入解方程可以得到：\n$$ R = \\frac{L}{2} \\cdot \\frac{V_r + V_l}{V_r - V_l} $$\n但是更直观的方式是利用角速度和线速度的定义来直接求解$R$，如下：\n$$ w_c = \\frac{V_r - V_l}{L} \\Rightarrow w_c \\cdot L = V_r - V_l $$\n并且\n$$ v_c = \\frac{V_r + V_l}{2} $$\n而$R$可以理解为机器人（中心点）绕ICC旋转的半径，其线速度$v_c$与角速度$w_c$之间存在如下关系：\n$$ R = \\frac{v_c}{w_c} $$\n圆周运动中的位置计算 对于绕ICC的圆周运动，可以用极坐标系下的圆方程来描述物体的位置。在极坐标系中，一个点的位置由它到原点的距离（半径$R$）和一个角度（$\\theta$）来定义。\n将ICC视为原点，则机器人中心就是这个点，其位置转换为笛卡尔坐标系来表示就是：\n$X = R \\sin(\\theta)$ $Y = R - R \\cos(\\theta) = R (1 - \\cos(\\theta))$ 物体点P在机器人坐标系$O_C$的表达 将2D-Lidar看作为一个点，也就是原点$O_L$，那么在雷达坐标系$O_L$下，物体点P可以直接通过三角关系得到在雷达坐标系$O_L$的表达：\n$X_{L_p} = dis \\cdot \\cos(angle)$ $Y_{L_p} = dis \\cdot \\sin(angle)$ 而雷达坐标系$O_L$和机器人坐标系$O_C$只相差了一个平移关系，也就是$(xlc, ylc)$，因此可以将平移的量加上得到在机器人坐标系$O_C$的表达\n$X_{C_p} = dis \\cdot \\cos(angle) + xlc$ $Y_{C_p} = dis \\cdot \\sin(angle) + ylc$ 物体点P在世界坐标系$O_W$的表达 因为机器人坐标系$O_C$和世界坐标系$O_W$相差了一个旋转，机器人坐标系$O_C$从与世界坐标系$O_W$重合的状态，通过逆时针旋转$\\theta = w \\cdot t$得到现在的状态。\n也就是说，以机器人坐标系$O_C$的表达的物体点P也需要通过这个**逆时针旋转$\\theta = w \\cdot t$**得到在当世界坐标系$O_W$的表达；\n因此世界坐标系$O_W$的物体点P为$(X_{W_p},Y_{W_p})$可以通过$(X_{C_p},Y_{C_p})$应用上逆时针旋转，再加上$O_C$的偏移得到。\n$X_{W_p} = X_{C_p} \\cdot \\cos{\\theta} - Y_{C_p} \\cdot \\sin{\\theta} + X_{W_C}$\n$X_{W_p} = X_{C_p} \\cdot \\sin{\\theta} + Y_{C_p} \\cdot \\cos{\\theta} + Y_{W_C}$\n注意，逆时针为正（以右手坐标系来看，X轴朝左，Y轴朝上）\n逆时针旋转角度 $\\theta$ 的旋转矩阵的证明 从二维旋转的基本几何出发。假设我们有一个点 $P$，在原点 $O$ 的坐标系中的初始位置为 $P(x, y)$，需要求出 $P$ 绕原点逆时针旋转角度 $\\theta$ 后的新位置 $P\u0026rsquo;(x\u0026rsquo;, y\u0026rsquo;)$：\n旋转前，点 $P$ 的位置可以由极坐标 $r$（原点到 $P$ 的距离）和 $\\phi$（$x$ 轴到 $OP$ 的角度）确定。在笛卡尔坐标系中，有：\n$x = r\\cos(\\phi)$ $y = r\\sin(\\phi)$ 当 $P$ 绕原点逆时针旋转 $\\theta$ 后，它的新位置 $P\u0026rsquo;$ 可以用新的极坐标 $r$ 和新的角度 $\\phi + \\theta$ 来表示（$r$ 保持不变，因为旋转不改变原点到 $P$ 的距离）。因此，旋转后的坐标为：\n$x\u0026rsquo; = r\\cos(\\phi + \\theta)$ $y\u0026rsquo; = r\\sin(\\phi + \\theta)$ 使用三角恒等式 $\\cos(a + b) = \\cos(a)\\cos(b) - \\sin(a)\\sin(b)$ 和 $\\sin(a + b) = \\sin(a)\\cos(b) + \\cos(a)\\sin(b)$，我们可以将 $x\u0026rsquo;$ 和 $y\u0026rsquo;$ 重写为：\n$x\u0026rsquo; = r\\cos(\\phi)\\cos(\\theta) - r\\sin(\\phi)\\sin(\\theta)$ $y\u0026rsquo; = r\\sin(\\phi)\\cos(\\theta) + r\\cos(\\phi)\\sin(\\theta)$ 将 $x = r\\cos(\\phi)$ 和 $y = r\\sin(\\phi)$，代入得到：\n$x\u0026rsquo; = x\\cos(\\theta) - y\\sin(\\theta)$ $y\u0026rsquo; = x\\sin(\\theta) + y\\cos(\\theta)$ 将上述方程写成矩阵形式，得到：\n$$ \\begin{bmatrix} x\u0026rsquo; \\ y\u0026rsquo; \\end{bmatrix} = \\begin{bmatrix} \\cos(\\theta) \u0026amp; -\\sin(\\theta) \\\\ \\sin(\\theta) \u0026amp; \\cos(\\theta) \\end{bmatrix} \\begin{bmatrix} x \\ y \\end{bmatrix} $$ 这个矩阵就是逆时针旋转角度 $\\theta$ 的旋转矩阵 $R(\\theta)$，能够将任何给定的点在二维平面上逆时针旋转 $\\theta$ 角度，而不改变点到原点的距离。\n如果旋转是顺时针方向的，角度 $\\theta$ 将被视为负值，以右手坐标系来看，X轴朝左，Y轴朝上时，逆时针旋转是正方向，顺时针旋转则是负方向。\n顺时针旋转角度 $\\theta$ 的旋转矩阵可以通过将逆时针旋转矩阵中的角度 $\\theta$ 替换为它的负值 $-\\theta$ 来得到，因为 $\\cos(-\\theta) = \\cos(\\theta)$ 和 $\\sin(-\\theta) = -\\sin(\\theta)$。这意味着，对于顺时针旋转，旋转矩阵 $R_{cw}(\\theta)$ 是：\n$$ R_{cw}(\\theta) = \\begin{bmatrix} \\cos(-\\theta) \u0026amp; -\\sin(-\\theta) \\\\ \\sin(-\\theta) \u0026amp; \\cos(-\\theta) \\end{bmatrix} $$ 使用三角函数的性质 $\\cos(-\\theta) = \\cos(\\theta)$ 和 $\\sin(-\\theta) = -\\sin(\\theta)$，得到：\n$$ R_{cw}(\\theta) = \\begin{bmatrix} \\cos(\\theta) \u0026amp; \\sin(\\theta) \\\\ -\\sin(\\theta) \u0026amp; \\cos(\\theta) \\end{bmatrix} $$ 因此，顺时针旋转矩阵直接使用正角度 $\\theta$ 为：\n$$ R_{cw}(\\theta) = \\begin{bmatrix} \\cos(\\theta) \u0026amp; \\sin(\\theta) \\\\ -\\sin(\\theta) \u0026amp; \\cos(\\theta) \\end{bmatrix} $$ 这个矩阵可以用来将任何给定的点在二维平面上顺时针旋转 $\\theta$ 角度，这与逆时针旋转从形式上看只是$\\sin(\\theta)$的正负号相反。\n完整的C++实现 #include \u0026lt;iostream\u0026gt; #include \u0026lt;cmath\u0026gt; int main() { // jj double angle, dis, xlc, ylc, vc, wc, t1; std::cin \u0026gt;\u0026gt; angle \u0026gt;\u0026gt; dis \u0026gt;\u0026gt; xlc \u0026gt;\u0026gt; ylc \u0026gt;\u0026gt; vc \u0026gt;\u0026gt; wc \u0026gt;\u0026gt; t1; // angle为弧度制，wc为弧度/s // dis为距离，vc为m/s // 起点(0, 0), 角速度wc, 线速度vc // 经过t1时间 double delta_v = vc * t1; double delta_w = wc * t1; // 机器人坐标系Oc，世界坐标Ow double Xwc, Ywc; double R = vc / wc; Xwc = R * sin(delta_w); Ywc = R * (1 - cos(delta_w)); // 打印Xwc, Ywc // std::cout \u0026lt;\u0026lt; Xwc \u0026lt;\u0026lt; \u0026#39; \u0026#39; \u0026lt;\u0026lt; Ywc \u0026lt;\u0026lt; std::endl; // 求P点在世界坐标系Ow的坐标 // 先求P点在Oc的坐标 double Xcp, Ycp; Xcp = xlc + dis * cos(angle); Ycp = ylc + dis * sin(angle); // 打印Xcp, Ycp // std::cout \u0026lt;\u0026lt; Xcp \u0026lt;\u0026lt; \u0026#39; \u0026#39; \u0026lt;\u0026lt; Ycp \u0026lt;\u0026lt; std::endl; // 再求P点在Ow的坐标 double Xwp, Ywp; Xwp = cos(delta_w) * Xcp - sin(delta_w) * Ycp + Xwc; Ywp = sin(delta_w) * Xcp + cos(delta_w) * Ycp + Ywc; // 打印Xwp, Ywp std::cout \u0026lt;\u0026lt; Xwp \u0026lt;\u0026lt; \u0026#39; \u0026#39; \u0026lt;\u0026lt; Ywp \u0026lt;\u0026lt; std::endl; return 0; } ","permalink":"https://ahaknow.com/posts/know/2d-coordinate-transformation/","summary":"问题的描述是这样的（用我理解后的语言来表达）： 以搭载2D激光雷达（2D-Lidar）的扫地机器人为例，只考虑在二维平面下的关系。世界坐标系$","title":"二维平面下的坐标系变换"},{"content":"就是要实现下面这种效果：\n# 就是展示一下效果^-^ 像这种想法怎么样一步步实现的呢？\n对于前端开发来说，最好的学习方式就是阅读源码学习：\n直接看实现过这个效果的的网站是怎么实现的（只要是HTML+CSS方式的，就能读出来，如果全是JS生成的，那就。。。只能另辟蹊径了。）\n","permalink":"https://ahaknow.com/posts/know/blogmac-style-code/","summary":"就是要实现下面这种效果： # 就是展示一下效果^-^ 像这种想法怎么样一步步实现的呢？ 对于前端开发来说，最好的学习方式就是阅读源码学习： 直接看实现","title":"博客自定义：实现Mac风格的代码栏"},{"content":"以下是心路历程：\n为了写博客，因为觉得工具看着不爽，变成了学习Python后端（为啥不是Java？质问脸），开始造引擎？\n纠结于用什么博客框架，Hexo？Hugo？Halo？甚至想学JS框架自己做一个！\n最终还是回归了Hugo，因为markdown写完，直接编译，然后push就OK了；\n但是，又纠结起要不要自己租个服务器！国内服务器要备案（想想当年怎么在腾讯云备案的，又怎么气鼓鼓说再也不干这事了），so考虑香港服务器，一台服务器只是用来放博客嘛？（当然可以有很多用处，个人邮箱、下载器等等等）……\n就是一个静态博客啊，Github不就够了！！\n……\n好吧，老老实实Gitub + Hugo（当然可以Vercel自动部署，暂时用不到，Gitub Page就可以了）\n只是为了写作输出，又觉得直接的Hugo模版不够满意，再次变成了重学HTML、CSS、JS，自己动手改……\n改了一些，比如：Mac窗户风格的代码栏（甚至灵机一动还模拟鼠标放置显示关闭、最小化、最大化的效果）\n但是还是有不满意的：比如：标题下面的显示，太单调；目录能不能放置在两边，等等等等……无底深渊！\n算了，先写一篇正经的博客出来吧，剩下的想法之后慢慢补充！\n写博客到底为了啥？ 我是典型的属于“本末倒置”，不管用啥，哪怕就是文本直接展示，写博客的核心就是写啊！哪里是说：哦，这个博客引擎不行，自己写一个；那个框架看着不错，要不要换一个。重点是写先把文章写出来啊！\n就比如再写这篇文章之前，还在纠结，要不要再完善一下博客的外观……是啊，等你都完善了，菜都凉了，结果就是工具做好了，然后没有工作了，这不是搞笑嘛！放到写博客就是，兴致过去了，说要重新拾起来在写，然后精力全放在怎么把博客弄得好看点，最终精力用光了，博客又没写成，于是一年又一年，没了。\n写博客啊，你得先写再考虑展示啊！\n另外一个就是，不能拖，打死不能拖，脑袋里有想法了就写下来，不然一定会忘，不会再想起来的！血的教训！\n因此，现在我需要做的就是：\n把这两天折腾博客的东西记录下来，总结下来，沉淀下来，虽然少，但积少成多！\n以及，此前的博文写得断断续续的，也需要整理再拿出来，或者直接存着吧，今天开始，好好记录！不要停！\n","permalink":"https://ahaknow.com/posts/think/%E6%8A%98%E8%85%BE%E5%8D%9A%E5%AE%A2%E6%98%AF%E4%B8%BA%E4%BA%86%E6%9B%B4%E5%A5%BD%E5%9C%B0%E4%B8%93%E6%B3%A8%E5%9C%A8%E5%86%99%E4%BD%9C%E4%B8%8A/","summary":"以下是心路历程： 为了写博客，因为觉得工具看着不爽，变成了学习Python后端（为啥不是Java？质问脸），开始造引擎？ 纠结于用什么博客框架，","title":"折腾博客是为了更好地专注在写作上？"}]