[{"content":"ç¬¬ä¸€æ­¥ï¼šå…ˆæµ‹è¯•æ¨¡å‹çš„åŸºå‡†é€Ÿåº¦ ä½¿ç”¨ONNXæµ‹è¯•æ¨¡å‹è¿è¡Œé€Ÿåº¦åŸºå‡† ç›®æ ‡ï¼š ä½¿ç”¨ONNX Runtimeæ¥æµ‹è¯•æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ¨ç†æ—¶é—´åŸºå‡†ã€‚\næ­¥éª¤ï¼š\nå®‰è£…ONNXå’ŒONNX Runtimeï¼š è¿™ä¸¤ä¸ªPythonåº“æ˜¯æµ‹è¯•åŸºå‡†çš„å…³é”®å·¥å…·ã€‚ONNXæ˜¯ä¸€ä¸ªå¼€æ”¾çš„æ¨¡å‹è¡¨ç¤ºæ ‡å‡†ï¼Œå®ƒå…è®¸åœ¨ä¸åŒçš„æœºå™¨å­¦ä¹ æ¡†æ¶ä¹‹é—´ç§»åŠ¨æ¨¡å‹ã€‚ONNX Runtimeåˆ™æ˜¯ä¸€ä¸ªç”¨äºè¿è¡ŒONNXæ¨¡å‹çš„é«˜æ€§èƒ½æ¨ç†å¼•æ“ã€‚å¯ä»¥é€šè¿‡pipè¿›è¡Œå®‰è£…ï¼Œå‘½ä»¤ä¸ºpip install onnx onnxruntime-gpuã€‚è¿™é‡Œçš„onnxruntime-gpuç‰ˆæœ¬æ˜¯æ”¯æŒGPUçš„ONNX Runtimeã€‚\nåŠ è½½æ¨¡å‹ï¼š ä½¿ç”¨ONNX RuntimeåŠ è½½æ¨¡å‹ï¼Œå…·ä½“åšæ³•æ˜¯åˆ›å»ºä¸€ä¸ªonnxruntime.InferenceSessionçš„å®ä¾‹ï¼Œå¹¶æŒ‡å®šæ¨¡å‹çš„æ–‡ä»¶è·¯å¾„å’Œæ¨ç†å¼•æ“æä¾›å•†ï¼ˆè¿™é‡Œé€‰æ‹©çš„æ˜¯'CUDAExecutionProvider'ï¼Œå³ä½¿ç”¨CUDAæ¥åˆ©ç”¨GPUåŠ é€Ÿï¼‰ã€‚\nè·å–è¾“å…¥ä¿¡æ¯å¹¶åˆ›å»ºè¾“å…¥æ•°æ®ï¼š éœ€è¦çŸ¥é“æ¨¡å‹çš„è¾“å…¥èŠ‚ç‚¹çš„åç§°å’Œå½¢çŠ¶ï¼Œè¿™å¯ä»¥é€šè¿‡session.get_inputs()æ¥è·å–ã€‚æœ‰äº†è¿™äº›ä¿¡æ¯åï¼Œå°±å¯ä»¥åˆ›å»ºä¸ä¹‹åŒ¹é…çš„éšæœºè¾“å…¥æ•°æ®ã€‚\næ¨ç†å¹¶è®¡æ—¶ï¼š è¿›è¡Œ100æ¬¡æ¨¡å‹æ¨ç†ï¼Œå¹¶ä½¿ç”¨time.time()æ¥è®¡æ—¶ã€‚æ³¨æ„ï¼Œè¿™é‡Œå¿½ç•¥äº†ç¬¬ä¸€æ¬¡æ¨ç†çš„æ—¶é—´ï¼Œå› ä¸ºç¬¬ä¸€æ¬¡æ¨ç†å¯èƒ½ä¼šåŒ…å«ä¸€äº›åˆå§‹åŒ–æ“ä½œï¼Œå¯¼è‡´è€—æ—¶åå¤§ã€‚\nè®¡ç®—å¹³å‡æ¨ç†æ—¶é—´ï¼š é€šè¿‡å°†æ‰€æœ‰æ¨ç†çš„æ—¶é—´åŠ èµ·æ¥ï¼Œç„¶åé™¤ä»¥æ¨ç†æ¬¡æ•°ï¼Œå°±å¾—åˆ°äº†å¹³å‡æ¨ç†æ—¶é—´ã€‚\né—®é¢˜å’Œè§£å†³ï¼š\nå¦‚ä½•åˆ›å»ºåŒ¹é…æ¨¡å‹è¾“å…¥çš„æ•°æ®ï¼š æ¨¡å‹çš„è¾“å…¥å¯èƒ½æœ‰å¤šä¸ªï¼Œä¹Ÿå¯èƒ½æœ‰å„ç§å„æ ·çš„å½¢çŠ¶å’Œç±»å‹ã€‚éœ€è¦é€šè¿‡session.get_inputs()è·å–è¿™äº›ä¿¡æ¯ï¼Œç„¶åæ ¹æ®è¿™äº›ä¿¡æ¯åˆ›å»ºç›¸åº”çš„è¾“å…¥æ•°æ®ã€‚\nå¦‚ä½•ç¡®ä¿ä½¿ç”¨äº†GPUï¼š éœ€è¦åœ¨åˆ›å»ºonnxruntime.InferenceSessionæ—¶æ˜ç¡®æŒ‡å®šä½¿ç”¨'CUDAExecutionProvider'ã€‚å¦‚æœæ²¡æœ‰æ­£ç¡®åœ°ä½¿ç”¨GPUï¼Œå¯èƒ½ä¼šå¯¼è‡´æ¨ç†é€Ÿåº¦å¤§å¹…åº¦é™ä½ã€‚\nå¦‚ä½•å‡†ç¡®è®¡æ—¶ï¼š é€‰æ‹©å¿½ç•¥äº†ç¬¬ä¸€æ¬¡æ¨ç†çš„æ—¶é—´ï¼Œå› ä¸ºç¬¬ä¸€æ¬¡æ¨ç†å¯èƒ½ä¼šåŒ…å«ä¸€äº›åˆå§‹åŒ–æ“ä½œï¼Œå¯¼è‡´è€—æ—¶åå¤§ã€‚\nPythonè„šæœ¬ä»£ç  import numpy as np import onnxruntime import time # åŠ è½½æ¨¡å‹ model_path = \"/home/metoak/Projects/ros_project/src/metoak_camera_driver/models/model_640_384_6125.onnx\" # session = onnxruntime.InferenceSession(model_path) session = onnxruntime.InferenceSession(model_path, providers=['CUDAExecutionProvider']) # è·å–è¾“å…¥åç§°å’Œå½¢çŠ¶ input_names = [inp.name for inp in session.get_inputs()] input_shapes = [inp.shape for inp in session.get_inputs()] print(\"Input names:\", input_names) print(\"Input shapes:\", input_shapes) # åˆ›å»ºä¸¤ä¸ªéšæœºè¾“å…¥ input1 = np.random.randint(0, 256, size=input_shapes[0]).astype(np.uint8) input2 = np.random.randint(0, 256, size=input_shapes[1]).astype(np.uint8) # å‡†å¤‡è¾“å…¥æ•°æ® inputs = {input_names[0]: input1, input_names[1]: input2} # æ¨ç†æ¬¡æ•° num_inferences = 100 times = [] # æ‰§è¡Œæ¨æ–­å¹¶è®¡æ—¶ for i in range(num_inferences): start = time.time() output = session.run(None, inputs) end = time.time() inference_time = end - start if i == 0: continue times.append(inference_time) print(f\"Inference {i+1}, Time: {inference_time * 1000} ms\") # è®¡ç®—å¹³å‡æ¨æ–­æ—¶é—´ avg_time = sum(times) / (num_inferences - 1) # æ‰“å°å¹³å‡æ¨æ–­æ—¶é—´ print(f\"Average inference time: {avg_time * 1000} ms\") æ¨¡å‹é€Ÿåº¦åŸºå‡† Model name: model_640_384_6125.onnx Input names: [\u0026#39;onnx::Cast_0\u0026#39;, \u0026#39;onnx::Cast_1\u0026#39;] Input shapes: [[1, 3, 384, 640], [1, 3, 384, 640]] Inference 2, Time: 299.41678047180176 ms Inference 3, Time: 306.2708377838135 ms ... Inference 98, Time: 332.33070373535156 ms Inference 99, Time: 317.1732425689697 ms Inference 100, Time: 333.3919048309326 ms Average inference time: 322.4095816564078 ms ç¬¬äºŒæ­¥ï¼šä½¿ç”¨TensorRTå®ç°åŠ é€Ÿ å¦‚æœä½¿ç”¨çš„3080Tiæ˜¾å¡\næµ®ç‚¹è®¡ç®—èƒ½åŠ›ï¼š\nSingle Precision Perf.: 34.1 TFLOPS\nTensor Perf. (FP16): 136 TFLOPS\nTensor Perf. (FP16-Sparse): ==273 TFLOPS==\nå¦‚æœä½¿ç”¨Tensoræ¥å¤„ç†å…¨éƒ¨ç²¾åº¦ä¸º16ä½æµ®ç‚¹æ•°çš„ç®—æ³•ï¼Œé‚£ä¹ˆç†è®ºçš„è¿ç®—é€Ÿç‡å¯ä»¥è¾¾åˆ°è‡³å°‘æ˜¯å•ç²¾åº¦æµ®ç‚¹è¿ç®—çš„ 136 / 34.1 = 3.99ï¼Œä¹Ÿå°±æ˜¯4å€é€Ÿåº¦çš„æå‡ï¼Œè€Œå¦‚æœæ¨¡å‹æœ¬èº«å°±æ˜¯æ··åˆè¿›åº¦è¿›è¡Œè¿ç®—çš„ï¼Œé‚£ä¹ˆæå‡çš„æ•ˆæœä¼šæ›´å¤§ï¼ï¼ˆå¦‚æœå†è€ƒè™‘ä¸Šç¨€ç–ä¼˜åŒ–ï¼Œé‚£ä¹ˆTensoråŠ é€Ÿçš„å€æ•°ä¼šæ›´é«˜ï¼Œè¾¾åˆ° 273 / 34.1 = 8å€çš„æå‡ï¼‰\nğŸŒŸTensorRTç¯å¢ƒéƒ¨ç½² å®‰è£…æ–¹æ³•å¯¹äº†ï¼Œå®‰è£…å…¶å®å¾ˆç®€å•ï¼Œä¸€å¥è¯è¯´æ˜å°±æ˜¯ï¼š\n==cudaã€tensorrtéƒ½ä½¿ç”¨depåŒ…ï¼Œç„¶åä½¿ç”¨aptå®‰è£…==\nä½†æ˜¯ç»†èŠ‚å¾ˆå…³é”®ï¼ä¸‹é¢çš„è¿‡ç¨‹æ¶‰åŠå®‰è£…CUDAã€cuDNNã€TensorRT\nå®˜æ–¹æ–‡æ¡£\n1. å®‰è£…CUDA CUDAå’ŒNvida Driverè¿™ä¿©ä¸ªæ˜¯ç‹¬ç«‹ä¸å½±å“çš„ï¼Œæ‰€ä»¥å•ç‹¬å®‰è£…CUDAç‰ˆæœ¬å³å¯ï¼Œä¸Nvida Driverç‰ˆæœ¬æ— å…³ï¼Œæ¯”å¦‚å®‰è£…==cuda 11.8==\nUbuntu20.04:\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda-repo-ubuntu2004-11-8-local_11.8.0-520.61.05-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu2004-11-8-local_11.8.0-520.61.05-1_amd64.deb sudo cp /var/cuda-repo-ubuntu2004-11-8-local/cuda-*-keyring.gpg /usr/share/keyrings/ sudo apt-get update sudo apt-get -y install cuda å¦‚æœä½¿ç”¨äº†runfileçš„æ–¹å¼å®‰è£…è¿‡cudaäº†ï¼Œé‚£ä¹ˆæœ€åä¸€æ­¥åªéœ€è¦ï¼š\nsudo apt install cuda-libraries-11-8 2. å®‰è£…cuDNN éœ€è¦æ³¨å†ŒNvidiaå¼€å‘è€…è´¦å·ï¼Œä¸‹è½½å¯¹åº”çš„CUDAç‰ˆæœ¬çš„å³å¯ï¼Œä½¿ç”¨å¦‚ä¸‹æ–¹å¼å®‰è£…ï¼š\nå®˜ç½‘\nä¸‹è½½debç‰ˆæœ¬å³å¯\nsudo dpkg -i cudnn-local-repo-ubuntu2004-8.9.2.26_1.0-1_amd64.deb sudo cp /var/cudnn-local-repo-ubuntu2004-8.9.2.26/cudnn-local-6D0A7AE1-keyring.gpg /usr/share/keyrings/ sudo apt update sudo apt-get install libcudnn8 3. å®‰è£…TensorRT å®˜ç½‘\nä¸‹è½½æœ€æ–°ç‰ˆæœ¬TensorRT 8ï¼ŒåŒæ ·éœ€è¦å¼€å‘è€…è´¦å·ï¼›\nä¸‹è½½å¯¹åº”ç³»ç»Ÿçš„å¯¹åº”CUDAçš„æœ€æ–°ç‰ˆå³å¯\nsudo dpkg -i nv-tensorrt-local-repo-ubuntu2004-8.6.1-cuda-11.8_1.0-1_amd64.deb sudo cp /var/nv-tensorrt-local-repo-ubuntu2004-8.6.1-cuda-11.8_1.0-1/*-keyring.gpg /usr/share/keyrings/ sudo apt update sudo apt install tensorrt ä½¿ç”¨trtexecå·¥å…·å®ç°æ¨¡å‹è½¬æ¢ # ä½¿ç”¨ä¾‹å­ trtexec --onnx=/home/metoak/Projects/ros_project/src/metoak_camera_driver/models/model_sim_1280_704.onnx --saveEngine=model_1280_704_fp16.engine --fp16 --avgRuns=100 å¯ä»¥ä½¿ç”¨trtexecè¿™ä¸ªå·¥å…·æ¥ç›´æ¥è½¬æ¢onnxæ¨¡å‹ï¼Œä½†å®é™…å¯èƒ½ä¸ç†æƒ³æˆ–è€…é‡åˆ°ä¸€äº›é—®é¢˜ï¼š\nonnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32. [07/25/2023-10:29:30] [W] [TRT] onnx2trt_utils.cpp:400: One or more weights outside the range of INT32 was clamped [07/25/2023-10:29:30] [I] Finished parsing network model. Parse time: 1.3651 [07/25/2023-10:29:30] [I] [TRT] Graph optimization time: 0.282094 seconds. [07/25/2023-10:29:30] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2630, GPU 3661 (MiB) [07/25/2023-10:29:30] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 2630, GPU 3671 (MiB) [07/25/2023-10:29:30] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\nYour ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.: è¿™ä¸ªè­¦å‘Šè¡¨æ˜ä½ çš„ONNXæ¨¡å‹ä¸­åŒ…å«äº†INT64ç±»å‹çš„æƒé‡ï¼Œè€ŒTensorRTåŸç”Ÿä¸æ”¯æŒINT64ç±»å‹ã€‚TensorRTæ­£åœ¨å°è¯•å°†è¿™äº›æƒé‡è½¬æ¢ï¼ˆcastï¼‰ä¸ºINT32ç±»å‹ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´ä¸€äº›ç²¾åº¦æŸå¤±ï¼Œå› ä¸ºINT32çš„èŒƒå›´å°äºINT64ã€‚\nOne or more weights outside the range of INT32 was clamped: è¿™ä¸ªè­¦å‘Šè¡¨æ˜åœ¨è½¬æ¢æƒé‡ç±»å‹çš„è¿‡ç¨‹ä¸­ï¼Œæœ‰ä¸€ä¸ªæˆ–å¤šä¸ªæƒé‡çš„å€¼è¶…å‡ºäº†INT32ç±»å‹çš„èŒƒå›´ï¼Œå¹¶è¢«å‹ç¼©ï¼ˆclampï¼‰åˆ°äº†INT32çš„èŒƒå›´å†…ã€‚è¿™ä¹Ÿå¯èƒ½ä¼šå¯¼è‡´ä¸€äº›ç²¾åº¦æŸå¤±ã€‚\nFinished parsing network model. Parse time: 1.3651: è¿™è¡¨ç¤ºTensorRTå·²ç»å®Œæˆäº†å¯¹ONNXæ¨¡å‹çš„è§£æï¼Œæ•´ä¸ªè§£æè¿‡ç¨‹èŠ±è´¹äº†1.3651ç§’ã€‚\nGraph optimization time: 0.282094 seconds: è¿™è¡¨ç¤ºTensorRTå¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–çš„æ—¶é—´æ˜¯0.282094ç§’ã€‚\nInit cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2630, GPU 3661 (MiB) å’Œ Init cuDNN: CPU +0, GPU +10, now: CPU 2630, GPU 3671 (MiB): è¿™äº›è¡Œæ˜¯å…³äºTensorRTåˆå§‹åŒ–cuBLASå’ŒcuDNNåº“ï¼ˆè¿™äº›åº“ç”¨äºGPUåŠ é€Ÿçš„çº¿æ€§ä»£æ•°è¿ç®—å’Œæ·±åº¦ç¥ç»ç½‘ç»œè®¡ç®—ï¼‰æ‰€éœ€å†…å­˜çš„ä¿¡æ¯ã€‚è¿™äº›è¡Œæ˜¾ç¤ºäº†åœ¨åˆå§‹åŒ–è¿™äº›åº“åï¼ŒCPUå’ŒGPUå†…å­˜çš„ä½¿ç”¨æƒ…å†µã€‚\nLocal timing cache in use. Profiling results in this builder pass will not be stored.: è¿™è¡¨ç¤ºTensorRTæ­£åœ¨ä½¿ç”¨æœ¬åœ°çš„è®¡æ—¶ç¼“å­˜è¿›è¡Œæ“ä½œï¼Œè€Œè¿™æ¬¡çš„åˆ†æç»“æœä¸ä¼šè¢«å­˜å‚¨ã€‚\nä½¿ç”¨trtexecæ¥è½¬æ¢æ¨¡å‹ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ï¼Œå¾—åˆ°ä¸‹é¢çš„è¾“å‡ºæ—¶ï¼Œä¼šæˆåŠŸï¼Œä½†ä¹Ÿæœ‰å¯èƒ½å¾—åˆ°æ¨¡å‹æ— æ³•æ­£å¸¸ä½¿ç”¨ï¼š\n[07/25/2023-14:44:13] [I] Starting inference [07/25/2023-14:44:16] [I] Warmup completed 12 queries over 200 ms [07/25/2023-14:44:16] [I] Timing trace has 180 queries over 3.05014 s [07/25/2023-14:44:16] [I] [07/25/2023-14:44:16] [I] === Trace details === [07/25/2023-14:44:16] [I] Trace averages of 100 runs: [07/25/2023-14:44:16] [I] Average on 100 runs - GPU latency: 16.9045 ms - Host latency: 17.8527 ms (enqueue 11.5822 ms) [07/25/2023-14:44:16] [I] [07/25/2023-14:44:16] [I] === Performance summary === [07/25/2023-14:44:16] [I] Throughput: 59.0137 qps [07/25/2023-14:44:16] [I] Latency: min = 14.8851 ms, max = 27.6843 ms, mean = 17.7984 ms, median = 16.9878 ms, percentile(90%) = 21.1488 ms, percentile(95%) = 23.4043 ms, percentile(99%) = 27.5272 ms [07/25/2023-14:44:16] [I] Enqueue Time: min = 3.59753 ms, max = 22.9529 ms, mean = 11.5029 ms, median = 10.7087 ms, percentile(90%) = 13.9353 ms, percentile(95%) = 16.418 ms, percentile(99%) = 20.6213 ms [07/25/2023-14:44:16] [I] H2D Latency: min = 0.0994873 ms, max = 0.790527 ms, mean = 0.303106 ms, median = 0.268311 ms, percentile(90%) = 0.458588 ms, percentile(95%) = 0.503906 ms, percentile(99%) = 0.564697 ms [07/25/2023-14:44:16] [I] GPU Compute Time: min = 14.0032 ms, max = 26.499 ms, mean = 16.8485 ms, median = 16.0006 ms, percentile(90%) = 19.6926 ms, percentile(95%) = 22.436 ms, percentile(99%) = 26.4436 ms [07/25/2023-14:44:16] [I] D2H Latency: min = 0.319824 ms, max = 1.09839 ms, mean = 0.646768 ms, median = 0.621216 ms, percentile(90%) = 0.887512 ms, percentile(95%) = 0.963013 ms, percentile(99%) = 1.05774 ms [07/25/2023-14:44:16] [I] Total Host Walltime: 3.05014 s [07/25/2023-14:44:16] [I] Total GPU Compute Time: 3.03273 s [07/25/2023-14:44:16] [W] * GPU compute time is unstable, with coefficient of variance = 15.1765%. [07/25/2023-14:44:16] [W] If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability. [07/25/2023-14:44:16] [I] Explanations of the performance metrics are printed in the verbose logs. [07/25/2023-14:44:16] [I] \u0026amp;\u0026amp;\u0026amp;\u0026amp; PASSED TensorRT.trtexec [TensorRT v8601] # /usr/src/tensorrt/bin/trtexec --onnx=/home/metoak/Projects/ros_project/src/metoak_camera_driver/models/model_640_384_6125.onnx --saveEngine=model_640_384.engine --fp16 --avgRuns=100 è¿™æ˜¯é€šè¿‡ä½¿ç”¨TensorRTå·¥å…·trtexecçš„ä¸€æ®µæµ‹è¯•è¾“å‡ºï¼Œä¸‹é¢æ˜¯æ¯ä¸ªé‡è¦éƒ¨åˆ†çš„è§£é‡Šï¼š\n[I] Warmup completed 12 queries over 200 msï¼šåœ¨å¼€å§‹æ­£å¼è®¡æ—¶ä¹‹å‰ï¼Œtrtexecé¦–å…ˆæ‰§è¡Œäº†12ä¸ªé¢„çƒ­æŸ¥è¯¢ï¼Œæ€»è€—æ—¶200æ¯«ç§’ã€‚\n[I] Timing trace has 180 queries over 3.05014 sï¼šè¿™è¡¨ç¤ºè¿è¡Œäº†180æ¬¡æ¨æ–­ï¼Œæ€»è€—æ—¶çº¦ä¸º3.05ç§’ã€‚\n[I] Average on 100 runs - GPU latency: 16.9045 ms - Host latency: 17.8527 msï¼šè¿™è¡¨ç¤ºåœ¨100æ¬¡è¿è¡Œä¸­ï¼Œå¹³å‡æ¯æ¬¡æ¨æ–­åœ¨GPUä¸Šçš„å»¶è¿Ÿæ˜¯16.9045æ¯«ç§’ï¼Œåœ¨ä¸»æœºä¸Šçš„å»¶è¿Ÿæ˜¯17.8527æ¯«ç§’ã€‚\n[I] Throughput: 59.0137 qpsï¼šè¿™è¡¨ç¤ºæ¨¡å‹çš„ååé‡ä¸º59.0137æŸ¥è¯¢æ¯ç§’ï¼ˆqpsï¼‰ã€‚\n[I] Latency: min = 14.8851 ms, max = 27.6843 ms, mean = 17.7984 ms, median = 16.9878 msï¼šè¿™æ˜¯å¯¹å»¶è¿Ÿçš„ä¸€äº›ç»Ÿè®¡æè¿°ï¼ŒåŒ…æ‹¬æœ€å°å€¼ã€æœ€å¤§å€¼ã€å¹³å‡å€¼å’Œä¸­ä½æ•°ã€‚\n[W] * GPU compute time is unstable, with coefficient of variance = 15.1765%.ï¼šè¿™æ˜¯ä¸€ä¸ªè­¦å‘Šï¼Œè¡¨ç¤ºGPUè®¡ç®—æ—¶é—´çš„å˜åŒ–æ˜¯ä¸ç¨³å®šçš„ï¼Œæ–¹å·®ç³»æ•°ä¸º15.1765%ï¼Œå¯èƒ½éœ€è¦é€šè¿‡é”å®šGPUæ—¶é’Ÿé¢‘ç‡æˆ–ä½¿ç”¨--useSpinWaité€‰é¡¹æ¥æ”¹å–„è¿™ç§æƒ…å†µã€‚\n\u0026amp;\u0026amp;\u0026amp;\u0026amp; PASSED TensorRT.trtexec [TensorRT v8601] # /usr/src/tensorrt/bin/trtexec --onnx=/home/metoak/Projects/ros_project/src/metoak_camera_driver/models/model_640_384_6125.onnx --saveEngine=model_640_384.engine --fp16 --avgRuns=100ï¼šè¿™è¡¨ç¤ºtrtexecæµ‹è¯•é€šè¿‡äº†ï¼Œå¹¶ä¸”æ˜¾ç¤ºäº†ä½ ç”¨æ¥è¿è¡Œtrtexecçš„å‘½ä»¤è¡Œå‚æ•°ã€‚\néªŒè¯engineæ¨¡å‹æ˜¯å¦å¯ç”¨ï¼Ÿ import tensorrt as trt TRT_LOGGER = trt.Logger(trt.Logger.WARNING) trt_runtime = trt.Runtime(TRT_LOGGER) engine_path = \u0026#34;/home/metoak/Projects/models/model_resnet101.engine\u0026#34; with open(engine_path, \u0026#34;rb\u0026#34;) as f: engine_data = f.read() engine = trt_runtime.deserialize_cuda_engine(engine_data) print(\u0026#34;Number of bindings: \u0026#34;, engine.num_bindings) for i in range(engine.num_bindings): binding_name = engine.get_tensor_name(i) print(\u0026#34;Binding \u0026#34;, i, \u0026#34;:\u0026#34;, \u0026#34;Binding name: \u0026#34;, binding_name, \u0026#34;Binding shape: \u0026#34;, engine.get_tensor_shape(binding_name), \u0026#34;Binding data type: \u0026#34;, engine.get_tensor_dtype(binding_name), \u0026#34;Is binding an input? \u0026#34;, engine.get_tensor_mode(binding_name) == trt.TensorIOMode.INPUT) print(\u0026#34;Number of layers: \u0026#34;, engine.num_layers) å¦‚æœæ‰€æœ‰çš„æ‰“å°è¾“å‡ºéƒ½æ­£å¸¸ï¼Œé‚£ä¹ˆè¯´æ˜è½¬æ¢çš„æ¨¡å‹å¯ä»¥ï¼Œå¹¶ä¸”ä»¥æ­£å¸¸æ–¹æ³•è¿›è¡Œæ¨ç†ä¹Ÿæ˜¯å¯è¡Œçš„\nä¹‹åå°±æ˜¯å°†æ¨¡å‹æ¨ç†éƒ¨åˆ†æ•´åˆè¿›å…¥é¡¹ç›®ä»£ç å³å¯ã€‚\nç¬¬ä¸‰æ­¥ï¼šä¸€èˆ¬çš„æ–¹æ³•ä¸è¡Œæ€ä¹ˆåŠï¼Ÿï¼ˆâ€¼ï¸éå¸¸å…³é”®ï¼‰ ä½¿ç”¨trtexecæä¾›çš„è½¬æ¢æ¨¡å‹æ–¹æ³•ï¼Œæ— æ³•ç›´æ¥ä½¿ç”¨\nåŸå› åˆ†æï¼šæ¨¡å‹æ¯”è¾ƒå¤æ‚ï¼ŒTensorRTæ— æ³•æ”¯æŒ â€”â€” è¿™ä¸ªé—®é¢˜åœ¨å®‰è£…TensorRTæœ€æ–°ç‰ˆä¹‹åè§£å†³ï¼Œä½†TensorRTå¢åŠ äº†æ–°ç‰¹å¾ï¼ˆä½¿ç”¨ç¬¬ä¸‰æ–¹æ’ä»¶ï¼‰ï¼Œæ— æ³•ç›´æ¥ä½¿ç”¨trtexecè½¬æ¢åçš„æ¨¡å‹ï¼š\nIn TensorRT 8.6 there are two implementations of InstanceNormalization that may perform differently depending on various parameters. By default the parser will insert an InstanceNormalization plugin layer as it performs best for general use cases. Users that want to benchmark using the native TensorRT implementation of InstanceNorm can set the parser flag kNATIVE_INSTANCENORM prior to parsing the model. For building version compatible or hardware compatible engines, this flag must be set.\nä½¿ç”¨pythonè„šæœ¬æ¥å¤„ç†ï¼ŒdebugåŠŸèƒ½è¾ƒå¼±ï¼Œå°è¯•ä½¿ç”¨C++çš„ä»£ç æ¥é‡å†™æ•´ä¸ªå·¥å…·é“¾\nä½¿ç”¨æ–°ç‰ˆæœ¬ç‰¹æ€§æ¥è½¬æ¢Onnx ç›´æ¥ä¸Šä»£ç ï¼š\n// Usage: ./onnx_to_tensorrt onnx_path #include \u0026lt;NvInfer.h\u0026gt; #include \u0026lt;NvOnnxParser.h\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;iostream\u0026gt; // ç»§æ‰¿è‡ª nvinfer1::ILogger ä»¥ä¾¿æˆ‘ä»¬å¯ä»¥ä¿®æ”¹æ—¥å¿—å¤„ç†æ–¹å¼ class Logger : public nvinfer1::ILogger { public: void log(Severity severity, const char *msg) noexcept override { // è°ƒè¯•å®Œæ•´ä¿¡æ¯ if (severity != Severity::kINFO) std::cout \u0026lt;\u0026lt; msg \u0026lt;\u0026lt; std::endl; } } gLogger; #include \u0026lt;fstream\u0026gt; int main(int argc, char *argv[]) { if (argc \u0026lt; 2) { std::cerr \u0026lt;\u0026lt; \u0026#34;Please specify path to the ONNX model.\\n\u0026#34;; return 1; } auto builder = nvinfer1::createInferBuilder(gLogger); const auto explicitBatch = 1U \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(nvinfer1::NetworkDefinitionCreationFlag::kEXPLICIT_BATCH); auto network = builder-\u0026gt;createNetworkV2(explicitBatch); auto config = builder-\u0026gt;createBuilderConfig(); auto parser = nvonnxparser::createParser(*network, gLogger); // æ³¨æ„è¿™é‡Œï¼šä½¿ç”¨TensorRTè‡ªå¸¦çš„InstanceNormalizationå®ç° auto flag = 1U \u0026lt;\u0026lt; static_cast\u0026lt;uint32_t\u0026gt;(nvonnxparser::OnnxParserFlag::kNATIVE_INSTANCENORM); parser-\u0026gt;setFlags(flag); std::string model_path = argv[1]; if (!parser-\u0026gt;parseFromFile(model_path.c_str(), 1)) { std::cerr \u0026lt;\u0026lt; \u0026#34;Failed to parse the ONNX model.\\n\u0026#34;; return 1; } auto engine = builder-\u0026gt;buildEngineWithConfig(*network, *config); if (!engine) { std::cerr \u0026lt;\u0026lt; \u0026#34;Failed to create the TensorRT engine.\\n\u0026#34;; return 1; } std::string engine_path = model_path.substr(0, model_path.find_last_of(\u0026#39;.\u0026#39;)) + \u0026#34;.trt\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34;Serializing the TensorRT engine to \u0026#34; \u0026lt;\u0026lt; engine_path \u0026lt;\u0026lt; \u0026#34;...\\n\u0026#34;; nvinfer1::IHostMemory *serializedModel = engine-\u0026gt;serialize(); std::ofstream engineFile(engine_path, std::ios::binary); engineFile.write(reinterpret_cast\u0026lt;const char *\u0026gt;(serializedModel-\u0026gt;data()), serializedModel-\u0026gt;size()); engineFile.close(); serializedModel-\u0026gt;destroy(); parser-\u0026gt;destroy(); engine-\u0026gt;destroy(); config-\u0026gt;destroy(); network-\u0026gt;destroy(); builder-\u0026gt;destroy(); return 0; } ç›´æ¥ä½¿ç”¨æ­¤ä»£ç è¿›è¡Œæ¨¡å‹è½¬æ¢;\néªŒè¯æ¨¡å‹å¯è¡Œæ€§ \u0026amp;\u0026amp; æ¨¡å‹æ¨ç†è¿‡ç¨‹ è¿™é‡Œç›´æ¥å†™ä¸€ä¸ªdemoï¼Œæ¥å®ç°æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œä»è€ŒéªŒè¯æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œè¿™éƒ¨åˆ†ç›´æ¥ä¸Šä»£ç ï¼š\n// tensorrt_image.cpp #include \u0026lt;NvInfer.h\u0026gt; #include \u0026lt;opencv2/opencv.hpp\u0026gt; #include \u0026lt;chrono\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;fstream\u0026gt; #define TEST_TIME 0 // ç»§æ‰¿è‡ª nvinfer1::ILogger ä»¥ä¾¿æˆ‘ä»¬å¯ä»¥ä¿®æ”¹æ—¥å¿—å¤„ç†æ–¹å¼ class Logger : public nvinfer1::ILogger { public: void log(Severity severity, const char *msg) noexcept override { // Severity::kVERBOSE æˆ– kINFO // åªæ‰“å°é”™è¯¯ä¿¡æ¯ if (severity == Severity::kERROR \u0026amp;\u0026amp; severity == Severity::kINTERNAL_ERROR) { std::cout \u0026lt;\u0026lt; msg \u0026lt;\u0026lt; std::endl; } } } gLogger; int main(int argc, char *argv[]) { if (argc \u0026lt; 2) { std::cerr \u0026lt;\u0026lt; \u0026#34;Please specify path to the engine file.\\n\u0026#34;; return 1; } // æ‰“å¼€å¹¶ååºåˆ—åŒ– TensorRT engine std::ifstream engineFile(argv[1], std::ios::binary); if (!engineFile) { std::cerr \u0026lt;\u0026lt; \u0026#34;Error opening engine file\\n\u0026#34;; return -1; } engineFile.seekg(0, engineFile.end); long int fsize = engineFile.tellg(); engineFile.seekg(0, engineFile.beg); std::vector\u0026lt;char\u0026gt; engineData(fsize); engineFile.read(engineData.data(), fsize); if (!engineFile) { std::cerr \u0026lt;\u0026lt; \u0026#34;Error reading engine file\\n\u0026#34;; return -1; } nvinfer1::IRuntime *runtime = nvinfer1::createInferRuntime(gLogger); nvinfer1::ICudaEngine *engine = runtime-\u0026gt;deserializeCudaEngine(engineData.data(), fsize, nullptr); // åˆ›å»ºæ¨ç†ä¸Šä¸‹æ–‡ nvinfer1::IExecutionContext *context = engine-\u0026gt;createExecutionContext(); // è¯»å–å¹¶é¢„å¤„ç†å›¾åƒ cv::Mat img1 = cv::imread(\u0026#34;/home/metoak/Projects/dependency/InferenceDemo/images/left.png\u0026#34;, cv::IMREAD_COLOR); cv::Mat img2 = cv::imread(\u0026#34;/home/metoak/Projects/dependency/InferenceDemo/images/right.png\u0026#34;, cv::IMREAD_COLOR); cv::resize(img1, img1, cv::Size(640, 384)); cv::resize(img2, img2, cv::Size(640, 384)); img1.convertTo(img1, CV_32FC3, 1 / 255.0); img2.convertTo(img2, CV_32FC3, 1 / 255.0); // åˆ›å»ºè¾“å…¥æ•°æ® int num_elements_input = 1 * 3 * 384 * 640; // æ ¹æ®ä½ çš„æ¨¡å‹çš„å®é™…éœ€æ±‚è¿›è¡Œè°ƒæ•´ // åˆ†å‰²é€šé“ std::vector\u0026lt;cv::Mat\u0026gt; channels1(3); std::vector\u0026lt;cv::Mat\u0026gt; channels2(3); cv::split(img1, channels1); cv::split(img2, channels2); std::vector\u0026lt;float\u0026gt; data1; std::vector\u0026lt;float\u0026gt; data2; // å°†æ¯ä¸ªé€šé“çš„æ•°æ®æ·»åŠ åˆ°ç›¸åº”çš„å‘é‡ for (auto \u0026amp;channel : channels1) { std::vector\u0026lt;float\u0026gt; channelData(channel.begin\u0026lt;float\u0026gt;(), channel.end\u0026lt;float\u0026gt;()); data1.insert(data1.end(), channelData.begin(), channelData.end()); } for (auto \u0026amp;channel : channels2) { std::vector\u0026lt;float\u0026gt; channelData(channel.begin\u0026lt;float\u0026gt;(), channel.end\u0026lt;float\u0026gt;()); data2.insert(data2.end(), channelData.begin(), channelData.end()); } #if TEST_TIME == 1 // æ‰“å°å›¾åƒçš„å‰10ä¸ªåƒç´ å€¼ std::cout \u0026lt;\u0026lt; \u0026#34;First 10 pixels of the input image:\\n\u0026#34;; for (int i = 0; i \u0026lt; 10; ++i) { std::cout \u0026lt;\u0026lt; data1[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } #endif // è½¬æ¢å›å›¾åƒ std::vector\u0026lt;cv::Mat\u0026gt; channels(3); for (int c = 0; c \u0026lt; 3; ++c) { channels[c] = cv::Mat(384, 640, CV_32F, data1.data() + c * 384 * 640); } cv::Mat imgData1; cv::merge(channels, imgData1); // å½’ä¸€åŒ–å¹¶å°†æ•°æ®ç±»å‹è½¬æ¢å› CV_8UC3 ä»¥ä¾¿æ˜¾ç¤º imgData1 = imgData1 * 255.0; imgData1.convertTo(imgData1, CV_8UC3); cv::imwrite(\u0026#34;/home/metoak/Projects/input.png\u0026#34;, imgData1); // åˆ†é… GPU å†…å­˜ç”¨äºè¾“å…¥ void *gpuData1; void *gpuData2; cudaMalloc(\u0026amp;gpuData1, num_elements_input * sizeof(float)); cudaMalloc(\u0026amp;gpuData2, num_elements_input * sizeof(float)); // å¤åˆ¶è¾“å…¥æ•°æ®åˆ° GPU cudaMemcpy(gpuData1, data1.data(), num_elements_input * sizeof(float), cudaMemcpyHostToDevice); cudaMemcpy(gpuData2, data2.data(), num_elements_input * sizeof(float), cudaMemcpyHostToDevice); // æ ¹æ®ä½ çš„æ¨¡å‹çš„è¾“å‡ºå¤§å°æ¥åˆ›å»ºè¾“å‡ºæ•°æ® int num_elements_output = 1 * 3 * 384 * 640; // ä½ éœ€è¦æ ¹æ®ä½ çš„æ¨¡å‹çš„å®é™…è¾“å‡ºå¤§å°è¿›è¡Œè°ƒæ•´ // åˆ†é… GPU å†…å­˜ç”¨äºè¾“å‡º void *gpuOutput[7]; for (int i = 0; i \u0026lt; 7; ++i) { cudaMalloc(\u0026amp;gpuOutput[i], num_elements_output * sizeof(float)); } // åˆ›å»ºè¾“å…¥è¾“å‡ºçš„æŒ‡é’ˆæ•°ç»„ void *bindings[9] = {gpuData1, gpuData2, gpuOutput[0], gpuOutput[1], gpuOutput[2], gpuOutput[3], gpuOutput[4], gpuOutput[5], gpuOutput[6]}; // åˆ›å»º CUDA stream cudaStream_t stream; cudaStreamCreate(\u0026amp;stream); #if TEST_TIME == 1 // 100æ¬¡æ¨ç† cudaEvent_t start, stop; cudaEventCreate(\u0026amp;start); cudaEventCreate(\u0026amp;stop); float totalTime = 0; int iteration = 100; // æ¨ç†æ¬¡æ•° for (int i = 0; i \u0026lt; iteration; i++) { cudaEventRecord(start, stream); // æ‰§è¡Œæ¨ç† context-\u0026gt;enqueue(1, bindings, stream, nullptr); std::cout \u0026lt;\u0026lt; \u0026#34;Inference iteration: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; cudaEventRecord(stop, stream); cudaEventSynchronize(stop); float milliseconds = 0; cudaEventElapsedTime(\u0026amp;milliseconds, start, stop); totalTime += milliseconds; } float avgTime = totalTime / iteration; std::cout \u0026lt;\u0026lt; \u0026#34;Average inference time per image: \u0026#34; \u0026lt;\u0026lt; avgTime \u0026lt;\u0026lt; \u0026#34; ms.\u0026#34; \u0026lt;\u0026lt; std::endl; cudaEventDestroy(start); cudaEventDestroy(stop); #else // æ‰§è¡Œæ¨ç† context-\u0026gt;enqueue(1, bindings, stream, nullptr); #endif // å°†éœ€è¦çš„è¾“å‡ºæ•°æ®ä» GPU å†…å­˜å¤åˆ¶å› CPU å†…å­˜ std::vector\u0026lt;float\u0026gt; outputData(num_elements_output); cudaMemcpy(outputData.data(), gpuOutput[6], num_elements_output * sizeof(float), cudaMemcpyDeviceToHost); #if TEST_TIME == 1 // æ‰“å°å›¾åƒçš„å‰10ä¸ªåƒç´ å€¼ std::cout \u0026lt;\u0026lt; \u0026#34;First 10 pixels of the output image:\\n\u0026#34;; for (int i = 0; i \u0026lt; 10; ++i) { std::cout \u0026lt;\u0026lt; outputData[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } #endif // å°†ä¸€ç»´å‘é‡è½¬æ¢ä¸º3é€šé“å›¾åƒ cv::Mat img_out(384, 640, CV_32F, outputData.data()); cv::Mat img_out_normalized; cv::normalize(img_out, img_out_normalized, 0, 255, cv::NORM_MINMAX, CV_8U); cv::Mat img_color_mapped; cv::applyColorMap(img_out_normalized, img_color_mapped, cv::COLORMAP_JET); // cv::resize(img_out, img_out, cv::Size(1920, 1080)); // ä¿å­˜å›¾åƒ cv::imwrite(\u0026#34;/home/metoak/Projects/output.png\u0026#34;, img_color_mapped); std::cout \u0026lt;\u0026lt; \u0026#34;Output image size: \u0026#34; \u0026lt;\u0026lt; img_out_normalized.size() \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; // æ¸…ç† cudaFree(gpuData1); cudaFree(gpuData2); for (int i = 0; i \u0026lt; 7; ++i) { cudaFree(gpuOutput[i]); } context-\u0026gt;destroy(); engine-\u0026gt;destroy(); runtime-\u0026gt;destroy(); // é”€æ¯ CUDA stream cudaStreamDestroy(stream); return 0; } è¿™é‡Œæœ‰å‡ ç‚¹æ³¨æ„ï¼š\næ•°æ®çš„å‰å¤„ç†éœ€è¦æ ¹æ®çš„æ¨¡å‹çš„ç»“æ„æ¥è°ƒæ•´ï¼Œè¿™é‡Œæ˜¯è¾“å…¥ä¸¤ç»„å›¾åƒ\n// åˆ†é… GPU å†…å­˜ç”¨äºè¾“å…¥ void *gpuData1; void *gpuData2; cudaMalloc(\u0026amp;gpuData1, num_elements_input * sizeof(float)); cudaMalloc(\u0026amp;gpuData2, num_elements_input * sizeof(float)); // å¤åˆ¶è¾“å…¥æ•°æ®åˆ° GPU cudaMemcpy(gpuData1, data1.data(), num_elements_input * sizeof(float), cudaMemcpyHostToDevice); cudaMemcpy(gpuData2, data2.data(), num_elements_input * sizeof(float), cudaMemcpyHostToDevice); // æ ¹æ®ä½ çš„æ¨¡å‹çš„è¾“å‡ºå¤§å°æ¥åˆ›å»ºè¾“å‡ºæ•°æ® int num_elements_output = 1 * 3 * 384 * 640; // ä½ éœ€è¦æ ¹æ®ä½ çš„æ¨¡å‹çš„å®é™…è¾“å‡ºå¤§å°è¿›è¡Œè°ƒæ•´ // åˆ†é… GPU å†…å­˜ç”¨äºè¾“å‡º void *gpuOutput[7]; for (int i = 0; i \u0026lt; 7; ++i) { cudaMalloc(\u0026amp;gpuOutput[i], num_elements_output * sizeof(float)); } è¿™ä¸€å—å°±æ˜¯æ ¹æ®æ¨¡å‹çš„ç»“æ„æ¥è°ƒæ•´è¾“å…¥å’Œè¾“å‡ºçš„\nè¿™é‡ŒåŠ å…¥äº†å¾ˆå¤šæ‰‹å·¥è°ƒè¯•çš„ä¿¡æ¯ï¼Œæ¯”å¦‚ä¿å­˜æ¨¡å‹è¾“å…¥å‰åçš„å›¾åƒæ¥éªŒè¯\nè¿™æ˜¯ä¸€æ®µå®Œæ•´çš„æ¨ç†ä»£ç ï¼Œå¦‚æœè¿™ä¸€æ®µä»£ç èƒ½å¤ŸæˆåŠŸè¿è¡Œï¼Œé‚£ä¹ˆå°†è¿™æ®µä»£ç æ•´åˆåˆ°å®é™…çš„åŠŸèƒ½ä¸­ï¼Œå®šä¹‰å¥½è¾“å…¥è¾“å‡ºçš„æ¥å£ï¼Œæ¨¡å‹ä¹Ÿä¸€æ ·å¯ä»¥æ­£å¸¸ä½¿ç”¨\nè¿™é‡Œå¾ˆå¤šåœ°æ–¹æ˜¯æ‰‹åŠ¨ç”³è¯·å†…å­˜ç©ºé—´çš„ï¼Œæ‰€ä»¥ä¸€å®šæ³¨æ„ä½¿ç”¨åçš„é”€æ¯ï¼ˆå½“ç„¶ä¹Ÿå¯ä»¥ä½¿ç”¨ç±»æ¥å®ç°ææ„è¿™ç§æ–¹æ³•ï¼‰\n// æ¸…ç† cudaFree(gpuData1); cudaFree(gpuData2); for (int i = 0; i \u0026lt; 7; ++i) { cudaFree(gpuOutput[i]); } context-\u0026gt;destroy(); engine-\u0026gt;destroy(); runtime-\u0026gt;destroy(); // é”€æ¯ CUDA stream cudaStreamDestroy(stream); æ³¨æ„ç†è§£è¿™ä¸€éƒ¨åˆ†çš„åŠŸèƒ½å’Œå¿…è¦æ€§\næœ‰äº†è¿™ä¸¤éƒ¨åˆ†åï¼Œæ¨¡å‹ä»ONNXåˆ°TensorRTï¼Œå†åˆ°TensorRTçš„æ¨ç†å°±æ‰“é€šäº†ï¼\nå¯¹äºä¸Šé¢çš„ä»£ç ç¼–è¯‘ï¼Œç›´æ¥ä½¿ç”¨cmakeçš„æ–¹æ³•ï¼ŒCMakeLists.txtä¾‹å­å¦‚ä¸‹ï¼š\ncmake_minimum_required(VERSION 3.10) project(OnnxInferenceDemo) # è°ƒè¯•æ¨¡å¼ set(CMAKE_BUILD_TYPE Debug) set(CMAKE_CXX_STANDARD 14) # è®¾ç½®METOAKçš„é©±åŠ¨è·¯å¾„ set(MO_SDK_DIR /opt/moak) find_package(OpenCV REQUIRED) find_package(PCL REQUIRED) find_package(CUDA REQUIRED) find_path(TENSORRT_INCLUDE_DIR NvInfer.h HINTS ${TENSORRT_ROOT_DIR} PATH_SUFFIXES include/) find_library(TENSORRT_LIBRARY nvinfer HINTS ${TENSORRT_ROOT_DIR} PATH_SUFFIXES lib lib64 lib/x64) find_library(TENSORRT_ONNX_LIBRARY nvonnxparser HINTS ${TENSORRT_ROOT_DIR} PATH_SUFFIXES lib lib64 lib/x64) set(TENSORRT_INCLUDE_DIRS ${TENSORRT_INCLUDE_DIR}) set(TENSORRT_LIBRARIES ${TENSORRT_LIBRARY} ${TENSORRT_ONNX_LIBRARY}) include_directories(${TENSORRT_INCLUDE_DIRS}) include_directories(${CUDA_INCLUDE_DIRS}) include_directories( include ${OpenCV_INCLUDE_DIRS} ${MO_SDK_DIR}/3rdparty/ONNX/x86_64/GPU/include ) link_directories(${MO_SDK_DIR}/3rdparty/ONNX/x86_64/GPU/lib) link_directories(lib) add_executable(onnx_to_tensorrt src/onnx_to_tensorrt.cpp) target_link_libraries(onnx_to_tensorrt ${TENSORRT_LIBRARIES} ${CUDA_LIBRARIES}) add_executable(tensorrt_image src/tensorrt_image.cpp) target_link_libraries(tensorrt_image ${OpenCV_LIBS} ${TENSORRT_LIBRARIES} ${CUDA_LIBRARIES}) æ€»ç»“ ä½¿ç”¨TensorRTæ¥è¿›è¡Œæ¨¡å‹åŠ é€Ÿï¼Œå¦‚æœå¾ˆå‡‘å·§ï¼Œå½“å‰ç‰ˆæœ¬æ— æ³•æ”¯æŒOnnxçš„æ¨¡å‹è½¬æ¢ï¼Œé‚£å¯èƒ½æ­£å¸¸æ–¹æ³•å°±æ— æ³•ä½¿ç”¨ï¼Œå°±æ¯”å¦‚è¿™é‡Œçš„æ¨¡å‹ï¼Œåœ¨TensorRT 8.6ä¹‹å‰çš„ç‰ˆæœ¬ï¼Œéƒ½æ— æ³•å®Œæˆæ­£å¸¸çš„è½¬æ¢ï¼Œå¹¶ä¸”æŠ¥å‡ºæ®µé”™è¯¯å¼‚å¸¸ï¼Œè¿™ç§æƒ…å†µï¼Œå¯èƒ½å°±æ˜¯æ— æ³•æ”¯æŒï¼›\nè€Œä½¿ç”¨äº†æœ€æ–°ç‰ˆæœ¬å‘ç°ï¼Œæ¨¡å‹å¯ä»¥è½¬æ¢ï¼Œç›´æ¥æ— æ³•ç›´æ¥è¿›è¡Œæ¨¡å‹çš„æ¨ç†ï¼Œé‚£ä¹ˆå°±è¦æ’æŸ¥ä¸€ä¸‹æ˜¯å“ªé‡Œçš„é—®é¢˜äº†ï¼Œæ¯”å¦‚è¿™é‡Œçš„æ¨¡å‹ï¼Œåœ¨TensorRT 8.6.1ä¸Šå¯ä»¥å®Œæ•´è½¬æ¢ï¼Œä½†æ˜¯éœ€è¦æ‰‹åŠ¨æŒ‡å®šä½¿ç”¨çš„æ’ä»¶ï¼Œå†…å»ºçš„è¿˜æ˜¯å…¶ä»–çš„ï¼Œè¿™å°±ä¼šå½±å“æ¨¡å‹å¯¹åº”çš„æ¨ç†ç¨‹åºçš„ä¹¦å†™ã€‚\nå¦å¤–ä¸€ç‚¹ï¼š\nTensorRTçš„å®‰è£…å’Œç¯å¢ƒéƒ¨ç½²å…¶å®å¾ˆç®€å•ï¼Œä½†æ˜¯å› ä¸ºç½‘ç»œä¸Šçš„æœç´¢å‡ºæ¥çš„æ•™ç¨‹æ‚ä¸ƒæ‚å…«ï¼Œå¯¼è‡´æ²¡æœ‰æŒ‰ç…§ç»Ÿä¸€çš„æ–¹å¼æ¥å®‰è£…CUDAã€cuDNNã€TensorRTï¼Œå¯¼è‡´aptå®‰è£…æ—¶å‡ºç°ä¾èµ–å¼‚å¸¸ï¼Œå…¶å®åªè¦å…¨éƒ¨é‡‡ç”¨depåŒ…ç®¡ç†çš„æ–¹å¼æ¥å®‰è£…å°±ä¸ä¼šæœ‰é—®é¢˜ã€‚\n","permalink":"http://ahaknow.com/posts/know/%E6%B5%8B%E8%AF%95/","summary":"ç¬¬ä¸€æ­¥ï¼šå…ˆæµ‹è¯•æ¨¡å‹çš„åŸºå‡†é€Ÿåº¦ ä½¿ç”¨ONNXæµ‹è¯•æ¨¡å‹è¿è¡Œé€Ÿåº¦åŸºå‡† ç›®æ ‡ï¼š ä½¿ç”¨ONNX Runtimeæ¥æµ‹è¯•æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ¨ç†æ—¶é—´åŸºå‡†ã€‚ æ­¥éª¤ï¼š å®‰è£…ON","title":"æµ‹è¯•"}]